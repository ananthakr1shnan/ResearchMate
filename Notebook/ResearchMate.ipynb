{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1898ad98",
      "metadata": {
        "id": "1898ad98"
      },
      "source": [
        "# ResearchMate - Advanced AI Research Assistant\n",
        "\n",
        "**Powered by Groq Llama 3.3 70B**\n",
        "\n",
        "A comprehensive AI research assistant that combines:\n",
        "- Multi-source paper collection (arXiv, Semantic Scholar, CrossRef, PubMed)\n",
        "- Advanced PDF processing and citation analysis\n",
        "- Real-time trend monitoring and gap identification\n",
        "- Research project management and literature review generation\n",
        "- RAG-powered question answering with LangChain\n",
        "\n",
        "## Quick Start\n",
        "1. Run all cells in order\n",
        "2. Set your Groq API key when prompted\n",
        "3. Use `research_mate.demo()` to see everything in action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "f8f723e9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8f723e9",
        "outputId": "4c079564-4fba-45e0-e0ee-9489d603625c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Installing ResearchMate dependencies...\n",
            "==================================================\n",
            "üì¶ Installing groq...\n",
            "‚úÖ groq installed successfully\n",
            "‚úÖ langchain already installed\n",
            "üì¶ Installing langchain-community...\n",
            "‚úÖ langchain-community installed successfully\n",
            "üì¶ Installing chromadb...\n",
            "‚úÖ chromadb installed successfully\n",
            "‚úÖ sentence-transformers already installed\n",
            "üì¶ Installing arxiv...\n",
            "‚úÖ arxiv installed successfully\n",
            "‚úÖ requests already installed\n",
            "üì¶ Installing PyPDF2...\n",
            "‚úÖ PyPDF2 installed successfully\n",
            "üì¶ Installing pdfplumber...\n",
            "‚úÖ pdfplumber installed successfully\n",
            "üì¶ Installing PyMuPDF...\n",
            "‚úÖ PyMuPDF installed successfully\n",
            "‚úÖ networkx already installed\n",
            "‚úÖ matplotlib already installed\n",
            "‚úÖ pandas already installed\n",
            "‚úÖ numpy already installed\n",
            "‚úÖ python-dotenv already installed\n",
            "‚úÖ beautifulsoup4 already installed\n",
            "‚úÖ plotly already installed\n",
            "‚úÖ wordcloud already installed\n",
            "\n",
            "==================================================\n",
            "‚úÖ All packages installed successfully!\n",
            "\n",
            "üîë Set your Groq API key:\n",
            "   os.environ['GROQ_API_KEY'] = 'your_api_key_here'\n",
            "   Get your key from: https://console.groq.com/keys\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# PACKAGE INSTALLATION\n",
        "# ============================================================================\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "\n",
        "def install_package(package_name, import_name=None):\n",
        "    \"\"\"Install a package and verify it can be imported\"\"\"\n",
        "    if import_name is None:\n",
        "        import_name = package_name\n",
        "\n",
        "    try:\n",
        "        __import__(import_name)\n",
        "        print(f\"‚úÖ {package_name} already installed\")\n",
        "        return True\n",
        "    except ImportError:\n",
        "        print(f\"üì¶ Installing {package_name}...\")\n",
        "        try:\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package_name])\n",
        "            print(f\"‚úÖ {package_name} installed successfully\")\n",
        "            return True\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"‚ùå Failed to install {package_name}: {e}\")\n",
        "            return False\n",
        "\n",
        "# Core packages for ResearchMate\n",
        "packages = [\n",
        "    (\"groq\", \"groq\"),\n",
        "    (\"langchain\", \"langchain\"),\n",
        "    (\"langchain-community\", \"langchain_community\"),\n",
        "    (\"chromadb\", \"chromadb\"),\n",
        "    (\"sentence-transformers\", \"sentence_transformers\"),\n",
        "    (\"arxiv\", \"arxiv\"),\n",
        "    (\"requests\", \"requests\"),\n",
        "    (\"PyPDF2\", \"PyPDF2\"),\n",
        "    (\"pdfplumber\", \"pdfplumber\"),\n",
        "    (\"PyMuPDF\", \"fitz\"),\n",
        "    (\"networkx\", \"networkx\"),\n",
        "    (\"matplotlib\", \"matplotlib\"),\n",
        "    (\"pandas\", \"pandas\"),\n",
        "    (\"numpy\", \"numpy\"),\n",
        "    (\"python-dotenv\", \"dotenv\"),\n",
        "    (\"beautifulsoup4\", \"bs4\"),\n",
        "    (\"plotly\", \"plotly\"),\n",
        "    (\"wordcloud\", \"wordcloud\")\n",
        "]\n",
        "\n",
        "print(\"üöÄ Installing ResearchMate dependencies...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "failed_packages = []\n",
        "for package_name, import_name in packages:\n",
        "    if not install_package(package_name, import_name):\n",
        "        failed_packages.append(package_name)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "if failed_packages:\n",
        "    print(f\"‚ùå Failed to install: {', '.join(failed_packages)}\")\n",
        "else:\n",
        "    print(\"‚úÖ All packages installed successfully!\")\n",
        "\n",
        "print(\"\\nüîë Set your Groq API key:\")\n",
        "print(\"   os.environ['GROQ_API_KEY'] = 'your_api_key_here'\")\n",
        "print(\"   Get your key from: https://console.groq.com/keys\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f10d8d03",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f10d8d03",
        "outputId": "994f0728-6dcb-4d8d-8cb9-b1729f6c2dc8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Groq API key configured!\n",
            "‚úÖ Using CPU\n",
            "‚úÖ Configuration loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# IMPORTS AND CONFIGURATION\n",
        "# ============================================================================\n",
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "import warnings\n",
        "from typing import List, Dict, Optional, Tuple, Any\n",
        "from datetime import datetime, timedelta\n",
        "from collections import defaultdict\n",
        "\n",
        "# Data processing\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "\n",
        "# PDF processing\n",
        "import PyPDF2\n",
        "try:\n",
        "    import pdfplumber\n",
        "    import fitz  # PyMuPDF\n",
        "    PDF_ENHANCED = True\n",
        "except ImportError:\n",
        "    PDF_ENHANCED = False\n",
        "\n",
        "# AI and ML\n",
        "from groq import Groq\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "\n",
        "# LangChain\n",
        "from langchain.llms.base import LLM\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain, RetrievalQA\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema import Document\n",
        "from pydantic import Field\n",
        "\n",
        "# Data sources\n",
        "import arxiv\n",
        "\n",
        "# Analysis and visualization\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Web scraping\n",
        "try:\n",
        "    from bs4 import BeautifulSoup\n",
        "    WEB_SCRAPING_AVAILABLE = True\n",
        "except ImportError:\n",
        "    WEB_SCRAPING_AVAILABLE = False\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class Config:\n",
        "    \"\"\"Configuration for ResearchMate\"\"\"\n",
        "\n",
        "    # Groq Llama 3.3 70B settings\n",
        "    LLAMA_MODEL = \"llama-3.3-70b-versatile\"\n",
        "    GROQ_API_KEY = os.getenv('GROQ_API_KEY')\n",
        "    MAX_INPUT_TOKENS = 128000\n",
        "    MAX_OUTPUT_TOKENS = 8000\n",
        "    TEMPERATURE = 0.7\n",
        "    TOP_P = 0.9\n",
        "\n",
        "    # Embeddings and chunking\n",
        "    EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "    CHUNK_SIZE = 2000\n",
        "    CHUNK_OVERLAP = 400\n",
        "\n",
        "    # Database settings\n",
        "    CHROMA_DB_PATH = \"./chroma_db\"\n",
        "    COLLECTION_NAME = \"research_papers\"\n",
        "    PERSIST_DIRECTORY = \"./chroma_persist\"\n",
        "\n",
        "    # Search settings\n",
        "    TOP_K_SIMILAR = 5\n",
        "    MAX_PAPER_LENGTH = 100000\n",
        "    MAX_SUMMARY_LENGTH = 2000\n",
        "\n",
        "    def __init__(self):\n",
        "        os.makedirs(self.CHROMA_DB_PATH, exist_ok=True)\n",
        "        os.makedirs(self.PERSIST_DIRECTORY, exist_ok=True)\n",
        "\n",
        "        if not self.GROQ_API_KEY:\n",
        "            print(\"‚ö†Ô∏è  GROQ_API_KEY not found in environment variables!\")\n",
        "            print(\"üí° Set it with: os.environ['GROQ_API_KEY'] = 'your_key_here'\")\n",
        "            print(\"   Get your key from: https://console.groq.com/keys\")\n",
        "        else:\n",
        "            print(\"‚úÖ Groq API key configured!\")\n",
        "\n",
        "config = Config()\n",
        "\n",
        "def setup_device():\n",
        "    \"\"\"Setup compute device\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"‚úÖ GPU Available: {torch.cuda.get_device_name(0)}\")\n",
        "        return torch.device(\"cuda\")\n",
        "    else:\n",
        "        print(\"‚úÖ Using CPU\")\n",
        "        return torch.device(\"cpu\")\n",
        "\n",
        "device = setup_device()\n",
        "print(\"‚úÖ Configuration loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "3f6ed0e5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3f6ed0e5",
        "outputId": "d5d376d8-1313-4b64-9c1b-d6b1f3324654"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Groq Llama 3.3 70B initialized successfully!\n",
            "‚úÖ Groq Llama 3.3 70B ready for research tasks!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# GROQ LLAMA 3.3 70B INTEGRATION\n",
        "# ============================================================================\n",
        "\n",
        "class GroqLlamaLLM(LLM):\n",
        "    \"\"\"LangChain-compatible wrapper for Groq Llama 3.3 70B\"\"\"\n",
        "\n",
        "    groq_client: Any = Field(default=None)\n",
        "    model_name: str = Field(default=\"llama-3.3-70b-versatile\")\n",
        "    temperature: float = Field(default=0.7)\n",
        "    max_tokens: int = Field(default=2000)\n",
        "    top_p: float = Field(default=0.9)\n",
        "\n",
        "    def __init__(self, api_key: str, **kwargs):\n",
        "        groq_client = Groq(api_key=api_key)\n",
        "        super().__init__(groq_client=groq_client, **kwargs)\n",
        "\n",
        "    class Config:\n",
        "        arbitrary_types_allowed = True\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"groq_llama\"\n",
        "\n",
        "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
        "        try:\n",
        "            response = self.groq_client.chat.completions.create(\n",
        "                model=self.model_name,\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                temperature=self.temperature,\n",
        "                max_tokens=self.max_tokens,\n",
        "                top_p=self.top_p,\n",
        "                stop=stop\n",
        "            )\n",
        "            return response.choices[0].message.content\n",
        "        except Exception as e:\n",
        "            return f\"Error: {str(e)}\"\n",
        "\n",
        "    @property\n",
        "    def _identifying_params(self) -> Dict[str, Any]:\n",
        "        return {\n",
        "            \"model_name\": self.model_name,\n",
        "            \"temperature\": self.temperature,\n",
        "            \"max_tokens\": self.max_tokens,\n",
        "            \"top_p\": self.top_p\n",
        "        }\n",
        "\n",
        "class GroqProcessor:\n",
        "    \"\"\"Enhanced Groq Llama processor with research capabilities\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        if not config.GROQ_API_KEY:\n",
        "            raise ValueError(\"Groq API key not found! Please set GROQ_API_KEY environment variable.\")\n",
        "\n",
        "        self.groq_client = Groq(api_key=config.GROQ_API_KEY)\n",
        "        self.llm = GroqLlamaLLM(\n",
        "            api_key=config.GROQ_API_KEY,\n",
        "            model_name=config.LLAMA_MODEL,\n",
        "            temperature=config.TEMPERATURE,\n",
        "            max_tokens=config.MAX_OUTPUT_TOKENS,\n",
        "            top_p=config.TOP_P\n",
        "        )\n",
        "        print(\"‚úÖ Groq Llama 3.3 70B initialized successfully!\")\n",
        "\n",
        "    def generate_response(self, prompt: str, max_tokens: int = 2000) -> str:\n",
        "        \"\"\"Generate response using Groq Llama\"\"\"\n",
        "        try:\n",
        "            response = self.groq_client.chat.completions.create(\n",
        "                model=config.LLAMA_MODEL,\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                temperature=config.TEMPERATURE,\n",
        "                max_tokens=max_tokens,\n",
        "                top_p=config.TOP_P\n",
        "            )\n",
        "            return response.choices[0].message.content.strip()\n",
        "        except Exception as e:\n",
        "            return f\"Error: {str(e)}\"\n",
        "\n",
        "    def summarize_paper(self, title: str, abstract: str, content: str) -> Dict[str, str]:\n",
        "        \"\"\"Generate comprehensive paper summary\"\"\"\n",
        "        try:\n",
        "            if len(content) > config.MAX_PAPER_LENGTH:\n",
        "                content = content[:config.MAX_PAPER_LENGTH] + \"...\"\n",
        "\n",
        "            prompt = f\"\"\"Analyze this research paper and provide a structured summary:\n",
        "\n",
        "Title: {title}\n",
        "Abstract: {abstract}\n",
        "Content: {content[:8000]}\n",
        "\n",
        "Provide a comprehensive summary with these sections:\n",
        "1. **MAIN SUMMARY** (2-3 sentences)\n",
        "2. **KEY CONTRIBUTIONS** (3-5 bullet points)\n",
        "3. **METHODOLOGY** (brief description)\n",
        "4. **KEY FINDINGS** (3-5 bullet points)\n",
        "5. **LIMITATIONS** (if mentioned)\n",
        "\n",
        "Format your response clearly with section headers.\"\"\"\n",
        "\n",
        "            response = self.generate_response(prompt, max_tokens=config.MAX_SUMMARY_LENGTH)\n",
        "            return self._parse_summary_response(response, title, abstract)\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                'summary': f'Error generating summary: {str(e)}',\n",
        "                'contributions': 'N/A',\n",
        "                'methodology': 'N/A',\n",
        "                'findings': 'N/A',\n",
        "                'limitations': 'N/A',\n",
        "                'title': title,\n",
        "                'abstract': abstract\n",
        "            }\n",
        "\n",
        "    def _parse_summary_response(self, response: str, title: str, abstract: str) -> Dict[str, str]:\n",
        "        \"\"\"Parse AI response into structured summary\"\"\"\n",
        "        sections = {\n",
        "            'summary': '',\n",
        "            'contributions': '',\n",
        "            'methodology': '',\n",
        "            'findings': '',\n",
        "            'limitations': '',\n",
        "            'title': title,\n",
        "            'abstract': abstract\n",
        "        }\n",
        "\n",
        "        if \"Error:\" in response:\n",
        "            return sections\n",
        "\n",
        "        lines = response.split('\\n')\n",
        "        current_section = 'summary'\n",
        "\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "\n",
        "            line_lower = line.lower()\n",
        "            if any(keyword in line_lower for keyword in ['main summary', '1.', '**main']):\n",
        "                current_section = 'summary'\n",
        "                continue\n",
        "            elif any(keyword in line_lower for keyword in ['key contributions', '2.', '**key contrib']):\n",
        "                current_section = 'contributions'\n",
        "                continue\n",
        "            elif any(keyword in line_lower for keyword in ['methodology', '3.', '**method']):\n",
        "                current_section = 'methodology'\n",
        "                continue\n",
        "            elif any(keyword in line_lower for keyword in ['key findings', 'findings', '4.', '**key find']):\n",
        "                current_section = 'findings'\n",
        "                continue\n",
        "            elif any(keyword in line_lower for keyword in ['limitations', '5.', '**limit']):\n",
        "                current_section = 'limitations'\n",
        "                continue\n",
        "\n",
        "            if not line.startswith(('1.', '2.', '3.', '4.', '5.', '**', '#')):\n",
        "                sections[current_section] += line + ' '\n",
        "\n",
        "        return sections\n",
        "\n",
        "    def analyze_trends(self, texts: List[str]) -> Dict:\n",
        "        \"\"\"Analyze research trends from multiple texts\"\"\"\n",
        "        try:\n",
        "            combined_text = ' '.join(texts[:10])  # Limit to avoid token limits\n",
        "\n",
        "            prompt = f\"\"\"Analyze research trends in this collection of texts:\n",
        "\n",
        "{combined_text[:5000]}\n",
        "\n",
        "Identify:\n",
        "1. Key research themes and topics\n",
        "2. Emerging trends and directions\n",
        "3. Frequently mentioned technologies/methods\n",
        "4. Research gaps or opportunities\n",
        "\n",
        "Provide analysis as structured points.\"\"\"\n",
        "\n",
        "            response = self.generate_response(prompt, max_tokens=1500)\n",
        "\n",
        "            return {\n",
        "                'trend_analysis': response,\n",
        "                'texts_analyzed': len(texts),\n",
        "                'analysis_date': datetime.now().isoformat(),\n",
        "                'keywords': self._extract_keywords(combined_text)\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                'trend_analysis': f'Error: {str(e)}',\n",
        "                'texts_analyzed': 0,\n",
        "                'analysis_date': datetime.now().isoformat(),\n",
        "                'keywords': []\n",
        "            }\n",
        "\n",
        "    def _extract_keywords(self, text: str) -> List[str]:\n",
        "        \"\"\"Extract keywords from text\"\"\"\n",
        "        words = re.findall(r'\\b[a-zA-Z]+\\b', text.lower())\n",
        "        stop_words = {'the', 'and', 'for', 'are', 'with', 'this', 'that', 'from', 'they', 'have'}\n",
        "        keywords = [w for w in words if len(w) > 3 and w not in stop_words]\n",
        "\n",
        "        # Count frequency and return top keywords\n",
        "        word_counts = {}\n",
        "        for word in keywords:\n",
        "            word_counts[word] = word_counts.get(word, 0) + 1\n",
        "\n",
        "        return [word for word, count in sorted(word_counts.items(), key=lambda x: x[1], reverse=True)[:20]]\n",
        "\n",
        "# Initialize Groq processor\n",
        "try:\n",
        "    groq_processor = GroqProcessor()\n",
        "    print(\"‚úÖ Groq Llama 3.3 70B ready for research tasks!\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Failed to initialize Groq processor: {e}\")\n",
        "    print(\"üí° Make sure you have set the GROQ_API_KEY environment variable\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "0052a3bf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438,
          "referenced_widgets": [
            "f17c7c4b81654bd182c84c04047b5e94",
            "8e0d1db6f08b4331a65b27327e6e4dbd",
            "c1e93cc3f19d444ead23a5dcb598e570",
            "1e713b8997b3420cb74aa90152055c00",
            "0cd803469ed74da8a17fd66b4d29d9d0",
            "d4eabb690fb44fac80bb1ed140c4fd98",
            "f33a6bf6f75040ca8e38c8497b2ecb40",
            "915c98ebc6a94f9fa51a8abbe78479c7",
            "8e70e588a6af4bddbb4bedefe6c6ebfa",
            "671f83945874437a962d1acaa1f173c5",
            "61ce5e244ea4444d8d660bd32db8eaf8",
            "a30bb18bba054c2fb0cdb3ffab4112a5",
            "9324aed3cba94c4191cc8601d71ac42c",
            "1f69800c098a43bbb9646e4dbf4a6001",
            "2b18ec66f5eb4535b064927174bc9729",
            "3332939f682448528033b95364677ae1",
            "02eb541780c34a6882c6c229fd396e82",
            "0b151a27391e4f6186d2b8a0b1f57382",
            "a000eb0f593b42e99197e27137f5729f",
            "0fb495a137bc4affa94ee09498cf9c12",
            "10f55dbba6a649239b1a5a103af460a7",
            "eab91f3980ec4bef95e744fbb72b3cb8",
            "a49ad0afc244473d8987da737970dad0",
            "738420ad403b467e9dd0a751cdc41b21",
            "cd377d8af3ac456d8b343f881f720b6c",
            "27b65473a60f4fb59bfa50e5d6a23c5c",
            "e07469a013774453a0b31a12551d398f",
            "ef3f04b3a70440c4ac88665aa1dedee2",
            "c124b9c134eb4614872027641ecf95c6",
            "de133caa40d447d480d733cba0afe27b",
            "49c601085670420ab7fd919673fbd400",
            "b708d5332e214b35bf7be28772db57d0",
            "09f3066952054f78965a74584cd0e1ce",
            "a57487e20776447a9e57d77df4288528",
            "ee87ef7578e14101b11a1d448f9a58a9",
            "2a0062c2dae84d049a343ccde5121dfb",
            "4ba16e2312224aa3b3c56c6f532f99a2",
            "dbd8cd875c0a487799037439f6c51588",
            "0284d6b08c7f42bd8c446825416f6258",
            "5e6b970307824f1db05c6a5eeaf67e18",
            "de08fa989d3242a998afe9b1f3b8bea4",
            "450d5ff4f0f847df8168550541682c17",
            "bcde909348124dabb6bf33e322004068",
            "7e3b6be4c79e4715b82502498ad97c6e",
            "a751281285f44cd09cf31949de5a916f",
            "8d9b102b6b2a4e17b812eccbe1de6592",
            "1dc9780edc89478cb0a113fced247622",
            "e9d8c925c88d4d68bc8d09be0755f8cd",
            "653c1cce08334983b335f8bc57c98bcb",
            "1a6d66dc1dde466fb3f455c4ce31362b",
            "dc8d37e8c9fc4a34bd18d4bb3b66ffac",
            "bbd8d5a7a88f43e2b8afdfff46a1ddca",
            "3051416b47474430834067a266be7062",
            "e348ab8f71204eb0b084fda45ade0fa9",
            "670bce775ac14e8f919ed7bbdebff95d",
            "70448b95ebbc435b8f572ce518f0ec7a",
            "a307d782d86747bd94ddeba74e1999c9",
            "6fafcc1e7fa84f2da0c952187091f53d",
            "723e4aeb89884e7e90ed0a02cfb209bb",
            "8d6820c9076e45619667c0df29b4e1d4",
            "5ccbafb9904940d3bafcff3f1644f783",
            "c64d833fa8534220909d79c245a5dfba",
            "0c23ac18d5cf479086127d2eb4cc5eb6",
            "4759d7e761474f81944395a05d26d9bb",
            "70f5f9d53a4d46cbb978e62628997c3f",
            "9acf329bfcfb4b549645e2230ccaff5f",
            "94163f516e284471bb2f57e9372c28d9",
            "c4c72c7b35e54f63a0a3b821a3ae1690",
            "a47d1c5d0fc9430292253eccbf46cf07",
            "f48073e335fb4dc88fdd924270148dc8",
            "447ebd4c59a24062bdbd98d7301d543e",
            "0704525e4399411a999c40a9712ceb08",
            "27e50cca49be46cfa408ca6179c8aab0",
            "5477616877cb481a985d43aee7796797",
            "dcb3bac21fea465ebe566d37939a8103",
            "72d89fa17bb74863810f802d7ea3d674",
            "d0bcff3978c84c9fadedbbfae00d61a2",
            "4dc6e9f044034cbbbff05478519c1dde",
            "15824affe4d744458f9db3a3e1491f65",
            "c99d3b98c3574a13a86d42d93e6b99f9",
            "c3db88cccb9a4e82b0cd6d00fc05579c",
            "e3465ce42b634f95bec698a7d6fb8e7d",
            "8b35063955e54c09a5dc4ba99e7bf3f3",
            "b89c551ca92c4298b9b54feb12c40711",
            "01c8c68536464ed4b2c0a7abbf2bd814",
            "4c2cbf3e4e564493aeb0b37941f6b051",
            "9615932f580c4e54a7338a926d34b634",
            "b4803ed445094570bc9fd1fbd434c547",
            "2ef870942bfc4f08941db0064f26ad73",
            "ef54d00e6f8d4fe39feabd929f0756cc",
            "977c2b37901941ac9cf2a7fe555c6b45",
            "b82e6746847445e2b09949f045a9770c",
            "02395738b2874fc4a2f08922188826a8",
            "33bb0df178b24b0eb8abffd9fb7fb859",
            "a6fb36df165c4c6b844d6555b19bca9e",
            "42b82f4a91a344968bcbfffd66af087c",
            "0622a55a482740139a1200eaf6532617",
            "24e1819cf504499ead647ce2b3c99279",
            "c5db68cd83c54225bea7dd70d3eeb7fc",
            "26e2b7e6f8094564a964d22c86336b24",
            "3b7c3c3c6ff145cfbc44f5d8912aa579",
            "ea6d7b06d7664bcfb2a53ab1fe33fd27",
            "41e658e12d7943e88d08a419894fd72b",
            "9c4b98bb430c47a5b070ca906bf1a2c1",
            "eb350b4787e64481adee64061ea49100",
            "4f634dca7a5640299b031c1872d7f416",
            "e057a92a108048c79fd098a5dfe3b54c",
            "048ded1ed86b499e963a79633aed190c",
            "935e91b7d6dc475dae1dc40899088e86",
            "3c6265126d664477bea2376dccea0914",
            "51586428e3a84ad084961d0410cb37c0",
            "780e6b411a8f4873a2ad658e95984744",
            "77735b885035477d908adbd7a789539a",
            "942fdc37fde44231ab273f9299708878",
            "f16933aa4e3f465b882507541708d2a1",
            "4680b945991c440ea83ba2933ab18239",
            "b01f362bece5437285fabb41344cc6c1",
            "c63184b41f534298a13305481ffe65e0",
            "518a65634f6145f6b89fce0e2249a4fd",
            "260925605f934c35a2826635d8f7c0b2",
            "9e439eba769143569409dd69a4534999"
          ]
        },
        "id": "0052a3bf",
        "outputId": "854997b0-17f3-45a0-e973-777c162ce7a3"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f17c7c4b81654bd182c84c04047b5e94",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a30bb18bba054c2fb0cdb3ffab4112a5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a49ad0afc244473d8987da737970dad0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a57487e20776447a9e57d77df4288528",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a751281285f44cd09cf31949de5a916f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "70448b95ebbc435b8f572ce518f0ec7a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "94163f516e284471bb2f57e9372c28d9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4dc6e9f044034cbbbff05478519c1dde",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2ef870942bfc4f08941db0064f26ad73",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "26e2b7e6f8094564a964d22c86336b24",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "51586428e3a84ad084961d0410cb37c0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ ChromaDB vectorstore ready!\n",
            "‚úÖ QA chain ready!\n",
            "‚úÖ RAG System initialized successfully!\n",
            "‚úÖ RAG System ready for research queries!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# LANGCHAIN RAG SYSTEM\n",
        "# ============================================================================\n",
        "\n",
        "class RAGSystem:\n",
        "    \"\"\"Retrieval-Augmented Generation system using LangChain + Groq Llama\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.embeddings = HuggingFaceEmbeddings(model_name=config.EMBEDDING_MODEL)\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=config.CHUNK_SIZE,\n",
        "            chunk_overlap=config.CHUNK_OVERLAP\n",
        "        )\n",
        "        self.vectorstore = None\n",
        "        self.qa_chain = None\n",
        "        self.retriever = None\n",
        "        self.memory = ConversationBufferMemory(\n",
        "            memory_key=\"chat_history\",\n",
        "            return_messages=True\n",
        "        )\n",
        "\n",
        "        # Initialize RAG system\n",
        "        self._setup_vectorstore()\n",
        "        self._setup_qa_chain()\n",
        "\n",
        "        print(\"‚úÖ RAG System initialized successfully!\")\n",
        "\n",
        "    def _setup_vectorstore(self):\n",
        "        \"\"\"Setup ChromaDB vectorstore\"\"\"\n",
        "        try:\n",
        "            self.vectorstore = Chroma(\n",
        "                persist_directory=config.PERSIST_DIRECTORY,\n",
        "                embedding_function=self.embeddings,\n",
        "                collection_name=config.COLLECTION_NAME\n",
        "            )\n",
        "            print(\"‚úÖ ChromaDB vectorstore ready!\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed to setup vectorstore: {e}\")\n",
        "            # Fallback to in-memory vectorstore\n",
        "            self.vectorstore = Chroma(\n",
        "                embedding_function=self.embeddings,\n",
        "                collection_name=config.COLLECTION_NAME\n",
        "            )\n",
        "            print(\"‚úÖ Fallback to in-memory vectorstore\")\n",
        "\n",
        "    def _setup_qa_chain(self):\n",
        "        \"\"\"Setup QA chain with Groq Llama\"\"\"\n",
        "        try:\n",
        "            self.retriever = self.vectorstore.as_retriever(\n",
        "                search_kwargs={\"k\": config.TOP_K_SIMILAR}\n",
        "            )\n",
        "\n",
        "            # Create QA chain with Groq Llama\n",
        "            self.qa_chain = RetrievalQA.from_chain_type(\n",
        "                llm=groq_processor.llm,\n",
        "                chain_type=\"stuff\",\n",
        "                retriever=self.retriever,\n",
        "                return_source_documents=True\n",
        "            )\n",
        "\n",
        "            print(\"‚úÖ QA chain ready!\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed to setup QA chain: {e}\")\n",
        "\n",
        "    def add_documents(self, documents: List[Dict]):\n",
        "        \"\"\"Add documents to the vectorstore\"\"\"\n",
        "        try:\n",
        "            doc_objects = []\n",
        "            for doc in documents:\n",
        "                content = doc.get('content', '') or doc.get('abstract', '')\n",
        "                if content:\n",
        "                    chunks = self.text_splitter.split_text(content)\n",
        "                    for chunk in chunks:\n",
        "                        # Convert authors list to string\n",
        "                        authors = doc.get('authors', 'Unknown')\n",
        "                        if isinstance(authors, list):\n",
        "                            authors = ', '.join(str(author) for author in authors)\n",
        "\n",
        "                        # Ensure year is a string\n",
        "                        year = doc.get('year', 'Unknown')\n",
        "                        if year is not None:\n",
        "                            year = str(year)\n",
        "                        else:\n",
        "                            year = 'Unknown'\n",
        "\n",
        "                        doc_objects.append(Document(\n",
        "                            page_content=chunk,\n",
        "                            metadata={\n",
        "                                'title': str(doc.get('title', 'Unknown')),\n",
        "                                'authors': str(authors),\n",
        "                                'source': str(doc.get('source', 'Unknown')),\n",
        "                                'url': str(doc.get('url', '')),\n",
        "                                'year': year\n",
        "                            }\n",
        "                        ))\n",
        "\n",
        "            if doc_objects:\n",
        "                self.vectorstore.add_documents(doc_objects)\n",
        "                print(f\"‚úÖ Added {len(doc_objects)} document chunks to vectorstore\")\n",
        "            else:\n",
        "                print(\"‚ö†Ô∏è  No valid documents to add\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed to add documents: {e}\")\n",
        "\n",
        "    def query(self, question: str, include_sources: bool = True) -> Dict:\n",
        "        \"\"\"Query the RAG system\"\"\"\n",
        "        try:\n",
        "            if not self.qa_chain:\n",
        "                return {\n",
        "                    'answer': 'RAG system not properly initialized',\n",
        "                    'sources': [],\n",
        "                    'error': 'System not ready'\n",
        "                }\n",
        "\n",
        "            # Get response from QA chain\n",
        "            response = self.qa_chain({\"query\": question})\n",
        "\n",
        "            result = {\n",
        "                'answer': response['result'],\n",
        "                'sources': [],\n",
        "                'query': question,\n",
        "                'timestamp': datetime.now().isoformat()\n",
        "            }\n",
        "\n",
        "            # Add source documents if requested\n",
        "            if include_sources and 'source_documents' in response:\n",
        "                for doc in response['source_documents']:\n",
        "                    result['sources'].append({\n",
        "                        'title': doc.metadata.get('title', 'Unknown'),\n",
        "                        'authors': doc.metadata.get('authors', 'Unknown'),\n",
        "                        'source': doc.metadata.get('source', 'Unknown'),\n",
        "                        'url': doc.metadata.get('url', ''),\n",
        "                        'content_snippet': doc.page_content[:200] + '...' if len(doc.page_content) > 200 else doc.page_content\n",
        "                    })\n",
        "\n",
        "            return result\n",
        "\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                'answer': f'Error processing query: {str(e)}',\n",
        "                'sources': [],\n",
        "                'query': question,\n",
        "                'error': str(e),\n",
        "                'timestamp': datetime.now().isoformat()\n",
        "            }\n",
        "\n",
        "    def get_stats(self) -> Dict:\n",
        "        \"\"\"Get RAG system statistics\"\"\"\n",
        "        try:\n",
        "            collection = self.vectorstore._collection\n",
        "            doc_count = collection.count()\n",
        "            return {\n",
        "                'total_documents': doc_count,\n",
        "                'embedding_model': config.EMBEDDING_MODEL,\n",
        "                'chunk_size': config.CHUNK_SIZE,\n",
        "                'chunk_overlap': config.CHUNK_OVERLAP,\n",
        "                'vectorstore_type': 'ChromaDB',\n",
        "                'status': 'Ready'\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                'total_documents': 0,\n",
        "                'status': f'Error: {str(e)}'\n",
        "            }\n",
        "\n",
        "    def clear_database(self):\n",
        "        \"\"\"Clear the vectorstore\"\"\"\n",
        "        try:\n",
        "            self.vectorstore.delete_collection()\n",
        "            self._setup_vectorstore()\n",
        "            print(\"‚úÖ Database cleared successfully!\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed to clear database: {e}\")\n",
        "\n",
        "    def persist(self):\n",
        "        \"\"\"Persist the vectorstore to disk\"\"\"\n",
        "        try:\n",
        "            self.vectorstore.persist()\n",
        "            print(\"‚úÖ Vectorstore persisted to disk!\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed to persist vectorstore: {e}\")\n",
        "\n",
        "# Initialize RAG system\n",
        "try:\n",
        "    rag_system = RAGSystem()\n",
        "    print(\"‚úÖ RAG System ready for research queries!\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Failed to initialize RAG system: {e}\")\n",
        "    rag_system = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "e866567c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e866567c",
        "outputId": "d5074629-3392-462c-8862-5dd4cb8a363c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ arXiv fetcher initialized!\n",
            "‚úÖ arXiv integration ready!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# ARXIV INTEGRATION\n",
        "# ============================================================================\n",
        "\n",
        "class ArxivFetcher:\n",
        "    \"\"\"Enhanced arXiv paper fetcher with robust error handling\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.client = arxiv.Client(\n",
        "            page_size=100,\n",
        "            delay_seconds=1.0,\n",
        "            num_retries=3\n",
        "        )\n",
        "        print(\"‚úÖ arXiv fetcher initialized!\")\n",
        "\n",
        "    def search_papers(self, query: str, max_results: int = 10, sort_by: str = \"relevance\") -> List[Dict]:\n",
        "        \"\"\"Search for papers on arXiv\"\"\"\n",
        "        try:\n",
        "            # Build search query\n",
        "            search = arxiv.Search(\n",
        "                query=query,\n",
        "                max_results=max_results,\n",
        "                sort_by=getattr(arxiv.SortCriterion, sort_by.title(), arxiv.SortCriterion.Relevance)\n",
        "            )\n",
        "\n",
        "            papers = []\n",
        "            for paper in self.client.results(search):\n",
        "                try:\n",
        "                    paper_data = {\n",
        "                        'title': paper.title,\n",
        "                        'authors': [author.name for author in paper.authors],\n",
        "                        'abstract': paper.summary,\n",
        "                        'url': paper.entry_id,\n",
        "                        'pdf_url': paper.pdf_url,\n",
        "                        'published': paper.published.isoformat() if paper.published else None,\n",
        "                        'updated': paper.updated.isoformat() if paper.updated else None,\n",
        "                        'categories': paper.categories,\n",
        "                        'primary_category': paper.primary_category,\n",
        "                        'source': 'arXiv',\n",
        "                        'year': paper.published.year if paper.published else None,\n",
        "                        'content': paper.summary  # Use abstract as content for now\n",
        "                    }\n",
        "                    papers.append(paper_data)\n",
        "                except Exception as e:\n",
        "                    print(f\"‚ö†Ô∏è  Error processing paper {paper.title}: {e}\")\n",
        "                    continue\n",
        "\n",
        "            print(f\"‚úÖ Found {len(papers)} papers on arXiv\")\n",
        "            return papers\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error searching arXiv: {e}\")\n",
        "            return []\n",
        "\n",
        "    def get_paper_details(self, arxiv_id: str) -> Optional[Dict]:\n",
        "        \"\"\"Get detailed information about a specific paper\"\"\"\n",
        "        try:\n",
        "            search = arxiv.Search(id_list=[arxiv_id])\n",
        "            paper = next(self.client.results(search))\n",
        "\n",
        "            return {\n",
        "                'title': paper.title,\n",
        "                'authors': [author.name for author in paper.authors],\n",
        "                'abstract': paper.summary,\n",
        "                'url': paper.entry_id,\n",
        "                'pdf_url': paper.pdf_url,\n",
        "                'published': paper.published.isoformat() if paper.published else None,\n",
        "                'updated': paper.updated.isoformat() if paper.updated else None,\n",
        "                'categories': paper.categories,\n",
        "                'primary_category': paper.primary_category,\n",
        "                'source': 'arXiv',\n",
        "                'year': paper.published.year if paper.published else None,\n",
        "                'content': paper.summary,\n",
        "                'doi': paper.doi,\n",
        "                'journal_ref': paper.journal_ref,\n",
        "                'comment': paper.comment\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error fetching arXiv paper {arxiv_id}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def download_pdf(self, paper_url: str, filename: str = None) -> Optional[str]:\n",
        "        \"\"\"Download PDF from arXiv\"\"\"\n",
        "        try:\n",
        "            if not filename:\n",
        "                filename = f\"arxiv_{int(time.time())}.pdf\"\n",
        "\n",
        "            # Get paper ID from URL\n",
        "            paper_id = paper_url.split('/')[-1]\n",
        "            if paper_id.startswith('abs'):\n",
        "                paper_id = paper_id[4:]\n",
        "\n",
        "            search = arxiv.Search(id_list=[paper_id])\n",
        "            paper = next(self.client.results(search))\n",
        "\n",
        "            # Download PDF\n",
        "            paper.download_pdf(dirpath=\"./\", filename=filename)\n",
        "\n",
        "            print(f\"‚úÖ Downloaded PDF: {filename}\")\n",
        "            return filename\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error downloading PDF: {e}\")\n",
        "            return None\n",
        "\n",
        "    def get_trending_papers(self, category: str = \"cs.AI\", days: int = 7) -> List[Dict]:\n",
        "        \"\"\"Get trending papers from a specific category\"\"\"\n",
        "        try:\n",
        "            # Calculate date range\n",
        "            end_date = datetime.now()\n",
        "            start_date = end_date - timedelta(days=days)\n",
        "\n",
        "            # Search for recent papers\n",
        "            query = f\"cat:{category} AND submittedDate:[{start_date.strftime('%Y%m%d')}* TO {end_date.strftime('%Y%m%d')}*]\"\n",
        "\n",
        "            search = arxiv.Search(\n",
        "                query=query,\n",
        "                max_results=50,\n",
        "                sort_by=arxiv.SortCriterion.SubmittedDate,\n",
        "                sort_order=arxiv.SortOrder.Descending\n",
        "            )\n",
        "\n",
        "            papers = []\n",
        "            for paper in self.client.results(search):\n",
        "                try:\n",
        "                    paper_data = {\n",
        "                        'title': paper.title,\n",
        "                        'authors': [author.name for author in paper.authors],\n",
        "                        'abstract': paper.summary,\n",
        "                        'url': paper.entry_id,\n",
        "                        'pdf_url': paper.pdf_url,\n",
        "                        'published': paper.published.isoformat() if paper.published else None,\n",
        "                        'categories': paper.categories,\n",
        "                        'primary_category': paper.primary_category,\n",
        "                        'source': 'arXiv',\n",
        "                        'year': paper.published.year if paper.published else None,\n",
        "                        'content': paper.summary\n",
        "                    }\n",
        "                    papers.append(paper_data)\n",
        "                except Exception as e:\n",
        "                    continue\n",
        "\n",
        "            print(f\"‚úÖ Found {len(papers)} trending papers in {category}\")\n",
        "            return papers\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error getting trending papers: {e}\")\n",
        "            return []\n",
        "\n",
        "# Initialize arXiv fetcher\n",
        "arxiv_fetcher = ArxivFetcher()\n",
        "print(\"‚úÖ arXiv integration ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "f6aa7e8e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6aa7e8e",
        "outputId": "4a37e3c2-a777-4ad7-8841-bfe73251f991"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Multi-source collector initialized!\n",
            "‚úÖ Multi-source data collector ready!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# MULTI-SOURCE DATA COLLECTOR\n",
        "# ============================================================================\n",
        "\n",
        "class MultiSourceCollector:\n",
        "    \"\"\"Unified data collector for multiple research sources\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.arxiv_fetcher = arxiv_fetcher\n",
        "        self.session = requests.Session()\n",
        "        self.session.headers.update({\n",
        "            'User-Agent': 'ResearchMate/1.0 (Research Assistant)',\n",
        "            'Accept': 'application/json'\n",
        "        })\n",
        "        print(\"‚úÖ Multi-source collector initialized!\")\n",
        "\n",
        "    def safe_get(self, url: str, params: Dict = None, timeout: int = 30) -> Optional[Dict]:\n",
        "        \"\"\"Safely make HTTP requests with error handling\"\"\"\n",
        "        try:\n",
        "            response = self.session.get(url, params=params, timeout=timeout)\n",
        "            response.raise_for_status()\n",
        "            return response.json()\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"‚ö†Ô∏è  HTTP error for {url}: {e}\")\n",
        "            return None\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"‚ö†Ô∏è  JSON decode error for {url}: {e}\")\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è  Unexpected error for {url}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def safe_string(self, value: Any) -> str:\n",
        "        \"\"\"Safely convert any value to string\"\"\"\n",
        "        if value is None:\n",
        "            return \"\"\n",
        "        if isinstance(value, (list, tuple)):\n",
        "            return \", \".join(str(v) for v in value if v is not None)\n",
        "        return str(value)\n",
        "\n",
        "    def search_semantic_scholar(self, query: str, limit: int = 10) -> List[Dict]:\n",
        "        \"\"\"Search Semantic Scholar API with retry logic\"\"\"\n",
        "        try:\n",
        "            url = \"https://api.semanticscholar.org/graph/v1/paper/search\"\n",
        "            params = {\n",
        "                'query': query,\n",
        "                'limit': limit,\n",
        "                'fields': 'title,authors,abstract,year,url,citationCount,publicationDate,venue,externalIds'\n",
        "            }\n",
        "\n",
        "            # Retry logic for rate limiting\n",
        "            max_retries = 3\n",
        "            for attempt in range(max_retries):\n",
        "                data = self.safe_get(url, params)\n",
        "                time.sleep((attempt + 1) * 5)  # Exponential backoff\n",
        "                if data and 'data' in data:\n",
        "                    break\n",
        "                elif attempt < max_retries - 1:\n",
        "                    print(f\"‚ö†Ô∏è  Semantic Scholar rate limited, waiting {(attempt + 1) * 5} seconds...\")\n",
        "                else:\n",
        "                    print(\"‚ö†Ô∏è  Semantic Scholar API unavailable after retries\")\n",
        "                    return []\n",
        "\n",
        "            if not data or 'data' not in data:\n",
        "                return []\n",
        "\n",
        "            papers = []\n",
        "            for paper in data['data']:\n",
        "                try:\n",
        "                    paper_data = {\n",
        "                        'title': self.safe_string(paper.get('title', '')),\n",
        "                        'authors': [author.get('name', '') for author in paper.get('authors', [])],\n",
        "                        'abstract': self.safe_string(paper.get('abstract', '')),\n",
        "                        'year': paper.get('year'),\n",
        "                        'url': paper.get('url', ''),\n",
        "                        'source': 'Semantic Scholar',\n",
        "                        'citation_count': paper.get('citationCount', 0),\n",
        "                        'venue': self.safe_string(paper.get('venue', '')),\n",
        "                        'publication_date': self.safe_string(paper.get('publicationDate', '')),\n",
        "                        'content': self.safe_string(paper.get('abstract', '')),\n",
        "                        'external_ids': paper.get('externalIds', {})\n",
        "                    }\n",
        "                    papers.append(paper_data)\n",
        "                except Exception as e:\n",
        "                    print(f\"‚ö†Ô∏è  Error processing Semantic Scholar paper: {e}\")\n",
        "                    continue\n",
        "\n",
        "            print(f\"‚úÖ Found {len(papers)} papers on Semantic Scholar\")\n",
        "            return papers\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error searching Semantic Scholar: {e}\")\n",
        "            return []\n",
        "\n",
        "    def search_crossref(self, query: str, limit: int = 10) -> List[Dict]:\n",
        "        \"\"\"Search CrossRef API\"\"\"\n",
        "        try:\n",
        "            url = \"https://api.crossref.org/works\"\n",
        "            params = {\n",
        "                'query': query,\n",
        "                'rows': limit,\n",
        "                'select': 'title,author,abstract,published-print,published-online,URL,DOI,publisher,container-title'\n",
        "            }\n",
        "\n",
        "            data = self.safe_get(url, params)\n",
        "            if not data or 'message' not in data or 'items' not in data['message']:\n",
        "                return []\n",
        "\n",
        "            papers = []\n",
        "            for item in data['message']['items']:\n",
        "                try:\n",
        "                    # Extract authors\n",
        "                    authors = []\n",
        "                    for author in item.get('author', []):\n",
        "                        if 'given' in author and 'family' in author:\n",
        "                            authors.append(f\"{author['given']} {author['family']}\")\n",
        "                        elif 'name' in author:\n",
        "                            authors.append(author['name'])\n",
        "\n",
        "                    # Extract publication year\n",
        "                    year = None\n",
        "                    if 'published-print' in item:\n",
        "                        year = item['published-print']['date-parts'][0][0]\n",
        "                    elif 'published-online' in item:\n",
        "                        year = item['published-online']['date-parts'][0][0]\n",
        "\n",
        "                    paper_data = {\n",
        "                        'title': self.safe_string(item.get('title', [''])[0]),\n",
        "                        'authors': authors,\n",
        "                        'abstract': self.safe_string(item.get('abstract', '')),\n",
        "                        'year': year,\n",
        "                        'url': item.get('URL', ''),\n",
        "                        'doi': item.get('DOI', ''),\n",
        "                        'publisher': self.safe_string(item.get('publisher', '')),\n",
        "                        'journal': self.safe_string(item.get('container-title', [''])[0]),\n",
        "                        'source': 'CrossRef',\n",
        "                        'content': self.safe_string(item.get('abstract', ''))\n",
        "                    }\n",
        "                    papers.append(paper_data)\n",
        "                except Exception as e:\n",
        "                    print(f\"‚ö†Ô∏è  Error processing CrossRef paper: {e}\")\n",
        "                    continue\n",
        "\n",
        "            print(f\"‚úÖ Found {len(papers)} papers on CrossRef\")\n",
        "            return papers\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error searching CrossRef: {e}\")\n",
        "            return []\n",
        "\n",
        "    def search_pubmed(self, query: str, limit: int = 10) -> List[Dict]:\n",
        "        \"\"\"Search PubMed via NCBI E-utilities\"\"\"\n",
        "        try:\n",
        "            # Search for paper IDs\n",
        "            search_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\"\n",
        "            search_params = {\n",
        "                'db': 'pubmed',\n",
        "                'term': query,\n",
        "                'retmax': limit,\n",
        "                'retmode': 'json'\n",
        "            }\n",
        "\n",
        "            search_data = self.safe_get(search_url, search_params)\n",
        "            if not search_data or 'esearchresult' not in search_data:\n",
        "                return []\n",
        "\n",
        "            ids = search_data['esearchresult'].get('idlist', [])\n",
        "            if not ids:\n",
        "                return []\n",
        "\n",
        "            # Fetch paper details\n",
        "            fetch_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi\"\n",
        "            fetch_params = {\n",
        "                'db': 'pubmed',\n",
        "                'id': ','.join(ids),\n",
        "                'retmode': 'xml'\n",
        "            }\n",
        "\n",
        "            response = self.session.get(fetch_url, params=fetch_params)\n",
        "            if response.status_code != 200:\n",
        "                return []\n",
        "\n",
        "            if not WEB_SCRAPING_AVAILABLE:\n",
        "                print(\"‚ö†Ô∏è  BeautifulSoup not available for PubMed XML parsing\")\n",
        "                return []\n",
        "\n",
        "            # Parse XML\n",
        "            soup = BeautifulSoup(response.content, 'xml')\n",
        "            papers = []\n",
        "\n",
        "            for article in soup.find_all('PubmedArticle'):\n",
        "                try:\n",
        "                    medline = article.find('MedlineCitation')\n",
        "                    if not medline:\n",
        "                        continue\n",
        "\n",
        "                    # Extract basic info\n",
        "                    title_elem = medline.find('ArticleTitle')\n",
        "                    title = title_elem.text if title_elem else 'Unknown Title'\n",
        "\n",
        "                    abstract_elem = medline.find('Abstract')\n",
        "                    abstract = ''\n",
        "                    if abstract_elem:\n",
        "                        abstract_texts = abstract_elem.find_all('AbstractText')\n",
        "                        abstract = ' '.join([t.text for t in abstract_texts])\n",
        "\n",
        "                    # Extract authors\n",
        "                    authors = []\n",
        "                    author_list = medline.find('AuthorList')\n",
        "                    if author_list:\n",
        "                        for author in author_list.find_all('Author'):\n",
        "                            first_name = author.find('ForeName')\n",
        "                            last_name = author.find('LastName')\n",
        "                            if first_name and last_name:\n",
        "                                authors.append(f\"{first_name.text} {last_name.text}\")\n",
        "\n",
        "                    # Extract publication year\n",
        "                    year = None\n",
        "                    pub_date = medline.find('PubDate')\n",
        "                    if pub_date:\n",
        "                        year_elem = pub_date.find('Year')\n",
        "                        if year_elem:\n",
        "                            year = int(year_elem.text)\n",
        "\n",
        "                    # Extract PMID\n",
        "                    pmid_elem = medline.find('PMID')\n",
        "                    pmid = pmid_elem.text if pmid_elem else ''\n",
        "\n",
        "                    paper_data = {\n",
        "                        'title': title,\n",
        "                        'authors': authors,\n",
        "                        'abstract': abstract,\n",
        "                        'year': year,\n",
        "                        'url': f\"https://pubmed.ncbi.nlm.nih.gov/{pmid}/\" if pmid else '',\n",
        "                        'pmid': pmid,\n",
        "                        'source': 'PubMed',\n",
        "                        'content': abstract\n",
        "                    }\n",
        "                    papers.append(paper_data)\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"‚ö†Ô∏è  Error processing PubMed paper: {e}\")\n",
        "                    continue\n",
        "\n",
        "            print(f\"‚úÖ Found {len(papers)} papers on PubMed\")\n",
        "            return papers\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error searching PubMed: {e}\")\n",
        "            return []\n",
        "\n",
        "    def search_all_sources(self, query: str, max_per_source: int = 5) -> List[Dict]:\n",
        "        \"\"\"Search all available sources\"\"\"\n",
        "        print(f\"üîç Searching all sources for: '{query}'\")\n",
        "\n",
        "        all_papers = []\n",
        "\n",
        "        # Search each source\n",
        "        sources = [\n",
        "            (\"arXiv\", lambda: self.arxiv_fetcher.search_papers(query, max_per_source)),\n",
        "            (\"Semantic Scholar\", lambda: self.search_semantic_scholar(query, max_per_source)),\n",
        "            (\"CrossRef\", lambda: self.search_crossref(query, max_per_source)),\n",
        "            (\"PubMed\", lambda: self.search_pubmed(query, max_per_source))\n",
        "        ]\n",
        "\n",
        "        for source_name, search_func in sources:\n",
        "            try:\n",
        "                print(f\"üîç Searching {source_name}...\")\n",
        "                papers = search_func()\n",
        "                all_papers.extend(papers)\n",
        "                # Longer delay for Semantic Scholar to avoid rate limiting\n",
        "                if source_name == \"Semantic Scholar\":\n",
        "                    time.sleep(3)  # Longer delay for Semantic Scholar\n",
        "                else:\n",
        "                    time.sleep(1)  # Standard delay for other APIs\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è  Error searching {source_name}: {e}\")\n",
        "                continue\n",
        "\n",
        "        # Deduplicate based on title similarity\n",
        "        unique_papers = self.deduplicate_papers(all_papers)\n",
        "\n",
        "        print(f\"‚úÖ Total unique papers found: {len(unique_papers)}\")\n",
        "        return unique_papers\n",
        "\n",
        "    def deduplicate_papers(self, papers: List[Dict]) -> List[Dict]:\n",
        "        \"\"\"Remove duplicate papers based on title similarity\"\"\"\n",
        "        if not papers:\n",
        "            return papers\n",
        "\n",
        "        unique_papers = []\n",
        "        seen_titles = set()\n",
        "\n",
        "        for paper in papers:\n",
        "            title = paper.get('title', '').lower().strip()\n",
        "            if not title:\n",
        "                continue\n",
        "\n",
        "            # Simple deduplication based on title\n",
        "            title_words = set(title.split())\n",
        "            is_duplicate = False\n",
        "\n",
        "            for seen_title in seen_titles:\n",
        "                seen_words = set(seen_title.split())\n",
        "                if len(title_words.intersection(seen_words)) / len(title_words.union(seen_words)) > 0.8:\n",
        "                    is_duplicate = True\n",
        "                    break\n",
        "\n",
        "            if not is_duplicate:\n",
        "                unique_papers.append(paper)\n",
        "                seen_titles.add(title)\n",
        "\n",
        "        return unique_papers\n",
        "\n",
        "# Initialize multi-source collector\n",
        "multi_source_collector = MultiSourceCollector()\n",
        "print(\"‚úÖ Multi-source data collector ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "a2bcea7e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2bcea7e",
        "outputId": "b40285f9-b8ed-4535-e791-cd7cd0f1a3e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ PDF processor initialized with methods: pdfplumber, fitz, PyPDF2\n",
            "‚úÖ Enhanced PDF processor ready!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# ENHANCED PDF PROCESSING\n",
        "# ============================================================================\n",
        "\n",
        "class EnhancedPDFProcessor:\n",
        "    \"\"\"Advanced PDF processing with multiple extraction methods\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.methods = []\n",
        "        if PDF_ENHANCED:\n",
        "            self.methods.extend(['pdfplumber', 'fitz'])\n",
        "        self.methods.append('PyPDF2')\n",
        "        print(f\"‚úÖ PDF processor initialized with methods: {', '.join(self.methods)}\")\n",
        "\n",
        "    def extract_text_pypdf2(self, pdf_path: str) -> Dict[str, Any]:\n",
        "        \"\"\"Extract text using PyPDF2\"\"\"\n",
        "        try:\n",
        "            text = \"\"\n",
        "            metadata = {}\n",
        "\n",
        "            with open(pdf_path, 'rb') as file:\n",
        "                pdf_reader = PyPDF2.PdfReader(file)\n",
        "\n",
        "                # Extract metadata\n",
        "                if pdf_reader.metadata:\n",
        "                    metadata = {\n",
        "                        'title': pdf_reader.metadata.get('/Title', ''),\n",
        "                        'author': pdf_reader.metadata.get('/Author', ''),\n",
        "                        'subject': pdf_reader.metadata.get('/Subject', ''),\n",
        "                        'creator': pdf_reader.metadata.get('/Creator', ''),\n",
        "                        'producer': pdf_reader.metadata.get('/Producer', ''),\n",
        "                        'creation_date': str(pdf_reader.metadata.get('/CreationDate', '')),\n",
        "                        'modification_date': str(pdf_reader.metadata.get('/ModDate', ''))\n",
        "                    }\n",
        "\n",
        "                # Extract text from all pages\n",
        "                for page_num in range(len(pdf_reader.pages)):\n",
        "                    page = pdf_reader.pages[page_num]\n",
        "                    text += page.extract_text() + \"\\n\\n\"\n",
        "\n",
        "            return {\n",
        "                'text': text.strip(),\n",
        "                'metadata': metadata,\n",
        "                'pages': len(pdf_reader.pages),\n",
        "                'method': 'PyPDF2',\n",
        "                'success': True\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                'text': '',\n",
        "                'metadata': {},\n",
        "                'pages': 0,\n",
        "                'method': 'PyPDF2',\n",
        "                'success': False,\n",
        "                'error': str(e)\n",
        "            }\n",
        "\n",
        "    def extract_text_pdfplumber(self, pdf_path: str) -> Dict[str, Any]:\n",
        "        \"\"\"Extract text using pdfplumber (better formatting)\"\"\"\n",
        "        if not PDF_ENHANCED:\n",
        "            return {'success': False, 'error': 'pdfplumber not available'}\n",
        "\n",
        "        try:\n",
        "            text = \"\"\n",
        "            tables = []\n",
        "\n",
        "            with pdfplumber.open(pdf_path) as pdf:\n",
        "                for page_num, page in enumerate(pdf.pages):\n",
        "                    # Extract text with better formatting\n",
        "                    page_text = page.extract_text()\n",
        "                    if page_text:\n",
        "                        text += f\"--- Page {page_num + 1} ---\\n\"\n",
        "                        text += page_text + \"\\n\\n\"\n",
        "\n",
        "                    # Extract tables if present\n",
        "                    page_tables = page.extract_tables()\n",
        "                    if page_tables:\n",
        "                        for table in page_tables:\n",
        "                            tables.append({\n",
        "                                'page': page_num + 1,\n",
        "                                'data': table\n",
        "                            })\n",
        "\n",
        "            return {\n",
        "                'text': text.strip(),\n",
        "                'tables': tables,\n",
        "                'pages': len(pdf.pages),\n",
        "                'method': 'pdfplumber',\n",
        "                'success': True\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                'text': '',\n",
        "                'tables': [],\n",
        "                'pages': 0,\n",
        "                'method': 'pdfplumber',\n",
        "                'success': False,\n",
        "                'error': str(e)\n",
        "            }\n",
        "\n",
        "    def extract_text_fitz(self, pdf_path: str) -> Dict[str, Any]:\n",
        "        \"\"\"Extract text using PyMuPDF (fitz) - best for complex layouts\"\"\"\n",
        "        if not PDF_ENHANCED:\n",
        "            return {'success': False, 'error': 'PyMuPDF not available'}\n",
        "\n",
        "        try:\n",
        "            text = \"\"\n",
        "            doc = fitz.open(pdf_path)\n",
        "\n",
        "            # Extract metadata\n",
        "            metadata = doc.metadata\n",
        "\n",
        "            # Extract text from all pages\n",
        "            for page_num in range(len(doc)):\n",
        "                page = doc.load_page(page_num)\n",
        "                page_text = page.get_text()\n",
        "                if page_text.strip():\n",
        "                    text += f\"--- Page {page_num + 1} ---\\n\"\n",
        "                    text += page_text + \"\\n\\n\"\n",
        "\n",
        "            doc.close()\n",
        "\n",
        "            return {\n",
        "                'text': text.strip(),\n",
        "                'metadata': metadata,\n",
        "                'pages': len(doc),\n",
        "                'method': 'PyMuPDF',\n",
        "                'success': True\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                'text': '',\n",
        "                'metadata': {},\n",
        "                'pages': 0,\n",
        "                'method': 'PyMuPDF',\n",
        "                'success': False,\n",
        "                'error': str(e)\n",
        "            }\n",
        "\n",
        "    def extract_text_best_method(self, pdf_path: str) -> Dict[str, Any]:\n",
        "        \"\"\"Try multiple methods and return the best result\"\"\"\n",
        "        methods = [\n",
        "            ('pdfplumber', self.extract_text_pdfplumber),\n",
        "            ('fitz', self.extract_text_fitz),\n",
        "            ('PyPDF2', self.extract_text_pypdf2)\n",
        "        ]\n",
        "\n",
        "        best_result = None\n",
        "        best_score = 0\n",
        "\n",
        "        for method_name, method_func in methods:\n",
        "            if method_name not in self.methods:\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                result = method_func(pdf_path)\n",
        "                if result['success']:\n",
        "                    # Score based on text length and quality\n",
        "                    text_length = len(result['text'])\n",
        "                    score = text_length\n",
        "\n",
        "                    # Bonus for having metadata\n",
        "                    if 'metadata' in result and result['metadata']:\n",
        "                        score += 1000\n",
        "\n",
        "                    # Bonus for having tables (pdfplumber)\n",
        "                    if 'tables' in result and result['tables']:\n",
        "                        score += 500\n",
        "\n",
        "                    if score > best_score:\n",
        "                        best_score = score\n",
        "                        best_result = result\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è  Method {method_name} failed: {e}\")\n",
        "                continue\n",
        "\n",
        "        if best_result:\n",
        "            print(f\"‚úÖ Best extraction method: {best_result['method']}\")\n",
        "            return best_result\n",
        "        else:\n",
        "            return {\n",
        "                'text': '',\n",
        "                'metadata': {},\n",
        "                'pages': 0,\n",
        "                'method': 'none',\n",
        "                'success': False,\n",
        "                'error': 'All extraction methods failed'\n",
        "            }\n",
        "\n",
        "    def identify_sections(self, text: str) -> Dict[str, str]:\n",
        "        \"\"\"Identify paper sections using AI\"\"\"\n",
        "        try:\n",
        "            # Use Groq to identify sections\n",
        "            prompt = f\"\"\"Analyze this research paper text and identify the main sections.\n",
        "            Extract the following sections if they exist:\n",
        "            1. Abstract\n",
        "            2. Introduction\n",
        "            3. Methodology/Methods\n",
        "            4. Results\n",
        "            5. Discussion\n",
        "            6. Conclusion\n",
        "            7. References\n",
        "\n",
        "            For each section found, provide the section name and its content.\n",
        "            If a section is not found, return \"Not found\" for that section.\n",
        "\n",
        "            Paper text (first 8000 characters):\n",
        "            {text[:8000]}\n",
        "\n",
        "            Please format your response as:\n",
        "            ABSTRACT: [content or \"Not found\"]\n",
        "            INTRODUCTION: [content or \"Not found\"]\n",
        "            METHODOLOGY: [content or \"Not found\"]\n",
        "            RESULTS: [content or \"Not found\"]\n",
        "            DISCUSSION: [content or \"Not found\"]\n",
        "            CONCLUSION: [content or \"Not found\"]\n",
        "            REFERENCES: [content or \"Not found\"]\"\"\"\n",
        "\n",
        "            response = groq_processor.generate_response(prompt, max_tokens=2000)\n",
        "\n",
        "            # Parse response\n",
        "            sections = {\n",
        "                'abstract': '',\n",
        "                'introduction': '',\n",
        "                'methodology': '',\n",
        "                'results': '',\n",
        "                'discussion': '',\n",
        "                'conclusion': '',\n",
        "                'references': ''\n",
        "            }\n",
        "\n",
        "            lines = response.split('\\n')\n",
        "            current_section = None\n",
        "\n",
        "            for line in lines:\n",
        "                line = line.strip()\n",
        "                if not line:\n",
        "                    continue\n",
        "\n",
        "                # Check for section headers\n",
        "                if line.startswith('ABSTRACT:'):\n",
        "                    current_section = 'abstract'\n",
        "                    sections[current_section] = line[9:].strip()\n",
        "                elif line.startswith('INTRODUCTION:'):\n",
        "                    current_section = 'introduction'\n",
        "                    sections[current_section] = line[13:].strip()\n",
        "                elif line.startswith('METHODOLOGY:'):\n",
        "                    current_section = 'methodology'\n",
        "                    sections[current_section] = line[12:].strip()\n",
        "                elif line.startswith('RESULTS:'):\n",
        "                    current_section = 'results'\n",
        "                    sections[current_section] = line[8:].strip()\n",
        "                elif line.startswith('DISCUSSION:'):\n",
        "                    current_section = 'discussion'\n",
        "                    sections[current_section] = line[11:].strip()\n",
        "                elif line.startswith('CONCLUSION:'):\n",
        "                    current_section = 'conclusion'\n",
        "                    sections[current_section] = line[11:].strip()\n",
        "                elif line.startswith('REFERENCES:'):\n",
        "                    current_section = 'references'\n",
        "                    sections[current_section] = line[11:].strip()\n",
        "                elif current_section and not line.startswith(('ABSTRACT:', 'INTRODUCTION:', 'METHODOLOGY:', 'RESULTS:', 'DISCUSSION:', 'CONCLUSION:', 'REFERENCES:')):\n",
        "                    # Continue adding to current section\n",
        "                    sections[current_section] += ' ' + line\n",
        "\n",
        "            return sections\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error identifying sections: {e}\")\n",
        "            return {\n",
        "                'abstract': 'Error extracting sections',\n",
        "                'introduction': 'Error extracting sections',\n",
        "                'methodology': 'Error extracting sections',\n",
        "                'results': 'Error extracting sections',\n",
        "                'discussion': 'Error extracting sections',\n",
        "                'conclusion': 'Error extracting sections',\n",
        "                'references': 'Error extracting sections'\n",
        "            }\n",
        "\n",
        "    def process_pdf_file(self, pdf_path: str) -> Dict[str, Any]:\n",
        "        \"\"\"Complete PDF processing pipeline\"\"\"\n",
        "        try:\n",
        "            if not os.path.exists(pdf_path):\n",
        "                return {\n",
        "                    'success': False,\n",
        "                    'error': f'PDF file not found: {pdf_path}'\n",
        "                }\n",
        "\n",
        "            print(f\"üìÑ Processing PDF: {pdf_path}\")\n",
        "\n",
        "            # Extract text using best method\n",
        "            extraction_result = self.extract_text_best_method(pdf_path)\n",
        "\n",
        "            if not extraction_result['success']:\n",
        "                return extraction_result\n",
        "\n",
        "            # Identify sections\n",
        "            sections = self.identify_sections(extraction_result['text'])\n",
        "\n",
        "            # Generate summary\n",
        "            summary = groq_processor.summarize_paper(\n",
        "                title=extraction_result.get('metadata', {}).get('title', 'PDF Document'),\n",
        "                abstract=sections.get('abstract', ''),\n",
        "                content=extraction_result['text']\n",
        "            )\n",
        "\n",
        "            return {\n",
        "                'success': True,\n",
        "                'file_path': pdf_path,\n",
        "                'text': extraction_result['text'],\n",
        "                'metadata': extraction_result.get('metadata', {}),\n",
        "                'pages': extraction_result.get('pages', 0),\n",
        "                'extraction_method': extraction_result.get('method', 'unknown'),\n",
        "                'sections': sections,\n",
        "                'summary': summary,\n",
        "                'tables': extraction_result.get('tables', []),\n",
        "                'processed_at': datetime.now().isoformat()\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                'success': False,\n",
        "                'error': str(e),\n",
        "                'file_path': pdf_path\n",
        "            }\n",
        "\n",
        "    def process_multiple_pdfs(self, pdf_paths: List[str]) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Process multiple PDF files\"\"\"\n",
        "        results = []\n",
        "\n",
        "        for pdf_path in pdf_paths:\n",
        "            result = self.process_pdf_file(pdf_path)\n",
        "            results.append(result)\n",
        "\n",
        "            # Add to RAG system if successful\n",
        "            if result['success'] and rag_system:\n",
        "                try:\n",
        "                    doc_data = {\n",
        "                        'title': result['metadata'].get('title', f'PDF: {os.path.basename(pdf_path)}'),\n",
        "                        'content': result['text'],\n",
        "                        'authors': result['metadata'].get('author', 'Unknown'),\n",
        "                        'source': 'PDF',\n",
        "                        'url': f'file://{pdf_path}',\n",
        "                        'year': 'Unknown'\n",
        "                    }\n",
        "                    rag_system.add_documents([doc_data])\n",
        "                    print(f\"‚úÖ Added PDF to RAG system: {doc_data['title']}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"‚ö†Ô∏è  Failed to add PDF to RAG system: {e}\")\n",
        "\n",
        "        return results\n",
        "\n",
        "# Initialize PDF processor\n",
        "pdf_processor = EnhancedPDFProcessor()\n",
        "print(\"‚úÖ Enhanced PDF processor ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "0a08c284",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0a08c284",
        "outputId": "5ea83edc-21eb-470b-dad9-eeb316cea552"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîÑ Citation network analyzer reset\n",
            "‚úÖ Citation network analyzer initialized (robust version)!\n",
            "‚úÖ Citation network analyzer ready (robust version)!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CITATION NETWORK ANALYSIS - COMPLETELY REWRITTEN VERSION\n",
        "# ============================================================================\n",
        "\n",
        "import networkx as nx\n",
        "import json\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Any\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class CitationNetworkAnalyzer:\n",
        "    \"\"\"Analyze citation networks and author collaborations - Robust Version\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "        print(\"‚úÖ Citation network analyzer initialized (robust version)!\")\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset all data structures\"\"\"\n",
        "        self.citation_graph = nx.DiGraph()\n",
        "        self.author_graph = nx.Graph()\n",
        "        self.paper_data = {}\n",
        "        self.author_data = {}  # Separate storage for author data\n",
        "        print(\"üîÑ Citation network analyzer reset\")\n",
        "\n",
        "    def _safe_get_authors(self, paper: Dict) -> List[str]:\n",
        "        \"\"\"Safely extract and normalize author list from paper\"\"\"\n",
        "        authors = paper.get('authors', [])\n",
        "\n",
        "        # Handle None\n",
        "        if authors is None:\n",
        "            return []\n",
        "\n",
        "        # Handle string (comma-separated)\n",
        "        if isinstance(authors, str):\n",
        "            if not authors.strip():\n",
        "                return []\n",
        "            return [a.strip() for a in authors.split(',') if a.strip()]\n",
        "\n",
        "        # Handle list\n",
        "        if isinstance(authors, list):\n",
        "            result = []\n",
        "            for author in authors:\n",
        "                if isinstance(author, str) and author.strip():\n",
        "                    result.append(author.strip())\n",
        "                elif isinstance(author, dict):\n",
        "                    # Handle author objects with 'name' field\n",
        "                    name = author.get('name', '') or author.get('authorId', '')\n",
        "                    if name and isinstance(name, str):\n",
        "                        result.append(name.strip())\n",
        "            return result\n",
        "\n",
        "        # Unknown format\n",
        "        return []\n",
        "\n",
        "    def _safe_add_author(self, author_name: str, paper_id: str, citation_count: int = 0):\n",
        "        \"\"\"Safely add author to the graph\"\"\"\n",
        "        try:\n",
        "            # Initialize author data if not exists\n",
        "            if author_name not in self.author_data:\n",
        "                self.author_data[author_name] = {\n",
        "                    'papers': [],\n",
        "                    'total_citations': 0\n",
        "                }\n",
        "\n",
        "            # Add to NetworkX graph if not exists\n",
        "            if not self.author_graph.has_node(author_name):\n",
        "                self.author_graph.add_node(author_name)\n",
        "\n",
        "            # Update author data\n",
        "            if paper_id not in self.author_data[author_name]['papers']:\n",
        "                self.author_data[author_name]['papers'].append(paper_id)\n",
        "                self.author_data[author_name]['total_citations'] += citation_count\n",
        "\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è  Error adding author {author_name}: {e}\")\n",
        "            return False\n",
        "\n",
        "    def _safe_add_collaboration(self, author1: str, author2: str, paper_id: str):\n",
        "        \"\"\"Safely add collaboration edge between authors\"\"\"\n",
        "        try:\n",
        "            # Ensure both authors exist\n",
        "            if not self.author_graph.has_node(author1):\n",
        "                self.author_graph.add_node(author1)\n",
        "            if not self.author_graph.has_node(author2):\n",
        "                self.author_graph.add_node(author2)\n",
        "\n",
        "            # Add or update edge\n",
        "            if self.author_graph.has_edge(author1, author2):\n",
        "                # Update existing edge\n",
        "                edge_data = self.author_graph.edges[author1, author2]\n",
        "                edge_data['weight'] = edge_data.get('weight', 0) + 1\n",
        "                if 'papers' not in edge_data:\n",
        "                    edge_data['papers'] = []\n",
        "                if paper_id not in edge_data['papers']:\n",
        "                    edge_data['papers'].append(paper_id)\n",
        "            else:\n",
        "                # Add new edge\n",
        "                self.author_graph.add_edge(author1, author2, weight=1, papers=[paper_id])\n",
        "\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è  Error adding collaboration {author1}-{author2}: {e}\")\n",
        "            return False\n",
        "\n",
        "    def add_papers(self, papers: List[Dict]):\n",
        "        \"\"\"Add papers to the citation network - Robust Version\"\"\"\n",
        "        if not papers:\n",
        "            print(\"‚ö†Ô∏è  No papers provided to add_papers\")\n",
        "            return\n",
        "\n",
        "        processed_count = 0\n",
        "        error_count = 0\n",
        "\n",
        "        print(f\"üìù Processing {len(papers)} papers...\")\n",
        "\n",
        "        for paper_idx, paper in enumerate(papers):\n",
        "            try:\n",
        "                # Validate paper input\n",
        "                if not isinstance(paper, dict):\n",
        "                    print(f\"‚ö†Ô∏è  Paper {paper_idx} is not a dict: {type(paper)}\")\n",
        "                    error_count += 1\n",
        "                    continue\n",
        "\n",
        "                # Generate paper ID\n",
        "                paper_id = paper.get('paper_id')\n",
        "                if not paper_id:\n",
        "                    paper_id = paper.get('url', '')\n",
        "                    if not paper_id:\n",
        "                        title = paper.get('title', f'Unknown_{paper_idx}')\n",
        "                        paper_id = f\"paper_{abs(hash(title)) % 1000000}\"\n",
        "\n",
        "                # Store paper data\n",
        "                self.paper_data[paper_id] = {\n",
        "                    'title': paper.get('title', ''),\n",
        "                    'authors': self._safe_get_authors(paper),\n",
        "                    'year': paper.get('year'),\n",
        "                    'venue': paper.get('venue', ''),\n",
        "                    'citation_count': paper.get('citation_count', 0),\n",
        "                    'source': paper.get('source', ''),\n",
        "                    'url': paper.get('url', ''),\n",
        "                    'abstract': paper.get('abstract', '')\n",
        "                }\n",
        "\n",
        "                # Add to citation graph\n",
        "                self.citation_graph.add_node(paper_id, **self.paper_data[paper_id])\n",
        "\n",
        "                # Process authors\n",
        "                authors = self._safe_get_authors(paper)\n",
        "                citation_count = paper.get('citation_count', 0)\n",
        "\n",
        "                # Validate citation count\n",
        "                if not isinstance(citation_count, (int, float)):\n",
        "                    citation_count = 0\n",
        "\n",
        "                # Add authors\n",
        "                valid_authors = []\n",
        "                for author in authors:\n",
        "                    if self._safe_add_author(author, paper_id, citation_count):\n",
        "                        valid_authors.append(author)\n",
        "\n",
        "                # Add collaborations\n",
        "                for i, author1 in enumerate(valid_authors):\n",
        "                    for j, author2 in enumerate(valid_authors):\n",
        "                        if i < j:  # Avoid duplicates and self-loops\n",
        "                            self._safe_add_collaboration(author1, author2, paper_id)\n",
        "\n",
        "                processed_count += 1\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è  Error processing paper {paper_idx}: {e}\")\n",
        "                error_count += 1\n",
        "                continue\n",
        "\n",
        "        print(f\"‚úÖ Successfully processed {processed_count} papers ({error_count} errors)\")\n",
        "        print(f\"üìä Total papers: {len(self.paper_data)}\")\n",
        "        print(f\"üìä Total authors: {len(self.author_data)}\")\n",
        "        print(f\"üìä Author graph nodes: {len(self.author_graph.nodes)}\")\n",
        "\n",
        "    def analyze_author_network(self) -> Dict:\n",
        "        \"\"\"Analyze author collaboration network\"\"\"\n",
        "        try:\n",
        "            if len(self.author_graph.nodes) == 0:\n",
        "                return {'error': 'No authors in network'}\n",
        "\n",
        "            # Basic network metrics\n",
        "            metrics = {\n",
        "                'total_authors': len(self.author_graph.nodes),\n",
        "                'total_collaborations': len(self.author_graph.edges),\n",
        "                'network_density': nx.density(self.author_graph),\n",
        "                'number_of_components': nx.number_connected_components(self.author_graph),\n",
        "                'largest_component_size': len(max(nx.connected_components(self.author_graph), key=len)) if nx.number_connected_components(self.author_graph) > 0 else 0\n",
        "            }\n",
        "\n",
        "            # Most collaborative authors\n",
        "            collaboration_counts = {node: self.author_graph.degree(node) for node in self.author_graph.nodes}\n",
        "            top_collaborators = sorted(collaboration_counts.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "\n",
        "            # Most productive authors (using separate author_data)\n",
        "            productivity = {}\n",
        "            for author, data in self.author_data.items():\n",
        "                productivity[author] = len(data.get('papers', []))\n",
        "            top_productive = sorted(productivity.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "\n",
        "            # Most cited authors\n",
        "            citation_counts = {}\n",
        "            for author, data in self.author_data.items():\n",
        "                citation_counts[author] = data.get('total_citations', 0)\n",
        "            top_cited = sorted(citation_counts.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "\n",
        "            # Central authors (betweenness centrality)\n",
        "            try:\n",
        "                if len(self.author_graph.nodes) > 1:\n",
        "                    centrality = nx.betweenness_centrality(self.author_graph)\n",
        "                    top_central = sorted(centrality.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "                else:\n",
        "                    top_central = []\n",
        "            except Exception as centrality_error:\n",
        "                print(f\"‚ö†Ô∏è  Error calculating centrality: {centrality_error}\")\n",
        "                top_central = []\n",
        "\n",
        "            return {\n",
        "                'network_metrics': metrics,\n",
        "                'top_collaborators': top_collaborators,\n",
        "                'top_productive_authors': top_productive,\n",
        "                'top_cited_authors': top_cited,\n",
        "                'top_central_authors': top_central,\n",
        "                'analysis_timestamp': datetime.now().isoformat()\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                'error': str(e),\n",
        "                'analysis_timestamp': datetime.now().isoformat()\n",
        "            }\n",
        "\n",
        "    def analyze_paper_network(self) -> Dict:\n",
        "        \"\"\"Analyze paper citation network\"\"\"\n",
        "        try:\n",
        "            if len(self.citation_graph.nodes) == 0:\n",
        "                return {'error': 'No papers in network'}\n",
        "\n",
        "            # Basic network metrics\n",
        "            metrics = {\n",
        "                'total_papers': len(self.citation_graph.nodes),\n",
        "                'total_citations': len(self.citation_graph.edges),\n",
        "                'network_density': nx.density(self.citation_graph),\n",
        "                'number_of_components': nx.number_weakly_connected_components(self.citation_graph),\n",
        "                'largest_component_size': len(max(nx.weakly_connected_components(self.citation_graph), key=len)) if nx.number_weakly_connected_components(self.citation_graph) > 0 else 0\n",
        "            }\n",
        "\n",
        "            # Most cited papers\n",
        "            in_degree = dict(self.citation_graph.in_degree())\n",
        "            most_cited = sorted(in_degree.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "\n",
        "            # Most citing papers\n",
        "            out_degree = dict(self.citation_graph.out_degree())\n",
        "            most_citing = sorted(out_degree.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "\n",
        "            # Convert paper IDs to titles for readability\n",
        "            most_cited_titles = []\n",
        "            for paper_id, count in most_cited:\n",
        "                if paper_id in self.paper_data:\n",
        "                    most_cited_titles.append((self.paper_data[paper_id]['title'], count))\n",
        "                else:\n",
        "                    most_cited_titles.append((paper_id, count))\n",
        "\n",
        "            most_citing_titles = []\n",
        "            for paper_id, count in most_citing:\n",
        "                if paper_id in self.paper_data:\n",
        "                    most_citing_titles.append((self.paper_data[paper_id]['title'], count))\n",
        "                else:\n",
        "                    most_citing_titles.append((paper_id, count))\n",
        "\n",
        "            return {\n",
        "                'network_metrics': metrics,\n",
        "                'most_cited_papers': most_cited_titles,\n",
        "                'most_citing_papers': most_citing_titles,\n",
        "                'analysis_timestamp': datetime.now().isoformat()\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                'error': str(e),\n",
        "                'analysis_timestamp': datetime.now().isoformat()\n",
        "            }\n",
        "\n",
        "    def find_citation_relationships(self, papers: List[Dict]) -> Dict:\n",
        "        \"\"\"Find citation relationships between papers\"\"\"\n",
        "        try:\n",
        "            # This is a placeholder for AI-based analysis\n",
        "            # You would integrate with your groq_processor here\n",
        "            return {\n",
        "                'relationships': [],\n",
        "                'total_papers_analyzed': len(papers),\n",
        "                'analysis_method': 'AI-inferred',\n",
        "                'timestamp': datetime.now().isoformat(),\n",
        "                'note': 'AI analysis not implemented in this version'\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                'relationships': [],\n",
        "                'error': str(e),\n",
        "                'total_papers_analyzed': 0,\n",
        "                'analysis_method': 'AI-inferred',\n",
        "                'timestamp': datetime.now().isoformat()\n",
        "            }\n",
        "\n",
        "    def visualize_author_network(self, top_n: int = 20, save_path: str = None) -> str:\n",
        "        \"\"\"Visualize author collaboration network\"\"\"\n",
        "        try:\n",
        "            if len(self.author_graph.nodes) == 0:\n",
        "                return \"No authors to visualize\"\n",
        "\n",
        "            # Get top N most collaborative authors\n",
        "            collaboration_counts = {node: self.author_graph.degree(node) for node in self.author_graph.nodes}\n",
        "            top_authors = sorted(collaboration_counts.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
        "            top_author_names = [author for author, count in top_authors]\n",
        "\n",
        "            # Create subgraph\n",
        "            subgraph = self.author_graph.subgraph(top_author_names)\n",
        "\n",
        "            # Create visualization\n",
        "            plt.figure(figsize=(12, 8))\n",
        "\n",
        "            # Position nodes\n",
        "            pos = nx.spring_layout(subgraph, k=1, iterations=50)\n",
        "\n",
        "            # Draw nodes\n",
        "            node_sizes = [subgraph.degree(node) * 100 for node in subgraph.nodes]\n",
        "            nx.draw_networkx_nodes(subgraph, pos, node_size=node_sizes,\n",
        "                                 node_color='lightblue', alpha=0.7)\n",
        "\n",
        "            # Draw edges\n",
        "            edge_weights = [subgraph.edges[edge].get('weight', 1) for edge in subgraph.edges]\n",
        "            nx.draw_networkx_edges(subgraph, pos, width=edge_weights,\n",
        "                                 alpha=0.5, edge_color='gray')\n",
        "\n",
        "            # Draw labels\n",
        "            nx.draw_networkx_labels(subgraph, pos, font_size=8, font_weight='bold')\n",
        "\n",
        "            plt.title(f\"Author Collaboration Network (Top {top_n} Authors)\")\n",
        "            plt.axis('off')\n",
        "\n",
        "            # Save or show\n",
        "            if save_path:\n",
        "                plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "                plt.close()\n",
        "                return f\"Visualization saved to {save_path}\"\n",
        "            else:\n",
        "                plt.show()\n",
        "                return \"Visualization displayed\"\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"Error creating visualization: {str(e)}\"\n",
        "\n",
        "    def get_network_summary(self) -> Dict:\n",
        "        \"\"\"Get comprehensive network summary\"\"\"\n",
        "        try:\n",
        "            author_analysis = self.analyze_author_network()\n",
        "            paper_analysis = self.analyze_paper_network()\n",
        "\n",
        "            return {\n",
        "                'author_network': author_analysis,\n",
        "                'paper_network': paper_analysis,\n",
        "                'overall_stats': {\n",
        "                    'total_papers': len(self.paper_data),\n",
        "                    'total_authors': len(self.author_data),\n",
        "                    'papers_per_author': len(self.paper_data) / max(len(self.author_data), 1),\n",
        "                    'collaborations_per_author': len(self.author_graph.edges) / max(len(self.author_graph.nodes), 1)\n",
        "                },\n",
        "                'analysis_timestamp': datetime.now().isoformat()\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                'error': str(e),\n",
        "                'analysis_timestamp': datetime.now().isoformat()\n",
        "            }\n",
        "\n",
        "# Create a new instance to replace the old one\n",
        "def create_new_citation_analyzer():\n",
        "    \"\"\"Factory function to create a new citation analyzer\"\"\"\n",
        "    return CitationNetworkAnalyzer()\n",
        "\n",
        "# Initialize citation network analyzer\n",
        "citation_analyzer = create_new_citation_analyzer()\n",
        "print(\"‚úÖ Citation network analyzer ready (robust version)!\")\n",
        "\n",
        "# If you're using this in an existing system, you might want to:\n",
        "# globals()['citation_analyzer'] = create_new_citation_analyzer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "4106bbb7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4106bbb7",
        "outputId": "e6882a3c-ee85-4ecd-daff-02bcc395bf64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Research trend monitor initialized!\n",
            "‚úÖ Research trend monitor ready!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# RESEARCH TREND MONITORING\n",
        "# ============================================================================\n",
        "\n",
        "class ResearchTrendMonitor:\n",
        "    \"\"\"Monitor research trends and detect emerging topics\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.trend_data = {}\n",
        "        self.keyword_trends = defaultdict(list)\n",
        "        self.topic_evolution = {}\n",
        "        print(\"‚úÖ Research trend monitor initialized!\")\n",
        "\n",
        "    def analyze_temporal_trends(self, papers: List[Dict], timeframe: str = \"yearly\") -> Dict:\n",
        "        \"\"\"Analyze trends over time\"\"\"\n",
        "        try:\n",
        "            if not papers:\n",
        "                return {'error': 'No papers provided'}\n",
        "\n",
        "            # Group papers by time period\n",
        "            time_groups = defaultdict(list)\n",
        "\n",
        "            for paper in papers:\n",
        "                year = paper.get('year')\n",
        "                if not year:\n",
        "                    continue\n",
        "\n",
        "                if timeframe == \"yearly\":\n",
        "                    time_key = str(year)\n",
        "                elif timeframe == \"monthly\":\n",
        "                    # For monthly, we'd need more detailed date info\n",
        "                    time_key = str(year)\n",
        "                else:\n",
        "                    time_key = str(year)\n",
        "\n",
        "                time_groups[time_key].append(paper)\n",
        "\n",
        "            # Analyze trends for each time period\n",
        "            trend_analysis = {}\n",
        "\n",
        "            for time_key, period_papers in time_groups.items():\n",
        "                # Extract keywords and topics\n",
        "                all_text = ' '.join([\n",
        "                    (paper.get('title', '') + ' ' + paper.get('abstract', ''))\n",
        "                    for paper in period_papers\n",
        "                ])\n",
        "\n",
        "                keywords = self._extract_period_keywords(all_text)\n",
        "\n",
        "                # Count papers by category/source\n",
        "                categories = defaultdict(int)\n",
        "                sources = defaultdict(int)\n",
        "\n",
        "                for paper in period_papers:\n",
        "                    paper_categories = paper.get('categories', [])\n",
        "                    if isinstance(paper_categories, list):\n",
        "                        for cat in paper_categories:\n",
        "                            categories[cat] += 1\n",
        "                    elif isinstance(paper_categories, str):\n",
        "                        categories[paper_categories] += 1\n",
        "\n",
        "                    source = paper.get('source', 'Unknown')\n",
        "                    sources[source] += 1\n",
        "\n",
        "                trend_analysis[time_key] = {\n",
        "                    'paper_count': len(period_papers),\n",
        "                    'top_keywords': keywords[:20],\n",
        "                    'top_categories': dict(sorted(categories.items(), key=lambda x: x[1], reverse=True)[:10]),\n",
        "                    'sources': dict(sources),\n",
        "                    'avg_citations': np.mean([paper.get('citation_count', 0) for paper in period_papers])\n",
        "                }\n",
        "\n",
        "            # Identify emerging trends\n",
        "            emerging_trends = self._identify_emerging_trends(trend_analysis)\n",
        "\n",
        "            return {\n",
        "                'temporal_analysis': trend_analysis,\n",
        "                'emerging_trends': emerging_trends,\n",
        "                'timeframe': timeframe,\n",
        "                'total_papers': len(papers),\n",
        "                'analysis_timestamp': datetime.now().isoformat()\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                'error': str(e),\n",
        "                'analysis_timestamp': datetime.now().isoformat()\n",
        "            }\n",
        "\n",
        "    def _extract_period_keywords(self, text: str) -> List[str]:\n",
        "        \"\"\"Extract keywords from text for a specific period\"\"\"\n",
        "        # Clean and tokenize\n",
        "        words = re.findall(r'\\b[a-zA-Z]{3,}\\b', text.lower())\n",
        "\n",
        "        # Common stop words\n",
        "        stop_words = {\n",
        "            'the', 'and', 'for', 'are', 'with', 'this', 'that', 'from', 'they', 'have',\n",
        "            'been', 'was', 'were', 'will', 'would', 'could', 'should', 'can', 'may',\n",
        "            'paper', 'study', 'research', 'method', 'approach', 'results', 'conclusion',\n",
        "            'show', 'present', 'propose', 'analysis', 'experiments', 'data', 'based'\n",
        "        }\n",
        "\n",
        "        # Filter and count\n",
        "        word_counts = defaultdict(int)\n",
        "        for word in words:\n",
        "            if word not in stop_words and len(word) > 3:\n",
        "                word_counts[word] += 1\n",
        "\n",
        "        # Return top keywords\n",
        "        return [word for word, count in sorted(word_counts.items(), key=lambda x: x[1], reverse=True)]\n",
        "\n",
        "    def _identify_emerging_trends(self, trend_analysis: Dict) -> List[Dict]:\n",
        "        \"\"\"Identify emerging trends from temporal analysis\"\"\"\n",
        "        emerging_trends = []\n",
        "\n",
        "        # Sort time periods\n",
        "        sorted_periods = sorted(trend_analysis.keys())\n",
        "\n",
        "        if len(sorted_periods) < 2:\n",
        "            return emerging_trends\n",
        "\n",
        "        # Compare recent periods with earlier ones\n",
        "        recent_period = sorted_periods[-1]\n",
        "        previous_period = sorted_periods[-2] if len(sorted_periods) >= 2 else None\n",
        "\n",
        "        if not previous_period:\n",
        "            return emerging_trends\n",
        "\n",
        "        recent_keywords = set(trend_analysis[recent_period]['top_keywords'][:10])\n",
        "        previous_keywords = set(trend_analysis[previous_period]['top_keywords'][:10])\n",
        "\n",
        "        # Find new keywords\n",
        "        new_keywords = recent_keywords - previous_keywords\n",
        "\n",
        "        # Find growing categories\n",
        "        recent_categories = trend_analysis[recent_period]['top_categories']\n",
        "        previous_categories = trend_analysis[previous_period]['top_categories']\n",
        "\n",
        "        growing_categories = []\n",
        "        for cat, count in recent_categories.items():\n",
        "            prev_count = previous_categories.get(cat, 0)\n",
        "            if count > prev_count * 1.5:  # 50% growth threshold\n",
        "                growing_categories.append({\n",
        "                    'category': cat,\n",
        "                    'growth_rate': (count - prev_count) / max(prev_count, 1),\n",
        "                    'recent_count': count,\n",
        "                    'previous_count': prev_count\n",
        "                })\n",
        "\n",
        "        emerging_trends.append({\n",
        "            'type': 'new_keywords',\n",
        "            'keywords': list(new_keywords),\n",
        "            'period': recent_period\n",
        "        })\n",
        "\n",
        "        emerging_trends.append({\n",
        "            'type': 'growing_categories',\n",
        "            'categories': growing_categories,\n",
        "            'period': recent_period\n",
        "        })\n",
        "\n",
        "        return emerging_trends\n",
        "\n",
        "    def detect_research_gaps(self, papers: List[Dict]) -> Dict:\n",
        "        \"\"\"Detect potential research gaps using AI analysis\"\"\"\n",
        "        try:\n",
        "            if not papers:\n",
        "                return {'error': 'No papers provided'}\n",
        "\n",
        "            # Analyze papers to identify gaps\n",
        "            sample_papers = papers[:15]  # Limit to avoid token limits\n",
        "\n",
        "            # Prepare paper summaries\n",
        "            paper_summaries = []\n",
        "            for paper in sample_papers:\n",
        "                summary = f\"Title: {paper.get('title', 'Unknown')}\\n\"\n",
        "                summary += f\"Abstract: {paper.get('abstract', 'N/A')[:300]}...\\n\"\n",
        "                summary += f\"Year: {paper.get('year', 'Unknown')}\\n\"\n",
        "                summary += f\"Authors: {', '.join(paper.get('authors', [])[:3])}\\n\"\n",
        "                paper_summaries.append(summary)\n",
        "\n",
        "            # Use AI to identify gaps\n",
        "            prompt = f\"\"\"Analyze these research papers and identify potential research gaps and opportunities:\n",
        "\n",
        "{chr(10).join(paper_summaries[:10])}\n",
        "\n",
        "Please identify:\n",
        "1. **Methodological Gaps**: Missing approaches or techniques\n",
        "2. **Temporal Gaps**: Time periods or recent developments not covered\n",
        "3. **Interdisciplinary Gaps**: Connections between fields that could be explored\n",
        "4. **Application Gaps**: Real-world applications that need more research\n",
        "5. **Data Gaps**: Types of data or datasets that are underexplored\n",
        "\n",
        "Format your response as structured points under each category.\n",
        "Focus on actionable research opportunities.\"\"\"\n",
        "\n",
        "            response = groq_processor.generate_response(prompt, max_tokens=2000)\n",
        "\n",
        "            # Extract methodological insights\n",
        "            methodologies = self._extract_methodologies(sample_papers)\n",
        "\n",
        "            return {\n",
        "                'ai_analysis': response,\n",
        "                'methodological_analysis': methodologies,\n",
        "                'papers_analyzed': len(sample_papers),\n",
        "                'analysis_timestamp': datetime.now().isoformat()\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                'error': str(e),\n",
        "                'analysis_timestamp': datetime.now().isoformat()\n",
        "            }\n",
        "\n",
        "    def _extract_methodologies(self, papers: List[Dict]) -> Dict:\n",
        "        \"\"\"Extract and analyze methodologies from papers across all research disciplines\"\"\"\n",
        "        methodologies = defaultdict(int)\n",
        "        data_types = defaultdict(int)\n",
        "        evaluation_metrics = defaultdict(int)\n",
        "\n",
        "        # General research methodology keywords (applicable across disciplines)\n",
        "        method_keywords = {\n",
        "            'quantitative_analysis': ['quantitative', 'statistical analysis', 'survey', 'questionnaire', 'regression', 'correlation'],\n",
        "            'qualitative_analysis': ['qualitative', 'interview', 'case study', 'ethnography', 'grounded theory', 'content analysis'],\n",
        "            'experimental': ['experiment', 'randomized', 'control group', 'controlled trial', 'intervention', 'treatment'],\n",
        "            'observational': ['observational', 'longitudinal', 'cross-sectional', 'cohort study', 'prospective', 'retrospective'],\n",
        "            'systematic_review': ['systematic review', 'meta-analysis', 'literature review', 'scoping review'],\n",
        "            'theoretical': ['theoretical framework', 'conceptual model', 'theoretical analysis', 'mathematical model'],\n",
        "            'simulation': ['simulation', 'modeling', 'monte carlo', 'computational model', 'numerical analysis'],\n",
        "            'comparative': ['comparative study', 'comparison', 'cross-country', 'benchmarking', 'comparative analysis'],\n",
        "            'mixed_methods': ['mixed methods', 'triangulation', 'multi-method', 'convergent parallel'],\n",
        "            'field_study': ['field study', 'field work', 'natural setting', 'in-situ', 'real-world']\n",
        "        }\n",
        "\n",
        "        for paper in papers:\n",
        "            content = (paper.get('title', '') + ' ' + paper.get('abstract', '')).lower()\n",
        "\n",
        "            # Count methodologies\n",
        "            for method, keywords in method_keywords.items():\n",
        "                if any(keyword in content for keyword in keywords):\n",
        "                    methodologies[method] += 1\n",
        "\n",
        "            # General data types (applicable across disciplines)\n",
        "            data_type_keywords = {\n",
        "                'quantitative_data': ['numerical data', 'statistical data', 'survey data', 'measurement', 'metrics'],\n",
        "                'qualitative_data': ['interview data', 'textual data', 'narrative', 'qualitative data', 'thematic'],\n",
        "                'archival_data': ['archival', 'historical data', 'records', 'documents', 'archives'],\n",
        "                'observational_data': ['observational data', 'behavioral data', 'field notes', 'observations'],\n",
        "                'secondary_data': ['secondary data', 'existing data', 'database', 'administrative data'],\n",
        "                'experimental_data': ['experimental data', 'controlled data', 'laboratory data', 'trial data']\n",
        "            }\n",
        "\n",
        "            for data_type, keywords in data_type_keywords.items():\n",
        "                if any(keyword in content for keyword in keywords):\n",
        "                    data_types[data_type] += 1\n",
        "\n",
        "            # General evaluation metrics (cross-disciplinary)\n",
        "            general_metrics = [\n",
        "                'validity', 'reliability', 'significance', 'confidence interval', 'p-value',\n",
        "                'effect size', 'correlation coefficient', 'cronbach alpha', 'inter-rater reliability',\n",
        "                'sensitivity', 'specificity', 'response rate', 'sample size', 'power analysis'\n",
        "            ]\n",
        "\n",
        "            for metric in general_metrics:\n",
        "                if metric in content:\n",
        "                    evaluation_metrics[metric] += 1\n",
        "\n",
        "        return {\n",
        "            'methodologies': dict(methodologies),\n",
        "            'data_types': dict(data_types),\n",
        "            'evaluation_metrics': dict(evaluation_metrics),\n",
        "            'total_papers': len(papers),\n",
        "            'analysis_timestamp': datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "    def generate_trend_report(self, papers: List[Dict]) -> Dict:\n",
        "        \"\"\"Generate comprehensive trend report\"\"\"\n",
        "        try:\n",
        "            # Temporal analysis\n",
        "            temporal_trends = self.analyze_temporal_trends(papers)\n",
        "\n",
        "            # Research gaps\n",
        "            research_gaps = self.detect_research_gaps(papers)\n",
        "\n",
        "            # Overall statistics\n",
        "            total_papers = len(papers)\n",
        "            years = [p.get('year') for p in papers if p.get('year')]\n",
        "            year_range = f\"{min(years)} - {max(years)}\" if years else \"Unknown\"\n",
        "\n",
        "            # Top authors\n",
        "            author_counts = defaultdict(int)\n",
        "            for paper in papers:\n",
        "                for author in paper.get('authors', []):\n",
        "                    author_counts[author] += 1\n",
        "\n",
        "            top_authors = sorted(author_counts.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "\n",
        "            # Top venues/sources\n",
        "            venue_counts = defaultdict(int)\n",
        "            for paper in papers:\n",
        "                venue = paper.get('venue', paper.get('source', 'Unknown'))\n",
        "                venue_counts[venue] += 1\n",
        "\n",
        "            top_venues = sorted(venue_counts.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "\n",
        "            # AI-powered summary\n",
        "            summary_prompt = f\"\"\"Generate a comprehensive research trend summary based on this data:\n",
        "\n",
        "Total Papers: {total_papers}\n",
        "Year Range: {year_range}\n",
        "Top Authors: {', '.join([f\"{author} ({count})\" for author, count in top_authors[:5]])}\n",
        "Top Venues: {', '.join([f\"{venue} ({count})\" for venue, count in top_venues[:5]])}\n",
        "\n",
        "Key Trends: {temporal_trends.get('emerging_trends', [])}\n",
        "\n",
        "Provide a 3-paragraph executive summary covering:\n",
        "1. Overall research landscape and activity\n",
        "2. Key trends and emerging areas\n",
        "3. Future research directions and opportunities\"\"\"\n",
        "\n",
        "            ai_summary = groq_processor.generate_response(summary_prompt, max_tokens=1500)\n",
        "\n",
        "            return {\n",
        "                'executive_summary': ai_summary,\n",
        "                'statistics': {\n",
        "                    'total_papers': total_papers,\n",
        "                    'year_range': year_range,\n",
        "                    'top_authors': top_authors,\n",
        "                    'top_venues': top_venues\n",
        "                },\n",
        "                'temporal_trends': temporal_trends,\n",
        "                'research_gaps': research_gaps,\n",
        "                'report_generated': datetime.now().isoformat()\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                'error': str(e),\n",
        "                'report_generated': datetime.now().isoformat()\n",
        "            }\n",
        "\n",
        "    def monitor_realtime_trends(self, query: str, sources: List[str] = None) -> Dict:\n",
        "        \"\"\"Monitor real-time trends for a specific query\"\"\"\n",
        "        try:\n",
        "            if not sources:\n",
        "                sources = ['arxiv', 'semantic_scholar']\n",
        "\n",
        "            # Collect recent papers\n",
        "            recent_papers = []\n",
        "\n",
        "            for source in sources:\n",
        "                try:\n",
        "                    if source == 'arxiv':\n",
        "                        papers = arxiv_fetcher.get_trending_papers(days=30)\n",
        "                        # Filter by query\n",
        "                        filtered_papers = [p for p in papers if query.lower() in p.get('title', '').lower() or query.lower() in p.get('abstract', '').lower()]\n",
        "                        recent_papers.extend(filtered_papers[:10])\n",
        "\n",
        "                    elif source == 'semantic_scholar':\n",
        "                        papers = multi_source_collector.search_semantic_scholar(query, limit=10)\n",
        "                        recent_papers.extend(papers)\n",
        "\n",
        "                    time.sleep(1)  # Rate limiting\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"‚ö†Ô∏è  Error monitoring {source}: {e}\")\n",
        "                    continue\n",
        "\n",
        "            if not recent_papers:\n",
        "                return {\n",
        "                    'query': query,\n",
        "                    'trend_status': 'No recent papers found',\n",
        "                    'papers_found': 0,\n",
        "                    'monitoring_timestamp': datetime.now().isoformat()\n",
        "                }\n",
        "\n",
        "            # Analyze trends\n",
        "            trend_analysis = self.analyze_temporal_trends(recent_papers)\n",
        "\n",
        "            # Generate insights\n",
        "            insights_prompt = f\"\"\"Analyze these recent research papers for the query \"{query}\":\n",
        "\n",
        "Found {len(recent_papers)} recent papers.\n",
        "\n",
        "Recent trends: {trend_analysis.get('emerging_trends', [])}\n",
        "\n",
        "Provide insights on:\n",
        "1. Current research momentum (high/medium/low)\n",
        "2. Key developments in the last 30 days\n",
        "3. Emerging sub-topics or applications\n",
        "4. Potential future directions\n",
        "\n",
        "Keep it concise and actionable.\"\"\"\n",
        "\n",
        "            insights = groq_processor.generate_response(insights_prompt, max_tokens=1000)\n",
        "\n",
        "            return {\n",
        "                'query': query,\n",
        "                'trend_status': 'active' if len(recent_papers) > 5 else 'emerging',\n",
        "                'papers_found': len(recent_papers),\n",
        "                'recent_papers': recent_papers[:5],  # Top 5 most recent\n",
        "                'trend_analysis': trend_analysis,\n",
        "                'ai_insights': insights,\n",
        "                'monitoring_timestamp': datetime.now().isoformat()\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                'query': query,\n",
        "                'error': str(e),\n",
        "                'monitoring_timestamp': datetime.now().isoformat()\n",
        "            }\n",
        "\n",
        "    def create_trend_visualization(self, trend_data: Dict, save_path: str = None) -> str:\n",
        "        \"\"\"Create trend visualization\"\"\"\n",
        "        try:\n",
        "            if 'temporal_analysis' not in trend_data:\n",
        "                return \"No temporal data available for visualization\"\n",
        "\n",
        "            temporal_data = trend_data['temporal_analysis']\n",
        "\n",
        "            # Extract data for plotting\n",
        "            years = sorted(temporal_data.keys())\n",
        "            paper_counts = [temporal_data[year]['paper_count'] for year in years]\n",
        "\n",
        "            # Create visualization\n",
        "            fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
        "\n",
        "            # Plot 1: Papers over time\n",
        "            ax1.plot(years, paper_counts, marker='o', linewidth=2, markersize=8)\n",
        "            ax1.set_title('Research Papers Over Time', fontsize=14, fontweight='bold')\n",
        "            ax1.set_xlabel('Year')\n",
        "            ax1.set_ylabel('Number of Papers')\n",
        "            ax1.grid(True, alpha=0.3)\n",
        "\n",
        "            # Plot 2: Top keywords word cloud for recent period\n",
        "            if years:\n",
        "                recent_year = years[-1]\n",
        "                recent_keywords = temporal_data[recent_year]['top_keywords'][:20]\n",
        "\n",
        "                if recent_keywords:\n",
        "                    # Create word frequency dict\n",
        "                    word_freq = {word: len(recent_keywords) - i for i, word in enumerate(recent_keywords)}\n",
        "\n",
        "                    # Create word cloud\n",
        "                    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)\n",
        "\n",
        "                    ax2.imshow(wordcloud, interpolation='bilinear')\n",
        "                    ax2.axis('off')\n",
        "                    ax2.set_title(f'Top Keywords ({recent_year})', fontsize=14, fontweight='bold')\n",
        "\n",
        "            plt.tight_layout()\n",
        "\n",
        "            # Save or show\n",
        "            if save_path:\n",
        "                plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "                plt.close()\n",
        "                return f\"Trend visualization saved to {save_path}\"\n",
        "            else:\n",
        "                plt.show()\n",
        "                return \"Trend visualization displayed\"\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"Error creating trend visualization: {str(e)}\"\n",
        "\n",
        "# Initialize trend monitor\n",
        "trend_monitor = ResearchTrendMonitor()\n",
        "print(\"‚úÖ Research trend monitor ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "bae4f5f0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bae4f5f0",
        "outputId": "70d5b098-b87d-4399-c409-16b2565786fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Advanced Research Assistant initialized!\n",
            "‚úÖ Advanced Research Assistant ready!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# ADVANCED RESEARCH ASSISTANT\n",
        "# ============================================================================\n",
        "\n",
        "class AdvancedResearchAssistant:\n",
        "    \"\"\"Advanced research assistant with project management and analysis capabilities\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.projects = {}\n",
        "        self.literature_reviews = {}\n",
        "        self.analysis_cache = {}\n",
        "\n",
        "        # Initialize components\n",
        "        self.groq_processor = groq_processor\n",
        "        self.rag_system = rag_system\n",
        "        self.arxiv_fetcher = arxiv_fetcher\n",
        "        self.multi_source_collector = multi_source_collector\n",
        "        self.pdf_processor = pdf_processor\n",
        "        self.citation_analyzer = citation_analyzer\n",
        "        self.trend_monitor = trend_monitor\n",
        "\n",
        "        print(\"‚úÖ Advanced Research Assistant initialized!\")\n",
        "\n",
        "    def create_research_project(self, project_name: str, research_question: str,\n",
        "                              keywords: List[str], scope: str = \"comprehensive\") -> Dict:\n",
        "        \"\"\"Create a new research project\"\"\"\n",
        "        try:\n",
        "            project_id = f\"proj_{int(time.time())}_{hash(project_name) % 10000}\"\n",
        "\n",
        "            project = {\n",
        "                'id': project_id,\n",
        "                'name': project_name,\n",
        "                'research_question': research_question,\n",
        "                'keywords': keywords,\n",
        "                'scope': scope,\n",
        "                'created_at': datetime.now().isoformat(),\n",
        "                'status': 'active',\n",
        "                'papers': [],\n",
        "                'analyses': {},\n",
        "                'notes': [],\n",
        "                'progress': {\n",
        "                    'literature_search': 'pending',\n",
        "                    'paper_analysis': 'pending',\n",
        "                    'gap_analysis': 'pending',\n",
        "                    'trend_analysis': 'pending',\n",
        "                    'report_generation': 'pending'\n",
        "                }\n",
        "            }\n",
        "\n",
        "            self.projects[project_id] = project\n",
        "\n",
        "            # Generate initial research plan\n",
        "            plan_prompt = f\"\"\"Create a comprehensive research plan for this project:\n",
        "\n",
        "Project: {project_name}\n",
        "Research Question: {research_question}\n",
        "Keywords: {', '.join(keywords)}\n",
        "Scope: {scope}\n",
        "\n",
        "Generate a structured research plan including:\n",
        "1. **Literature Search Strategy**\n",
        "2. **Key Areas to Investigate**\n",
        "3. **Methodology Approach**\n",
        "4. **Expected Outcomes**\n",
        "5. **Timeline Recommendations**\n",
        "\n",
        "Format as a detailed research plan.\"\"\"\n",
        "\n",
        "            research_plan = self.groq_processor.generate_response(plan_prompt, max_tokens=2000)\n",
        "            project['research_plan'] = research_plan\n",
        "\n",
        "            return {\n",
        "                'success': True,\n",
        "                'project_id': project_id,\n",
        "                'project': project,\n",
        "                'message': f'Research project \"{project_name}\" created successfully'\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                'success': False,\n",
        "                'error': str(e),\n",
        "                'message': 'Failed to create research project'\n",
        "            }\n",
        "\n",
        "    def conduct_literature_search(self, project_id: str, max_papers_per_source: int = 10) -> Dict:\n",
        "        \"\"\"Conduct comprehensive literature search for a project\"\"\"\n",
        "        try:\n",
        "            if project_id not in self.projects:\n",
        "                return {'success': False, 'error': 'Project not found'}\n",
        "\n",
        "            project = self.projects[project_id]\n",
        "            keywords = project['keywords']\n",
        "\n",
        "            print(f\"üîç Conducting literature search for project: {project['name']}\")\n",
        "\n",
        "            # Search all sources\n",
        "            all_papers = []\n",
        "            search_results = {}\n",
        "\n",
        "            for keyword in keywords:\n",
        "                print(f\"üîç Searching for keyword: {keyword}\")\n",
        "\n",
        "                # Multi-source search\n",
        "                papers = self.multi_source_collector.search_all_sources(keyword, max_papers_per_source)\n",
        "                all_papers.extend(papers)\n",
        "                search_results[keyword] = len(papers)\n",
        "\n",
        "                time.sleep(2)  # Rate limiting\n",
        "\n",
        "            # Deduplicate papers\n",
        "            unique_papers = self.multi_source_collector.deduplicate_papers(all_papers)\n",
        "\n",
        "            # Add papers to project\n",
        "            project['papers'] = unique_papers\n",
        "            project['progress']['literature_search'] = 'completed'\n",
        "\n",
        "            # Add to RAG system\n",
        "            if self.rag_system:\n",
        "                self.rag_system.add_documents(unique_papers)\n",
        "\n",
        "            # Add to citation analyzer\n",
        "            self.citation_analyzer.add_papers(unique_papers)\n",
        "\n",
        "            return {\n",
        "                'success': True,\n",
        "                'papers_found': len(unique_papers),\n",
        "                'search_results': search_results,\n",
        "                'unique_papers': len(unique_papers),\n",
        "                'message': f'Found {len(unique_papers)} unique papers'\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                'success': False,\n",
        "                'error': str(e),\n",
        "                'message': 'Literature search failed'\n",
        "            }\n",
        "\n",
        "    def analyze_literature(self, project_id: str) -> Dict:\n",
        "        \"\"\"Analyze literature for a project\"\"\"\n",
        "        try:\n",
        "            if project_id not in self.projects:\n",
        "                return {'success': False, 'error': 'Project not found'}\n",
        "\n",
        "            project = self.projects[project_id]\n",
        "            papers = project['papers']\n",
        "\n",
        "            if not papers:\n",
        "                return {'success': False, 'error': 'No papers found in project'}\n",
        "\n",
        "            print(f\"üìä Analyzing literature for project: {project['name']}\")\n",
        "\n",
        "            # Comprehensive analysis\n",
        "            analyses = {}\n",
        "\n",
        "            # 1. Citation network analysis\n",
        "            print(\"üìä Analyzing citation networks...\")\n",
        "            citation_analysis = self.citation_analyzer.get_network_summary()\n",
        "            analyses['citation_network'] = citation_analysis\n",
        "\n",
        "            # 2. Trend analysis\n",
        "            print(\"üìä Analyzing research trends...\")\n",
        "            trend_analysis = self.trend_monitor.generate_trend_report(papers)\n",
        "            analyses['trends'] = trend_analysis\n",
        "\n",
        "            # 3. Gap analysis\n",
        "            print(\"üìä Detecting research gaps...\")\n",
        "            gap_analysis = self.trend_monitor.detect_research_gaps(papers)\n",
        "            analyses['gaps'] = gap_analysis\n",
        "\n",
        "            # 4. Thematic analysis using AI\n",
        "            print(\"üìä Conducting thematic analysis...\")\n",
        "            thematic_analysis = self._conduct_thematic_analysis(papers, project['research_question'])\n",
        "            analyses['thematic'] = thematic_analysis\n",
        "\n",
        "            # 5. Methodology analysis\n",
        "            print(\"üìä Analyzing methodologies...\")\n",
        "            methodology_analysis = self._analyze_methodologies(papers)\n",
        "            analyses['methodologies'] = methodology_analysis\n",
        "\n",
        "            # Store analyses\n",
        "            project['analyses'] = analyses\n",
        "            project['progress']['paper_analysis'] = 'completed'\n",
        "            project['progress']['gap_analysis'] = 'completed'\n",
        "            project['progress']['trend_analysis'] = 'completed'\n",
        "\n",
        "            return {\n",
        "                'success': True,\n",
        "                'analyses': analyses,\n",
        "                'message': 'Literature analysis completed successfully'\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                'success': False,\n",
        "                'error': str(e),\n",
        "                'message': 'Literature analysis failed'\n",
        "            }\n",
        "\n",
        "    def _conduct_thematic_analysis(self, papers: List[Dict], research_question: str) -> Dict:\n",
        "        \"\"\"Conduct thematic analysis of papers\"\"\"\n",
        "        try:\n",
        "            # Prepare paper data for analysis\n",
        "            paper_themes = []\n",
        "            for paper in papers[:20]:  # Limit for token management\n",
        "                theme_text = f\"Title: {paper.get('title', '')}\\nAbstract: {paper.get('abstract', '')[:400]}\"\n",
        "                paper_themes.append(theme_text)\n",
        "\n",
        "            # AI-powered thematic analysis\n",
        "            prompt = f\"\"\"Conduct a comprehensive thematic analysis of these research papers in relation to the research question: \"{research_question}\"\n",
        "\n",
        "Papers to analyze:\n",
        "{chr(10).join(paper_themes[:15])}\n",
        "\n",
        "Identify:\n",
        "1. **Main Themes**: 5-7 major themes across the papers\n",
        "2. **Sub-themes**: Related concepts under each main theme\n",
        "3. **Research Approaches**: Common methodological approaches\n",
        "4. **Theoretical Frameworks**: Underlying theories and models\n",
        "5. **Contradictions**: Conflicting findings or viewpoints\n",
        "6. **Consensus Areas**: Where researchers agree\n",
        "\n",
        "Format as structured analysis with clear sections.\"\"\"\n",
        "\n",
        "            analysis = self.groq_processor.generate_response(prompt, max_tokens=2500)\n",
        "\n",
        "            return {\n",
        "                'analysis': analysis,\n",
        "                'papers_analyzed': len(paper_themes),\n",
        "                'research_question': research_question,\n",
        "                'analysis_timestamp': datetime.now().isoformat()\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                'analysis': f'Error in thematic analysis: {str(e)}',\n",
        "                'error': str(e),\n",
        "                'analysis_timestamp': datetime.now().isoformat()\n",
        "            }\n",
        "\n",
        "    def _analyze_methodologies(self, papers: List[Dict]) -> Dict:\n",
        "        \"\"\"Analyze methodologies used in papers\"\"\"\n",
        "        try:\n",
        "            methodologies = defaultdict(int)\n",
        "            data_types = defaultdict(int)\n",
        "            evaluation_metrics = defaultdict(int)\n",
        "\n",
        "            # Common methodology keywords\n",
        "            method_keywords = {\n",
        "                'machine_learning': ['machine learning', 'ml', 'supervised', 'unsupervised'],\n",
        "                'deep_learning': ['deep learning', 'neural network', 'cnn', 'rnn', 'transformer'],\n",
        "                'nlp': ['natural language processing', 'nlp', 'text mining', 'sentiment analysis'],\n",
        "                'computer_vision': ['computer vision', 'image processing', 'object detection'],\n",
        "                'reinforcement_learning': ['reinforcement learning', 'rl', 'q-learning'],\n",
        "                'statistical_analysis': ['statistical analysis', 'regression', 'correlation', 'anova'],\n",
        "                'experimental': ['experiment', 'randomized', 'control group', 'a/b test'],\n",
        "                'survey': ['survey', 'questionnaire', 'interview', 'qualitative'],\n",
        "                'simulation': ['simulation', 'model', 'monte carlo']\n",
        "            }\n",
        "\n",
        "            for paper in papers:\n",
        "                content = (paper.get('title', '') + ' ' + paper.get('abstract', '')).lower()\n",
        "\n",
        "                # Count methodologies\n",
        "                for method, keywords in method_keywords.items():\n",
        "                    if any(keyword in content for keyword in keywords):\n",
        "                        methodologies[method] += 1\n",
        "\n",
        "                # Identify data types\n",
        "                if any(word in content for word in ['dataset', 'data', 'corpus']):\n",
        "                    if 'text' in content or 'nlp' in content:\n",
        "                        data_types['text'] += 1\n",
        "                    elif 'image' in content or 'vision' in content:\n",
        "                        data_types['image'] += 1\n",
        "                    elif 'audio' in content or 'speech' in content:\n",
        "                        data_types['audio'] += 1\n",
        "                    else:\n",
        "                        data_types['tabular'] += 1\n",
        "\n",
        "                # Common evaluation metrics\n",
        "                metrics = ['accuracy', 'precision', 'recall', 'f1', 'bleu', 'rouge', 'mae', 'mse']\n",
        "                for metric in metrics:\n",
        "                    if metric in content:\n",
        "                        evaluation_metrics[metric] += 1\n",
        "\n",
        "            return {\n",
        "                'methodologies': dict(methodologies),\n",
        "                'data_types': dict(data_types),\n",
        "                'evaluation_metrics': dict(evaluation_metrics),\n",
        "                'total_papers': len(papers),\n",
        "                'analysis_timestamp': datetime.now().isoformat()\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                'error': str(e),\n",
        "                'analysis_timestamp': datetime.now().isoformat()\n",
        "            }\n",
        "\n",
        "    def generate_literature_review(self, project_id: str, review_type: str = \"comprehensive\") -> Dict:\n",
        "        \"\"\"Generate comprehensive literature review\"\"\"\n",
        "        try:\n",
        "            if project_id not in self.projects:\n",
        "                return {'success': False, 'error': 'Project not found'}\n",
        "\n",
        "            project = self.projects[project_id]\n",
        "\n",
        "            if 'analyses' not in project:\n",
        "                return {'success': False, 'error': 'No analyses found. Run analyze_literature first.'}\n",
        "\n",
        "            print(f\"üìù Generating literature review for project: {project['name']}\")\n",
        "\n",
        "            # Gather all analysis data\n",
        "            analyses = project['analyses']\n",
        "            papers = project['papers']\n",
        "\n",
        "            # Generate comprehensive review\n",
        "            review_prompt = f\"\"\"Generate a comprehensive literature review based on this research project:\n",
        "\n",
        "Project: {project['name']}\n",
        "Research Question: {project['research_question']}\n",
        "Keywords: {', '.join(project['keywords'])}\n",
        "\n",
        "Analysis Summary:\n",
        "- Total Papers: {len(papers)}\n",
        "- Thematic Analysis: {analyses.get('thematic', {}).get('analysis', 'N/A')[:500]}...\n",
        "- Trend Analysis: {analyses.get('trends', {}).get('executive_summary', 'N/A')[:500]}...\n",
        "- Gap Analysis: {analyses.get('gaps', {}).get('ai_analysis', 'N/A')[:500]}...\n",
        "\n",
        "Generate a structured literature review with:\n",
        "1. **Introduction** - Research context and objectives\n",
        "2. **Literature Search Methodology** - How papers were identified\n",
        "3. **Thematic Analysis** - Key themes and patterns\n",
        "4. **Current State of Research** - What we know\n",
        "5. **Research Gaps** - What's missing\n",
        "6. **Future Directions** - Recommendations for future work\n",
        "7. **Conclusion** - Summary and implications\n",
        "\n",
        "Write in academic style, approximately 2000-3000 words.\"\"\"\n",
        "\n",
        "            literature_review = self.groq_processor.generate_response(review_prompt, max_tokens=4000)\n",
        "\n",
        "            # Store review\n",
        "            review_id = f\"review_{int(time.time())}\"\n",
        "            review_data = {\n",
        "                'id': review_id,\n",
        "                'project_id': project_id,\n",
        "                'review_type': review_type,\n",
        "                'content': literature_review,\n",
        "                'generated_at': datetime.now().isoformat(),\n",
        "                'word_count': len(literature_review.split()),\n",
        "                'papers_reviewed': len(papers)\n",
        "            }\n",
        "\n",
        "            self.literature_reviews[review_id] = review_data\n",
        "            project['progress']['report_generation'] = 'completed'\n",
        "\n",
        "            return {\n",
        "                'success': True,\n",
        "                'review_id': review_id,\n",
        "                'review': review_data,\n",
        "                'message': 'Literature review generated successfully'\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                'success': False,\n",
        "                'error': str(e),\n",
        "                'message': 'Literature review generation failed'\n",
        "            }\n",
        "\n",
        "    def ask_research_question(self, project_id: str, question: str) -> Dict:\n",
        "        \"\"\"Ask questions about the research project using RAG\"\"\"\n",
        "        try:\n",
        "            if project_id not in self.projects:\n",
        "                return {'success': False, 'error': 'Project not found'}\n",
        "\n",
        "            project = self.projects[project_id]\n",
        "\n",
        "            # Use RAG system to answer question\n",
        "            if self.rag_system:\n",
        "                response = self.rag_system.query(question, include_sources=True)\n",
        "\n",
        "                # Add project context to response\n",
        "                response['project_context'] = {\n",
        "                    'project_name': project['name'],\n",
        "                    'research_question': project['research_question'],\n",
        "                    'total_papers': len(project.get('papers', [])),\n",
        "                    'project_id': project_id\n",
        "                }\n",
        "\n",
        "                return {\n",
        "                    'success': True,\n",
        "                    'response': response\n",
        "                }\n",
        "            else:\n",
        "                return {\n",
        "                    'success': False,\n",
        "                    'error': 'RAG system not available'\n",
        "                }\n",
        "\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                'success': False,\n",
        "                'error': str(e)\n",
        "            }\n",
        "\n",
        "    def get_project_summary(self, project_id: str) -> Dict:\n",
        "        \"\"\"Get comprehensive project summary\"\"\"\n",
        "        try:\n",
        "            if project_id not in self.projects:\n",
        "                return {'success': False, 'error': 'Project not found'}\n",
        "\n",
        "            project = self.projects[project_id]\n",
        "\n",
        "            # Calculate completion percentage\n",
        "            progress = project['progress']\n",
        "            completed_tasks = sum(1 for status in progress.values() if status == 'completed')\n",
        "            total_tasks = len(progress)\n",
        "            completion_percentage = (completed_tasks / total_tasks) * 100\n",
        "\n",
        "            # Generate executive summary\n",
        "            exec_summary_prompt = f\"\"\"Generate an executive summary for this research project:\n",
        "\n",
        "Project: {project['name']}\n",
        "Research Question: {project['research_question']}\n",
        "Keywords: {', '.join(project['keywords'])}\n",
        "Papers Found: {len(project.get('papers', []))}\n",
        "Completion: {completion_percentage:.1f}%\n",
        "\n",
        "Key Findings:\n",
        "{project.get('analyses', {}).get('thematic', {}).get('analysis', 'Analysis pending')[:300]}...\n",
        "\n",
        "Provide a concise 3-paragraph executive summary covering:\n",
        "1. Project overview and objectives\n",
        "2. Key findings and insights\n",
        "3. Next steps and recommendations\"\"\"\n",
        "\n",
        "            executive_summary = self.groq_processor.generate_response(exec_summary_prompt, max_tokens=1000)\n",
        "\n",
        "            return {\n",
        "                'success': True,\n",
        "                'project_id': project_id,\n",
        "                'project_name': project['name'],\n",
        "                'executive_summary': executive_summary,\n",
        "                'completion_percentage': completion_percentage,\n",
        "                'progress': progress,\n",
        "                'statistics': {\n",
        "                    'papers_found': len(project.get('papers', [])),\n",
        "                    'analyses_completed': len(project.get('analyses', {})),\n",
        "                    'reviews_generated': len([r for r in self.literature_reviews.values() if r['project_id'] == project_id]),\n",
        "                    'created_at': project['created_at']\n",
        "                },\n",
        "                'next_steps': self._get_next_steps(project)\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                'success': False,\n",
        "                'error': str(e)\n",
        "            }\n",
        "\n",
        "    def _get_next_steps(self, project: Dict) -> List[str]:\n",
        "        \"\"\"Get next steps for a project\"\"\"\n",
        "        next_steps = []\n",
        "        progress = project['progress']\n",
        "\n",
        "        if progress['literature_search'] == 'pending':\n",
        "            next_steps.append('Conduct literature search')\n",
        "        elif progress['paper_analysis'] == 'pending':\n",
        "            next_steps.append('Analyze collected papers')\n",
        "        elif progress['gap_analysis'] == 'pending':\n",
        "            next_steps.append('Identify research gaps')\n",
        "        elif progress['trend_analysis'] == 'pending':\n",
        "            next_steps.append('Analyze research trends')\n",
        "        elif progress['report_generation'] == 'pending':\n",
        "            next_steps.append('Generate literature review')\n",
        "        else:\n",
        "            next_steps.append('Project completed - consider follow-up research')\n",
        "\n",
        "        return next_steps\n",
        "\n",
        "    def list_projects(self) -> Dict:\n",
        "        \"\"\"List all projects\"\"\"\n",
        "        try:\n",
        "            project_list = []\n",
        "            for project_id, project in self.projects.items():\n",
        "                progress = project['progress']\n",
        "                completed_tasks = sum(1 for status in progress.values() if status == 'completed')\n",
        "                total_tasks = len(progress)\n",
        "                completion_percentage = (completed_tasks / total_tasks) * 100\n",
        "\n",
        "                project_list.append({\n",
        "                    'id': project_id,\n",
        "                    'name': project['name'],\n",
        "                    'research_question': project['research_question'],\n",
        "                    'status': project['status'],\n",
        "                    'completion_percentage': completion_percentage,\n",
        "                    'papers_count': len(project.get('papers', [])),\n",
        "                    'created_at': project['created_at']\n",
        "                })\n",
        "\n",
        "            return {\n",
        "                'success': True,\n",
        "                'projects': project_list,\n",
        "                'total_projects': len(project_list)\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                'success': False,\n",
        "                'error': str(e)\n",
        "            }\n",
        "\n",
        "    def get_system_status(self) -> Dict:\n",
        "        \"\"\"Get comprehensive system status\"\"\"\n",
        "        try:\n",
        "            # RAG system stats\n",
        "            rag_stats = self.rag_system.get_stats() if self.rag_system else {'status': 'Not available'}\n",
        "\n",
        "            # Citation network stats\n",
        "            citation_stats = self.citation_analyzer.get_network_summary()\n",
        "\n",
        "            # Project stats\n",
        "            project_stats = {\n",
        "                'total_projects': len(self.projects),\n",
        "                'active_projects': len([p for p in self.projects.values() if p['status'] == 'active']),\n",
        "                'completed_projects': len([p for p in self.projects.values() if p['status'] == 'completed']),\n",
        "                'total_papers': sum(len(p.get('papers', [])) for p in self.projects.values()),\n",
        "                'total_reviews': len(self.literature_reviews)\n",
        "            }\n",
        "\n",
        "            return {\n",
        "                'system_status': 'operational',\n",
        "                'components': {\n",
        "                    'groq_processor': 'ready' if self.groq_processor else 'not available',\n",
        "                    'rag_system': rag_stats.get('status', 'unknown'),\n",
        "                    'arxiv_fetcher': 'ready' if self.arxiv_fetcher else 'not available',\n",
        "                    'multi_source_collector': 'ready' if self.multi_source_collector else 'not available',\n",
        "                    'pdf_processor': 'ready' if self.pdf_processor else 'not available',\n",
        "                    'citation_analyzer': 'ready' if self.citation_analyzer else 'not available',\n",
        "                    'trend_monitor': 'ready' if self.trend_monitor else 'not available'\n",
        "                },\n",
        "                'statistics': {\n",
        "                    'projects': project_stats,\n",
        "                    'rag_system': rag_stats,\n",
        "                    'citation_network': citation_stats\n",
        "                },\n",
        "                'timestamp': datetime.now().isoformat()\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                'system_status': 'error',\n",
        "                'error': str(e),\n",
        "                'timestamp': datetime.now().isoformat()\n",
        "            }\n",
        "\n",
        "# Initialize Advanced Research Assistant\n",
        "research_assistant = AdvancedResearchAssistant()\n",
        "print(\"‚úÖ Advanced Research Assistant ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "b7192cfe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7192cfe",
        "outputId": "a53b3d14-48a5-404a-dad3-e27a0dac2c03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "üöÄ RESEARCHMATE - AI RESEARCH ASSISTANT\n",
            "======================================================================\n",
            "Version: 2.0.0\n",
            "Powered by: Groq Llama 3.3 70B\n",
            "Initialized: 2025-07-08T17:54:17.016088\n",
            "======================================================================\n",
            "‚úÖ All systems operational!\n",
            "======================================================================\n",
            "‚úÖ ResearchMate unified interface ready!\n",
            "\n",
            "üí° Quick start:\n",
            "   research_mate.demo()  # Run comprehensive demo\n",
            "   research_mate.help()  # Show help\n",
            "   research_mate.quick_search('your topic')  # Quick search\n",
            "   research_mate.ask('your question')  # Ask questions\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# UNIFIED RESEARCHMATE INTERFACE\n",
        "# ============================================================================\n",
        "\n",
        "class ResearchMate:\n",
        "    \"\"\"Unified interface for the ResearchMate AI Research Assistant\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # Initialize all components\n",
        "        self.assistant = research_assistant\n",
        "        self.version = \"2.0.0\"\n",
        "        self.initialized_at = datetime.now().isoformat()\n",
        "\n",
        "        print(\"=\" * 70)\n",
        "        print(\"üöÄ RESEARCHMATE - AI RESEARCH ASSISTANT\")\n",
        "        print(\"=\" * 70)\n",
        "        print(f\"Version: {self.version}\")\n",
        "        print(f\"Powered by: Groq Llama 3.3 70B\")\n",
        "        print(f\"Initialized: {self.initialized_at}\")\n",
        "        print(\"=\" * 70)\n",
        "        print(\"‚úÖ All systems operational!\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "    # =========================\n",
        "    # QUICK START METHODS\n",
        "    # =========================\n",
        "\n",
        "    def quick_search(self, query: str, max_results: int = 10) -> Dict:\n",
        "        \"\"\"Quick search across all sources\"\"\"\n",
        "        try:\n",
        "            print(f\"üîç Quick search: '{query}'\")\n",
        "            results = self.assistant.multi_source_collector.search_all_sources(query, max_results)\n",
        "\n",
        "            # Add to RAG system for immediate querying\n",
        "            if results and self.assistant.rag_system:\n",
        "                self.assistant.rag_system.add_documents(results)\n",
        "\n",
        "            return {\n",
        "                'success': True,\n",
        "                'query': query,\n",
        "                'results': results,\n",
        "                'count': len(results),\n",
        "                'message': f'Found {len(results)} papers'\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {'success': False, 'error': str(e)}\n",
        "\n",
        "    def ask(self, question: str) -> Dict:\n",
        "        \"\"\"Ask a question using RAG system\"\"\"\n",
        "        try:\n",
        "            if not self.assistant.rag_system:\n",
        "                return {'success': False, 'error': 'RAG system not available'}\n",
        "\n",
        "            response = self.assistant.rag_system.query(question, include_sources=True)\n",
        "            return {\n",
        "                'success': True,\n",
        "                'question': question,\n",
        "                'answer': response['answer'],\n",
        "                'sources': response.get('sources', []),\n",
        "                'timestamp': response.get('timestamp')\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {'success': False, 'error': str(e)}\n",
        "\n",
        "    def analyze_pdf(self, pdf_path: str) -> Dict:\n",
        "        \"\"\"Analyze a PDF file\"\"\"\n",
        "        try:\n",
        "            result = self.assistant.pdf_processor.process_pdf_file(pdf_path)\n",
        "            return result\n",
        "        except Exception as e:\n",
        "            return {'success': False, 'error': str(e)}\n",
        "\n",
        "    def get_trends(self, topic: str) -> Dict:\n",
        "        \"\"\"Get research trends for a topic\"\"\"\n",
        "        try:\n",
        "            return self.assistant.trend_monitor.monitor_realtime_trends(topic)\n",
        "        except Exception as e:\n",
        "            return {'success': False, 'error': str(e)}\n",
        "\n",
        "    def status(self) -> Dict:\n",
        "        \"\"\"Get system status\"\"\"\n",
        "        try:\n",
        "            return self.assistant.get_system_status()\n",
        "        except Exception as e:\n",
        "            return {'success': False, 'error': str(e)}\n",
        "\n",
        "    # =========================\n",
        "    # PROJECT MANAGEMENT\n",
        "    # =========================\n",
        "\n",
        "    def create_project(self, name: str, research_question: str, keywords: List[str]) -> Dict:\n",
        "        \"\"\"Create a new research project\"\"\"\n",
        "        try:\n",
        "            return self.assistant.create_research_project(name, research_question, keywords)\n",
        "        except Exception as e:\n",
        "            return {'success': False, 'error': str(e)}\n",
        "\n",
        "    def list_projects(self) -> Dict:\n",
        "        \"\"\"List all projects\"\"\"\n",
        "        try:\n",
        "            return self.assistant.list_projects()\n",
        "        except Exception as e:\n",
        "            return {'success': False, 'error': str(e)}\n",
        "\n",
        "    def get_project(self, project_id: str) -> Dict:\n",
        "        \"\"\"Get project summary\"\"\"\n",
        "        try:\n",
        "            return self.assistant.get_project_summary(project_id)\n",
        "        except Exception as e:\n",
        "            return {'success': False, 'error': str(e)}\n",
        "\n",
        "    def search_literature(self, project_id: str, max_papers: int = 10) -> Dict:\n",
        "        \"\"\"Search literature for a project\"\"\"\n",
        "        try:\n",
        "            return self.assistant.conduct_literature_search(project_id, max_papers)\n",
        "        except Exception as e:\n",
        "            return {'success': False, 'error': str(e)}\n",
        "\n",
        "    def analyze_project(self, project_id: str) -> Dict:\n",
        "        \"\"\"Analyze project literature\"\"\"\n",
        "        try:\n",
        "            return self.assistant.analyze_literature(project_id)\n",
        "        except Exception as e:\n",
        "            return {'success': False, 'error': str(e)}\n",
        "\n",
        "    def generate_review(self, project_id: str) -> Dict:\n",
        "        \"\"\"Generate literature review\"\"\"\n",
        "        try:\n",
        "            return self.assistant.generate_literature_review(project_id)\n",
        "        except Exception as e:\n",
        "            return {'success': False, 'error': str(e)}\n",
        "\n",
        "    def ask_project(self, project_id: str, question: str) -> Dict:\n",
        "        \"\"\"Ask question about a specific project\"\"\"\n",
        "        try:\n",
        "            return self.assistant.ask_research_question(project_id, question)\n",
        "        except Exception as e:\n",
        "            return {'success': False, 'error': str(e)}\n",
        "\n",
        "    # =========================\n",
        "    # ANALYSIS METHODS\n",
        "    # =========================\n",
        "\n",
        "    def analyze_citations(self, papers: List[Dict]) -> Dict:\n",
        "        \"\"\"Analyze citation network\"\"\"\n",
        "        try:\n",
        "            self.assistant.citation_analyzer.add_papers(papers)\n",
        "            return self.assistant.citation_analyzer.get_network_summary()\n",
        "        except Exception as e:\n",
        "            return {'success': False, 'error': str(e)}\n",
        "\n",
        "    def detect_gaps(self, papers: List[Dict]) -> Dict:\n",
        "        \"\"\"Detect research gaps\"\"\"\n",
        "        try:\n",
        "            return self.assistant.trend_monitor.detect_research_gaps(papers)\n",
        "        except Exception as e:\n",
        "            return {'success': False, 'error': str(e)}\n",
        "\n",
        "    def generate_report(self, papers: List[Dict]) -> Dict:\n",
        "        \"\"\"Generate comprehensive trend report\"\"\"\n",
        "        try:\n",
        "            return self.assistant.trend_monitor.generate_trend_report(papers)\n",
        "        except Exception as e:\n",
        "            return {'success': False, 'error': str(e)}\n",
        "\n",
        "    # =========================\n",
        "    # WORKFLOW METHODS\n",
        "    # =========================\n",
        "\n",
        "    def full_workflow(self, project_name: str, research_question: str, keywords: List[str]) -> Dict:\n",
        "        \"\"\"Complete research workflow\"\"\"\n",
        "        try:\n",
        "            print(\"üöÄ Starting full research workflow...\")\n",
        "\n",
        "            # Step 1: Create project\n",
        "            print(\"üìù Creating research project...\")\n",
        "            project_result = self.create_project(project_name, research_question, keywords)\n",
        "            if not project_result['success']:\n",
        "                return project_result\n",
        "\n",
        "            project_id = project_result['project_id']\n",
        "\n",
        "            # Step 2: Literature search\n",
        "            print(\"üîç Conducting literature search...\")\n",
        "            search_result = self.search_literature(project_id, max_papers=15)\n",
        "            if not search_result['success']:\n",
        "                return search_result\n",
        "\n",
        "            # Step 3: Analysis\n",
        "            print(\"üìä Analyzing literature...\")\n",
        "            analysis_result = self.analyze_project(project_id)\n",
        "            if not analysis_result['success']:\n",
        "                return analysis_result\n",
        "\n",
        "            # Step 4: Generate review\n",
        "            print(\"üìù Generating literature review...\")\n",
        "            review_result = self.generate_review(project_id)\n",
        "            if not review_result['success']:\n",
        "                return review_result\n",
        "\n",
        "            # Step 5: Get final summary\n",
        "            print(\"üìã Preparing final summary...\")\n",
        "            summary_result = self.get_project(project_id)\n",
        "\n",
        "            return {\n",
        "                'success': True,\n",
        "                'project_id': project_id,\n",
        "                'workflow_completed': True,\n",
        "                'steps_completed': {\n",
        "                    'project_creation': project_result['success'],\n",
        "                    'literature_search': search_result['success'],\n",
        "                    'analysis': analysis_result['success'],\n",
        "                    'review_generation': review_result['success']\n",
        "                },\n",
        "                'final_summary': summary_result,\n",
        "                'message': 'Full research workflow completed successfully!'\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            return {'success': False, 'error': str(e)}\n",
        "\n",
        "    def demo(self) -> Dict:\n",
        "        \"\"\"Run a comprehensive demonstration\"\"\"\n",
        "        try:\n",
        "            print(\"üéØ ResearchMate Demo Starting...\")\n",
        "            print(\"=\" * 50)\n",
        "\n",
        "            demo_results = {}\n",
        "\n",
        "            # Demo 1: System Status\n",
        "            print(\"1Ô∏è‚É£ System Status Check...\")\n",
        "            status_result = self.status()\n",
        "            demo_results['system_status'] = status_result\n",
        "            print(f\"   ‚úÖ System Status: {status_result.get('system_status', 'unknown')}\")\n",
        "\n",
        "            # Demo 2: Quick Search\n",
        "            print(\"\\n2Ô∏è‚É£ Quick Search Demo...\")\n",
        "            search_result = self.quick_search(\"transformer attention mechanism\", max_results=5)\n",
        "            demo_results['quick_search'] = search_result\n",
        "            print(f\"   ‚úÖ Found {search_result.get('count', 0)} papers\")\n",
        "\n",
        "            # Demo 3: Ask Question\n",
        "            print(\"\\n3Ô∏è‚É£ Question Answering Demo...\")\n",
        "            qa_result = self.ask(\"What is attention mechanism in transformers?\")\n",
        "            demo_results['question_answering'] = qa_result\n",
        "            if qa_result['success']:\n",
        "                print(f\"   ‚úÖ Answer: {qa_result['answer'][:100]}...\")\n",
        "\n",
        "            # Demo 4: Create Demo Project\n",
        "            print(\"\\n4Ô∏è‚É£ Project Creation Demo...\")\n",
        "            project_result = self.create_project(\n",
        "                \"Transformer Architecture Analysis\",\n",
        "                \"How do attention mechanisms improve transformer performance?\",\n",
        "                [\"transformer\", \"attention mechanism\", \"neural networks\"]\n",
        "            )\n",
        "            demo_results['project_creation'] = project_result\n",
        "\n",
        "            if project_result['success']:\n",
        "                project_id = project_result['project_id']\n",
        "                print(f\"   ‚úÖ Project created: {project_id}\")\n",
        "\n",
        "                # Demo 5: Literature Search\n",
        "                print(\"\\n5Ô∏è‚É£ Literature Search Demo...\")\n",
        "                lit_search_result = self.search_literature(project_id, max_papers=3)\n",
        "                demo_results['literature_search'] = lit_search_result\n",
        "                print(f\"   ‚úÖ Found {lit_search_result.get('papers_found', 0)} papers\")\n",
        "\n",
        "            # Demo 6: Trend Analysis\n",
        "            print(\"\\n6Ô∏è‚É£ Trend Analysis Demo...\")\n",
        "            trend_result = self.get_trends(\"large language models\")\n",
        "            demo_results['trend_analysis'] = trend_result\n",
        "            print(f\"   ‚úÖ Trend status: {trend_result.get('trend_status', 'unknown')}\")\n",
        "\n",
        "            print(\"\\n\" + \"=\" * 50)\n",
        "            print(\"üéâ Demo completed successfully!\")\n",
        "            print(\"=\" * 50)\n",
        "\n",
        "            return {\n",
        "                'success': True,\n",
        "                'demo_completed': True,\n",
        "                'demo_results': demo_results,\n",
        "                'message': 'All demo components executed successfully!'\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                'success': False,\n",
        "                'error': str(e),\n",
        "                'message': 'Demo failed'\n",
        "            }\n",
        "\n",
        "    # =========================\n",
        "    # UTILITY METHODS\n",
        "    # =========================\n",
        "\n",
        "    def help(self) -> str:\n",
        "        \"\"\"Show help information\"\"\"\n",
        "        help_text = \"\"\"\n",
        "        üöÄ RESEARCHMATE - AI RESEARCH ASSISTANT HELP\n",
        "        ==========================================\n",
        "\n",
        "        QUICK START METHODS:\n",
        "        - quick_search(query, max_results=10) : Search all sources\n",
        "        - ask(question) : Ask questions using RAG\n",
        "        - analyze_pdf(pdf_path) : Analyze PDF files\n",
        "        - get_trends(topic) : Get research trends\n",
        "        - status() : Get system status\n",
        "\n",
        "        PROJECT MANAGEMENT:\n",
        "        - create_project(name, research_question, keywords) : Create new project\n",
        "        - list_projects() : List all projects\n",
        "        - get_project(project_id) : Get project summary\n",
        "        - search_literature(project_id, max_papers=10) : Search literature\n",
        "        - analyze_project(project_id) : Analyze project literature\n",
        "        - generate_review(project_id) : Generate literature review\n",
        "        - ask_project(project_id, question) : Ask project-specific questions\n",
        "\n",
        "        ANALYSIS METHODS:\n",
        "        - analyze_citations(papers) : Analyze citation networks\n",
        "        - detect_gaps(papers) : Detect research gaps\n",
        "        - generate_report(papers) : Generate trend reports\n",
        "\n",
        "        WORKFLOW METHODS:\n",
        "        - full_workflow(name, question, keywords) : Complete research workflow\n",
        "        - demo() : Run comprehensive demonstration\n",
        "\n",
        "        EXAMPLES:\n",
        "        >>> rm = ResearchMate()\n",
        "        >>> rm.demo()  # Run demo\n",
        "        >>> rm.quick_search(\"machine learning\")  # Quick search\n",
        "        >>> rm.ask(\"What is deep learning?\")  # Ask question\n",
        "        >>> rm.create_project(\"My Research\", \"Research question?\", [\"keyword1\", \"keyword2\"])\n",
        "        \"\"\"\n",
        "        print(help_text)\n",
        "        return help_text\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        \"\"\"String representation\"\"\"\n",
        "        return f\"ResearchMate v{self.version} - AI Research Assistant powered by Groq Llama 3.3 70B\"\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        \"\"\"Detailed representation\"\"\"\n",
        "        return f\"ResearchMate(version='{self.version}', initialized_at='{self.initialized_at}')\"\n",
        "\n",
        "# Initialize the unified ResearchMate interface\n",
        "research_mate = ResearchMate()\n",
        "print(\"‚úÖ ResearchMate unified interface ready!\")\n",
        "print(\"\\nüí° Quick start:\")\n",
        "print(\"   research_mate.demo()  # Run comprehensive demo\")\n",
        "print(\"   research_mate.help()  # Show help\")\n",
        "print(\"   research_mate.quick_search('your topic')  # Quick search\")\n",
        "print(\"   research_mate.ask('your question')  # Ask questions\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "35ced5a6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35ced5a6",
        "outputId": "d71ed25f-38f7-4810-d676-38e34a5dc9ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üéØ Ready to run ResearchMate complete demo!\n",
            "üí° Execute: run_complete_demo() to see everything in action\n",
            "\n",
            "üöÄ RESEARCHMATE IS READY!\n",
            "======================================================================\n",
            "üìö Quick Start Commands:\n",
            "   research_mate.demo()                    # Run built-in demo\n",
            "   research_mate.quick_search('topic')     # Quick paper search\n",
            "   research_mate.ask('question')           # Ask research questions\n",
            "   research_mate.help()                    # Show detailed help\n",
            "   run_complete_demo()                     # This comprehensive demo\n",
            "======================================================================\n",
            "\n",
            "üéâ Welcome to ResearchMate - Your AI Research Assistant!\n",
            "   Powered by Groq Llama 3.3 70B with advanced RAG capabilities\n",
            "   Ready to accelerate your research workflow!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# DEMO: COMPLETE RESEARCHMATE WORKFLOW\n",
        "# ============================================================================\n",
        "\n",
        "def run_complete_demo():\n",
        "    \"\"\"Run a complete demonstration of ResearchMate capabilities\"\"\"\n",
        "    print(\"üöÄ STARTING COMPLETE RESEARCHMATE DEMO\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    try:\n",
        "        # Initialize timing\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Step 1: System Status\n",
        "        print(\"1Ô∏è‚É£ SYSTEM STATUS CHECK\")\n",
        "        print(\"-\" * 30)\n",
        "        status = research_mate.status()\n",
        "        if status['success']:\n",
        "            print(f\"‚úÖ System Status: {status['system_status']}\")\n",
        "            print(f\"‚úÖ Components Ready: {len([c for c in status['components'].values() if c == 'ready'])}\")\n",
        "            print(f\"‚úÖ Total Projects: {status['statistics']['projects']['total_projects']}\")\n",
        "        else:\n",
        "            print(f\"‚ùå System Error: {status.get('error', 'Unknown error')}\")\n",
        "\n",
        "        print()\n",
        "\n",
        "        # Step 2: Quick Search Demo\n",
        "        print(\"2Ô∏è‚É£ QUICK SEARCH DEMONSTRATION\")\n",
        "        print(\"-\" * 30)\n",
        "        search_query = \"transformer attention mechanism\"\n",
        "        print(f\"üîç Searching for: '{search_query}'\")\n",
        "\n",
        "        search_result = research_mate.quick_search(search_query, max_results=5)\n",
        "        if search_result['success']:\n",
        "            print(f\"‚úÖ Found {search_result['count']} papers\")\n",
        "            for i, paper in enumerate(search_result['results'][:3], 1):\n",
        "                print(f\"   {i}. {paper.get('title', 'Unknown Title')[:60]}...\")\n",
        "                print(f\"      Authors: {', '.join(paper.get('authors', ['Unknown'])[:2])}\")\n",
        "                print(f\"      Source: {paper.get('source', 'Unknown')}\")\n",
        "        else:\n",
        "            print(f\"‚ùå Search failed: {search_result.get('error', 'Unknown error')}\")\n",
        "\n",
        "        print()\n",
        "\n",
        "        # Step 3: Question Answering Demo\n",
        "        print(\"3Ô∏è‚É£ QUESTION ANSWERING DEMONSTRATION\")\n",
        "        print(\"-\" * 30)\n",
        "        question = \"What is the attention mechanism in transformer models?\"\n",
        "        print(f\"‚ùì Question: {question}\")\n",
        "\n",
        "        qa_result = research_mate.ask(question)\n",
        "        if qa_result['success']:\n",
        "            print(f\"‚úÖ Answer: {qa_result['answer'][:200]}...\")\n",
        "            print(f\"‚úÖ Sources: {len(qa_result.get('sources', []))} papers referenced\")\n",
        "        else:\n",
        "            print(f\"‚ùå QA failed: {qa_result.get('error', 'Unknown error')}\")\n",
        "\n",
        "        print()\n",
        "\n",
        "        # Step 4: Project Creation and Management\n",
        "        print(\"4Ô∏è‚É£ PROJECT MANAGEMENT DEMONSTRATION\")\n",
        "        print(\"-\" * 30)\n",
        "        project_name = \"Transformer Architecture Research\"\n",
        "        research_question = \"How do attention mechanisms improve model performance in transformers?\"\n",
        "        keywords = [\"transformer\", \"attention mechanism\", \"neural networks\", \"deep learning\"]\n",
        "\n",
        "        print(f\"üìù Creating project: '{project_name}'\")\n",
        "        project_result = research_mate.create_project(project_name, research_question, keywords)\n",
        "\n",
        "        if project_result['success']:\n",
        "            project_id = project_result['project_id']\n",
        "            print(f\"‚úÖ Project created with ID: {project_id}\")\n",
        "\n",
        "            # Literature search\n",
        "            print(f\"üîç Conducting literature search...\")\n",
        "            lit_result = research_mate.search_literature(project_id, max_papers=5)\n",
        "            if lit_result['success']:\n",
        "                print(f\"‚úÖ Found {lit_result['papers_found']} papers for the project\")\n",
        "\n",
        "                # Project analysis\n",
        "                print(f\"üìä Analyzing project literature...\")\n",
        "                analysis_result = research_mate.analyze_project(project_id)\n",
        "                if analysis_result['success']:\n",
        "                    print(f\"‚úÖ Literature analysis completed\")\n",
        "                    print(f\"   - Citation network analysis: ‚úÖ\")\n",
        "                    print(f\"   - Trend analysis: ‚úÖ\")\n",
        "                    print(f\"   - Gap analysis: ‚úÖ\")\n",
        "                    print(f\"   - Thematic analysis: ‚úÖ\")\n",
        "                else:\n",
        "                    print(f\"‚ùå Analysis failed: {analysis_result.get('error', 'Unknown error')}\")\n",
        "            else:\n",
        "                print(f\"‚ùå Literature search failed: {lit_result.get('error', 'Unknown error')}\")\n",
        "        else:\n",
        "            print(f\"‚ùå Project creation failed: {project_result.get('error', 'Unknown error')}\")\n",
        "\n",
        "        print()\n",
        "\n",
        "        # Step 5: Trend Analysis Demo\n",
        "        print(\"5Ô∏è‚É£ TREND ANALYSIS DEMONSTRATION\")\n",
        "        print(\"-\" * 30)\n",
        "        trend_topic = \"large language models\"\n",
        "        print(f\"üìà Analyzing trends for: '{trend_topic}'\")\n",
        "\n",
        "        trend_result = research_mate.get_trends(trend_topic)\n",
        "        if trend_result.get('trend_status'):\n",
        "            print(f\"‚úÖ Trend Status: {trend_result['trend_status']}\")\n",
        "            print(f\"‚úÖ Papers Found: {trend_result.get('papers_found', 0)}\")\n",
        "            if 'ai_insights' in trend_result:\n",
        "                print(f\"‚úÖ AI Insights: {trend_result['ai_insights'][:150]}...\")\n",
        "        else:\n",
        "            print(f\"‚ùå Trend analysis failed: {trend_result.get('error', 'Unknown error')}\")\n",
        "\n",
        "        print()\n",
        "\n",
        "        # Step 6: System Statistics\n",
        "        print(\"6Ô∏è‚É£ FINAL SYSTEM STATISTICS\")\n",
        "        print(\"-\" * 30)\n",
        "        final_status = research_mate.status()\n",
        "        if final_status['success']:\n",
        "            stats = final_status['statistics']\n",
        "            print(f\"‚úÖ Total Projects: {stats['projects']['total_projects']}\")\n",
        "            print(f\"‚úÖ RAG Documents: {stats['rag_system'].get('total_documents', 0)}\")\n",
        "            print(f\"‚úÖ Citation Network Authors: {stats['citation_network'].get('author_network', {}).get('network_metrics', {}).get('total_authors', 0)}\")\n",
        "            print(f\"‚úÖ System Status: All components operational\")\n",
        "\n",
        "        # Calculate demo time\n",
        "        end_time = time.time()\n",
        "        demo_duration = end_time - start_time\n",
        "\n",
        "        print()\n",
        "        print(\"üéâ DEMO COMPLETED SUCCESSFULLY!\")\n",
        "        print(\"=\" * 70)\n",
        "        print(f\"‚è±Ô∏è  Demo Duration: {demo_duration:.2f} seconds\")\n",
        "        print(f\"üöÄ ResearchMate is ready for your research projects!\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "        return {\n",
        "            'success': True,\n",
        "            'demo_duration': demo_duration,\n",
        "            'components_tested': ['search', 'qa', 'projects', 'trends', 'analysis'],\n",
        "            'message': 'Complete demo executed successfully'\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå DEMO FAILED: {str(e)}\")\n",
        "        return {\n",
        "            'success': False,\n",
        "            'error': str(e),\n",
        "            'message': 'Demo execution failed'\n",
        "        }\n",
        "\n",
        "# Run the complete demonstration\n",
        "print(\"üéØ Ready to run ResearchMate complete demo!\")\n",
        "print(\"üí° Execute: run_complete_demo() to see everything in action\")\n",
        "print()\n",
        "print(\"üöÄ RESEARCHMATE IS READY!\")\n",
        "print(\"=\" * 70)\n",
        "print(\"üìö Quick Start Commands:\")\n",
        "print(\"   research_mate.demo()                    # Run built-in demo\")\n",
        "print(\"   research_mate.quick_search('topic')     # Quick paper search\")\n",
        "print(\"   research_mate.ask('question')           # Ask research questions\")\n",
        "print(\"   research_mate.help()                    # Show detailed help\")\n",
        "print(\"   run_complete_demo()                     # This comprehensive demo\")\n",
        "print(\"=\" * 70)\n",
        "print()\n",
        "print(\"üéâ Welcome to ResearchMate - Your AI Research Assistant!\")\n",
        "print(\"   Powered by Groq Llama 3.3 70B with advanced RAG capabilities\")\n",
        "print(\"   Ready to accelerate your research workflow!\")\n",
        "\n",
        "# Uncomment the line below to run the demo automatically\n",
        "# run_complete_demo()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "LftQdb_BNKFo",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LftQdb_BNKFo",
        "outputId": "2524c90b-6870-4d00-d8f4-456df89cd64d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üéØ ResearchMate Demo Starting...\n",
            "==================================================\n",
            "1Ô∏è‚É£ System Status Check...\n",
            "   ‚úÖ System Status: operational\n",
            "\n",
            "2Ô∏è‚É£ Quick Search Demo...\n",
            "üîç Quick search: 'transformer attention mechanism'\n",
            "üîç Searching all sources for: 'transformer attention mechanism'\n",
            "üîç Searching arXiv...\n",
            "‚úÖ Found 5 papers on arXiv\n",
            "üîç Searching Semantic Scholar...\n",
            "‚úÖ Found 5 papers on Semantic Scholar\n",
            "üîç Searching CrossRef...\n",
            "‚úÖ Found 5 papers on CrossRef\n",
            "üîç Searching PubMed...\n",
            "‚úÖ Found 5 papers on PubMed\n",
            "‚úÖ Total unique papers found: 20\n",
            "‚úÖ Added 15 document chunks to vectorstore\n",
            "   ‚úÖ Found 20 papers\n",
            "\n",
            "3Ô∏è‚É£ Question Answering Demo...\n",
            "   ‚úÖ Answer: The attention mechanism in transformers is a computational model that is claimed to implement attent...\n",
            "\n",
            "4Ô∏è‚É£ Project Creation Demo...\n",
            "   ‚úÖ Project created: proj_1751997279_3656\n",
            "\n",
            "5Ô∏è‚É£ Literature Search Demo...\n",
            "üîç Conducting literature search for project: Transformer Architecture Analysis\n",
            "üîç Searching for keyword: transformer\n",
            "üîç Searching all sources for: 'transformer'\n",
            "üîç Searching arXiv...\n",
            "‚úÖ Found 3 papers on arXiv\n",
            "üîç Searching Semantic Scholar...\n",
            "‚úÖ Found 3 papers on Semantic Scholar\n",
            "üîç Searching CrossRef...\n",
            "‚úÖ Found 3 papers on CrossRef\n",
            "üîç Searching PubMed...\n",
            "‚úÖ Found 3 papers on PubMed\n",
            "‚úÖ Total unique papers found: 12\n",
            "üîç Searching for keyword: attention mechanism\n",
            "üîç Searching all sources for: 'attention mechanism'\n",
            "üîç Searching arXiv...\n",
            "‚úÖ Found 3 papers on arXiv\n",
            "üîç Searching Semantic Scholar...\n",
            "‚úÖ Found 3 papers on Semantic Scholar\n",
            "üîç Searching CrossRef...\n",
            "‚úÖ Found 3 papers on CrossRef\n",
            "üîç Searching PubMed...\n",
            "‚úÖ Found 3 papers on PubMed\n",
            "‚úÖ Total unique papers found: 12\n",
            "üîç Searching for keyword: neural networks\n",
            "üîç Searching all sources for: 'neural networks'\n",
            "üîç Searching arXiv...\n",
            "‚úÖ Found 3 papers on arXiv\n",
            "üîç Searching Semantic Scholar...\n",
            "‚úÖ Found 3 papers on Semantic Scholar\n",
            "üîç Searching CrossRef...\n",
            "‚úÖ Found 3 papers on CrossRef\n",
            "üîç Searching PubMed...\n",
            "‚úÖ Found 3 papers on PubMed\n",
            "‚úÖ Total unique papers found: 11\n",
            "‚úÖ Added 24 document chunks to vectorstore\n",
            "üìù Processing 33 papers...\n",
            "‚úÖ Successfully processed 33 papers (0 errors)\n",
            "üìä Total papers: 35\n",
            "üìä Total authors: 109\n",
            "üìä Author graph nodes: 109\n",
            "   ‚úÖ Found 33 papers\n",
            "\n",
            "6Ô∏è‚É£ Trend Analysis Demo...\n",
            "‚úÖ Found 50 trending papers in cs.AI\n",
            "‚úÖ Found 10 papers on Semantic Scholar\n",
            "   ‚úÖ Trend status: active\n",
            "\n",
            "==================================================\n",
            "üéâ Demo completed successfully!\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'success': True,\n",
              " 'demo_completed': True,\n",
              " 'demo_results': {'system_status': {'system_status': 'operational',\n",
              "   'components': {'groq_processor': 'ready',\n",
              "    'rag_system': 'Ready',\n",
              "    'arxiv_fetcher': 'ready',\n",
              "    'multi_source_collector': 'ready',\n",
              "    'pdf_processor': 'ready',\n",
              "    'citation_analyzer': 'ready',\n",
              "    'trend_monitor': 'ready'},\n",
              "   'statistics': {'projects': {'total_projects': 2,\n",
              "     'active_projects': 2,\n",
              "     'completed_projects': 0,\n",
              "     'total_papers': 66,\n",
              "     'total_reviews': 0},\n",
              "    'rag_system': {'total_documents': 111,\n",
              "     'embedding_model': 'sentence-transformers/all-MiniLM-L6-v2',\n",
              "     'chunk_size': 2000,\n",
              "     'chunk_overlap': 400,\n",
              "     'vectorstore_type': 'ChromaDB',\n",
              "     'status': 'Ready'},\n",
              "    'citation_network': {'author_network': {'network_metrics': {'total_authors': 109,\n",
              "       'total_collaborations': 288,\n",
              "       'network_density': 0.04892966360856269,\n",
              "       'number_of_components': 27,\n",
              "       'largest_component_size': 13},\n",
              "      'top_collaborators': [('Diego Rojo', 12),\n",
              "       ('Alba Jim√©nez-Masip', 12),\n",
              "       ('Laura Pag√®s', 12),\n",
              "       ('Juan Ba√±ares', 12),\n",
              "       ('Clara Sabiote', 12),\n",
              "       ('Mar√≠a Mart√≠nez-G√≥mez', 12),\n",
              "       ('Laia Aceituno', 12),\n",
              "       ('M Serra Cusid√≥', 12),\n",
              "       ('M Teresa Salcedo-Allende', 12),\n",
              "       ('Zyanya Calixto', 12)],\n",
              "      'top_productive_authors': [('George Sparling', 1),\n",
              "       ('Yasushi Kajihara', 1),\n",
              "       ('Xing-Tang Dong', 1),\n",
              "       ('Kehe Zhu', 1),\n",
              "       ('Ze Liu', 1),\n",
              "       ('Yutong Lin', 1),\n",
              "       ('Yue Cao', 1),\n",
              "       ('Han Hu', 1),\n",
              "       ('Yixuan Wei', 1),\n",
              "       ('Zheng Zhang', 1)],\n",
              "      'top_cited_authors': [('Ze Liu', 21778),\n",
              "       ('Yutong Lin', 21778),\n",
              "       ('Yue Cao', 21778),\n",
              "       ('Han Hu', 21778),\n",
              "       ('Yixuan Wei', 21778),\n",
              "       ('Zheng Zhang', 21778),\n",
              "       ('Stephen Lin', 21778),\n",
              "       ('B. Guo', 21778),\n",
              "       ('Colin Raffel', 20431),\n",
              "       ('Noam M. Shazeer', 20431)],\n",
              "      'top_central_authors': [('George Sparling', 0.0),\n",
              "       ('Yasushi Kajihara', 0.0),\n",
              "       ('Xing-Tang Dong', 0.0),\n",
              "       ('Kehe Zhu', 0.0),\n",
              "       ('Ze Liu', 0.0),\n",
              "       ('Yutong Lin', 0.0),\n",
              "       ('Yue Cao', 0.0),\n",
              "       ('Han Hu', 0.0),\n",
              "       ('Yixuan Wei', 0.0),\n",
              "       ('Zheng Zhang', 0.0)],\n",
              "      'analysis_timestamp': '2025-07-08T17:54:26.742509'},\n",
              "     'paper_network': {'network_metrics': {'total_papers': 35,\n",
              "       'total_citations': 0,\n",
              "       'network_density': 0,\n",
              "       'number_of_components': 35,\n",
              "       'largest_component_size': 1},\n",
              "      'most_cited_papers': [('The Xi-transform for conformally flat space-time',\n",
              "        0),\n",
              "       ('Multiple basic hypergeometric transformation formulas arising from the balanced duality transformation',\n",
              "        0),\n",
              "       ('The Fourier and Hilbert transforms under the Bargmann transform', 0),\n",
              "       ('Swin Transformer: Hierarchical Vision Transformer using Shifted Windows',\n",
              "        0),\n",
              "       ('Longformer: The Long-Document Transformer', 0),\n",
              "       ('Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer',\n",
              "        0),\n",
              "       ('Transformer ratings and transformer life', 0),\n",
              "       ('Figure 4: Hierarchical transformer module (H-Transformer).', 0),\n",
              "       ('Transformer Testing', 0),\n",
              "       ('Application of Transformer Neural Networks for Data Cleaning in Emergency Room Logs: A Case Study from the Bordeaux University Hospital.',\n",
              "        0)],\n",
              "      'most_citing_papers': [('The Xi-transform for conformally flat space-time',\n",
              "        0),\n",
              "       ('Multiple basic hypergeometric transformation formulas arising from the balanced duality transformation',\n",
              "        0),\n",
              "       ('The Fourier and Hilbert transforms under the Bargmann transform', 0),\n",
              "       ('Swin Transformer: Hierarchical Vision Transformer using Shifted Windows',\n",
              "        0),\n",
              "       ('Longformer: The Long-Document Transformer', 0),\n",
              "       ('Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer',\n",
              "        0),\n",
              "       ('Transformer ratings and transformer life', 0),\n",
              "       ('Figure 4: Hierarchical transformer module (H-Transformer).', 0),\n",
              "       ('Transformer Testing', 0),\n",
              "       ('Application of Transformer Neural Networks for Data Cleaning in Emergency Room Logs: A Case Study from the Bordeaux University Hospital.',\n",
              "        0)],\n",
              "      'analysis_timestamp': '2025-07-08T17:54:26.742860'},\n",
              "     'overall_stats': {'total_papers': 35,\n",
              "      'total_authors': 109,\n",
              "      'papers_per_author': 0.3211009174311927,\n",
              "      'collaborations_per_author': 2.6422018348623855},\n",
              "     'analysis_timestamp': '2025-07-08T17:54:26.742904'}},\n",
              "   'timestamp': '2025-07-08T17:54:26.742931'},\n",
              "  'quick_search': {'success': True,\n",
              "   'query': 'transformer attention mechanism',\n",
              "   'results': [{'title': 'Generalized Probabilistic Attention Mechanism in Transformers',\n",
              "     'authors': ['DongNyeong Heo', 'Heeyoul Choi'],\n",
              "     'abstract': 'The Transformer architecture has become widely adopted due to its\\ndemonstrated success, attributed to the attention mechanism at its core.\\nDespite these successes, the attention mechanism of Transformers is associated\\nwith two well-known issues: rank-collapse and gradient vanishing. In this\\npaper, we present a theoretical analysis that it is inherently difficult to\\naddress both issues simultaneously in the conventional attention mechanism. To\\nhandle these issues, we introduce a novel class of attention mechanism,\\nreferred to as generalized probabilistic attention mechanism (GPAM), and its\\ndual-attention implementation within the Transformer architecture. Unlike\\nconventional attention mechanisms, GPAM allows for negative attention scores\\nwhile preserving a fixed total sum. We provide theoretical evidence that the\\nproposed dual-attention GPAM (daGPAM) effectively mitigates both the\\nrank-collapse and gradient vanishing issues which are difficult to resolve\\nsimultaneously with the conventional attention mechanisms. Furthermore, we\\nempirically validate this theoretical evidence, demonstrating the superiority\\nof daGPAM compared to other alternative attention mechanisms that were proposed\\nto address the same issues. Additionally, we demonstrate the practical benefits\\nof GPAM in natural language processing tasks, such as language modeling and\\nneural machine translation.',\n",
              "     'url': 'http://arxiv.org/abs/2410.15578v1',\n",
              "     'pdf_url': 'http://arxiv.org/pdf/2410.15578v1',\n",
              "     'published': '2024-10-21T01:55:52+00:00',\n",
              "     'updated': '2024-10-21T01:55:52+00:00',\n",
              "     'categories': ['cs.LG', 'cs.CL'],\n",
              "     'primary_category': 'cs.LG',\n",
              "     'source': 'arXiv',\n",
              "     'year': 2024,\n",
              "     'content': 'The Transformer architecture has become widely adopted due to its\\ndemonstrated success, attributed to the attention mechanism at its core.\\nDespite these successes, the attention mechanism of Transformers is associated\\nwith two well-known issues: rank-collapse and gradient vanishing. In this\\npaper, we present a theoretical analysis that it is inherently difficult to\\naddress both issues simultaneously in the conventional attention mechanism. To\\nhandle these issues, we introduce a novel class of attention mechanism,\\nreferred to as generalized probabilistic attention mechanism (GPAM), and its\\ndual-attention implementation within the Transformer architecture. Unlike\\nconventional attention mechanisms, GPAM allows for negative attention scores\\nwhile preserving a fixed total sum. We provide theoretical evidence that the\\nproposed dual-attention GPAM (daGPAM) effectively mitigates both the\\nrank-collapse and gradient vanishing issues which are difficult to resolve\\nsimultaneously with the conventional attention mechanisms. Furthermore, we\\nempirically validate this theoretical evidence, demonstrating the superiority\\nof daGPAM compared to other alternative attention mechanisms that were proposed\\nto address the same issues. Additionally, we demonstrate the practical benefits\\nof GPAM in natural language processing tasks, such as language modeling and\\nneural machine translation.'},\n",
              "    {'title': 'Adaptive Sparse and Monotonic Attention for Transformer-based Automatic Speech Recognition',\n",
              "     'authors': ['Chendong Zhao',\n",
              "      'Jianzong Wang',\n",
              "      'Wen qi Wei',\n",
              "      'Xiaoyang Qu',\n",
              "      'Haoqian Wang',\n",
              "      'Jing Xiao'],\n",
              "     'abstract': 'The Transformer architecture model, based on self-attention and multi-head\\nattention, has achieved remarkable success in offline end-to-end Automatic\\nSpeech Recognition (ASR). However, self-attention and multi-head attention\\ncannot be easily applied for streaming or online ASR. For self-attention in\\nTransformer ASR, the softmax normalization function-based attention mechanism\\nmakes it impossible to highlight important speech information. For multi-head\\nattention in Transformer ASR, it is not easy to model monotonic alignments in\\ndifferent heads. To overcome these two limits, we integrate sparse attention\\nand monotonic attention into Transformer-based ASR. The sparse mechanism\\nintroduces a learned sparsity scheme to enable each self-attention structure to\\nfit the corresponding head better. The monotonic attention deploys\\nregularization to prune redundant heads for the multi-head attention structure.\\nThe experiments show that our method can effectively improve the attention\\nmechanism on widely used benchmarks of speech recognition.',\n",
              "     'url': 'http://arxiv.org/abs/2209.15176v1',\n",
              "     'pdf_url': 'http://arxiv.org/pdf/2209.15176v1',\n",
              "     'published': '2022-09-30T01:55:57+00:00',\n",
              "     'updated': '2022-09-30T01:55:57+00:00',\n",
              "     'categories': ['cs.CL', 'cs.AI'],\n",
              "     'primary_category': 'cs.CL',\n",
              "     'source': 'arXiv',\n",
              "     'year': 2022,\n",
              "     'content': 'The Transformer architecture model, based on self-attention and multi-head\\nattention, has achieved remarkable success in offline end-to-end Automatic\\nSpeech Recognition (ASR). However, self-attention and multi-head attention\\ncannot be easily applied for streaming or online ASR. For self-attention in\\nTransformer ASR, the softmax normalization function-based attention mechanism\\nmakes it impossible to highlight important speech information. For multi-head\\nattention in Transformer ASR, it is not easy to model monotonic alignments in\\ndifferent heads. To overcome these two limits, we integrate sparse attention\\nand monotonic attention into Transformer-based ASR. The sparse mechanism\\nintroduces a learned sparsity scheme to enable each self-attention structure to\\nfit the corresponding head better. The monotonic attention deploys\\nregularization to prune redundant heads for the multi-head attention structure.\\nThe experiments show that our method can effectively improve the attention\\nmechanism on widely used benchmarks of speech recognition.'},\n",
              "    {'title': 'Self-attention in Vision Transformers Performs Perceptual Grouping, Not Attention',\n",
              "     'authors': ['Paria Mehrani', 'John K. Tsotsos'],\n",
              "     'abstract': 'Recently, a considerable number of studies in computer vision involves deep\\nneural architectures called vision transformers. Visual processing in these\\nmodels incorporates computational models that are claimed to implement\\nattention mechanisms. Despite an increasing body of work that attempts to\\nunderstand the role of attention mechanisms in vision transformers, their\\neffect is largely unknown. Here, we asked if the attention mechanisms in vision\\ntransformers exhibit similar effects as those known in human visual attention.\\nTo answer this question, we revisited the attention formulation in these models\\nand found that despite the name, computationally, these models perform a\\nspecial class of relaxation labeling with similarity grouping effects.\\nAdditionally, whereas modern experimental findings reveal that human visual\\nattention involves both feed-forward and feedback mechanisms, the purely\\nfeed-forward architecture of vision transformers suggests that attention in\\nthese models will not have the same effects as those known in humans. To\\nquantify these observations, we evaluated grouping performance in a family of\\nvision transformers. Our results suggest that self-attention modules group\\nfigures in the stimuli based on similarity in visual features such as color.\\nAlso, in a singleton detection experiment as an instance of saliency detection,\\nwe studied if these models exhibit similar effects as those of feed-forward\\nvisual salience mechanisms utilized in human visual attention. We found that\\ngenerally, the transformer-based attention modules assign more salience either\\nto distractors or the ground. Together, our study suggests that the attention\\nmechanisms in vision transformers perform similarity grouping and not\\nattention.',\n",
              "     'url': 'http://arxiv.org/abs/2303.01542v1',\n",
              "     'pdf_url': 'http://arxiv.org/pdf/2303.01542v1',\n",
              "     'published': '2023-03-02T19:18:11+00:00',\n",
              "     'updated': '2023-03-02T19:18:11+00:00',\n",
              "     'categories': ['cs.CV'],\n",
              "     'primary_category': 'cs.CV',\n",
              "     'source': 'arXiv',\n",
              "     'year': 2023,\n",
              "     'content': 'Recently, a considerable number of studies in computer vision involves deep\\nneural architectures called vision transformers. Visual processing in these\\nmodels incorporates computational models that are claimed to implement\\nattention mechanisms. Despite an increasing body of work that attempts to\\nunderstand the role of attention mechanisms in vision transformers, their\\neffect is largely unknown. Here, we asked if the attention mechanisms in vision\\ntransformers exhibit similar effects as those known in human visual attention.\\nTo answer this question, we revisited the attention formulation in these models\\nand found that despite the name, computationally, these models perform a\\nspecial class of relaxation labeling with similarity grouping effects.\\nAdditionally, whereas modern experimental findings reveal that human visual\\nattention involves both feed-forward and feedback mechanisms, the purely\\nfeed-forward architecture of vision transformers suggests that attention in\\nthese models will not have the same effects as those known in humans. To\\nquantify these observations, we evaluated grouping performance in a family of\\nvision transformers. Our results suggest that self-attention modules group\\nfigures in the stimuli based on similarity in visual features such as color.\\nAlso, in a singleton detection experiment as an instance of saliency detection,\\nwe studied if these models exhibit similar effects as those of feed-forward\\nvisual salience mechanisms utilized in human visual attention. We found that\\ngenerally, the transformer-based attention modules assign more salience either\\nto distractors or the ground. Together, our study suggests that the attention\\nmechanisms in vision transformers perform similarity grouping and not\\nattention.'},\n",
              "    {'title': 'Continuous-Time Attention: PDE-Guided Mechanisms for Long-Sequence Transformers',\n",
              "     'authors': ['Yukun Zhang', 'Xueqing Zhou'],\n",
              "     'abstract': \"We propose a novel framework, Continuous_Time Attention, which infuses\\npartial differential equations (PDEs) into the Transformer's attention\\nmechanism to address the challenges of extremely long input sequences. Instead\\nof relying solely on a static attention matrix, we allow attention weights to\\nevolve over a pseudo_time dimension via diffusion, wave, or reaction_diffusion\\ndynamics. This mechanism systematically smooths local noise, enhances\\nlong_range dependencies, and stabilizes gradient flow. Theoretically, our\\nanalysis shows that PDE_based attention leads to better optimization landscapes\\nand polynomial rather than exponential decay of distant interactions.\\nEmpirically, we benchmark our method on diverse experiments_demonstrating\\nconsistent gains over both standard and specialized long sequence Transformer\\nvariants. Our findings highlight the potential of PDE_based formulations to\\nenrich attention mechanisms with continuous_time dynamics and global coherence.\",\n",
              "     'url': 'http://arxiv.org/abs/2505.20666v1',\n",
              "     'pdf_url': 'http://arxiv.org/pdf/2505.20666v1',\n",
              "     'published': '2025-05-27T03:30:10+00:00',\n",
              "     'updated': '2025-05-27T03:30:10+00:00',\n",
              "     'categories': ['cs.LG', 'cs.AI'],\n",
              "     'primary_category': 'cs.LG',\n",
              "     'source': 'arXiv',\n",
              "     'year': 2025,\n",
              "     'content': \"We propose a novel framework, Continuous_Time Attention, which infuses\\npartial differential equations (PDEs) into the Transformer's attention\\nmechanism to address the challenges of extremely long input sequences. Instead\\nof relying solely on a static attention matrix, we allow attention weights to\\nevolve over a pseudo_time dimension via diffusion, wave, or reaction_diffusion\\ndynamics. This mechanism systematically smooths local noise, enhances\\nlong_range dependencies, and stabilizes gradient flow. Theoretically, our\\nanalysis shows that PDE_based attention leads to better optimization landscapes\\nand polynomial rather than exponential decay of distant interactions.\\nEmpirically, we benchmark our method on diverse experiments_demonstrating\\nconsistent gains over both standard and specialized long sequence Transformer\\nvariants. Our findings highlight the potential of PDE_based formulations to\\nenrich attention mechanisms with continuous_time dynamics and global coherence.\"},\n",
              "    {'title': 'Armour: Generalizable Compact Self-Attention for Vision Transformers',\n",
              "     'authors': ['Lingchuan Meng'],\n",
              "     'abstract': 'Attention-based transformer networks have demonstrated promising potential as\\ntheir applications extend from natural language processing to vision. However,\\ndespite the recent improvements, such as sub-quadratic attention approximation\\nand various training enhancements, the compact vision transformers to date\\nusing the regular attention still fall short in comparison with its convnet\\ncounterparts, in terms of \\\\textit{accuracy,} \\\\textit{model size}, \\\\textit{and}\\n\\\\textit{throughput}. This paper introduces a compact self-attention mechanism\\nthat is fundamental and highly generalizable. The proposed method reduces\\nredundancy and improves efficiency on top of the existing attention\\noptimizations. We show its drop-in applicability for both the regular attention\\nmechanism and some most recent variants in vision transformers. As a result, we\\nproduced smaller and faster models with the same or better accuracies.',\n",
              "     'url': 'http://arxiv.org/abs/2108.01778v1',\n",
              "     'pdf_url': 'http://arxiv.org/pdf/2108.01778v1',\n",
              "     'published': '2021-08-03T22:33:58+00:00',\n",
              "     'updated': '2021-08-03T22:33:58+00:00',\n",
              "     'categories': ['cs.CV'],\n",
              "     'primary_category': 'cs.CV',\n",
              "     'source': 'arXiv',\n",
              "     'year': 2021,\n",
              "     'content': 'Attention-based transformer networks have demonstrated promising potential as\\ntheir applications extend from natural language processing to vision. However,\\ndespite the recent improvements, such as sub-quadratic attention approximation\\nand various training enhancements, the compact vision transformers to date\\nusing the regular attention still fall short in comparison with its convnet\\ncounterparts, in terms of \\\\textit{accuracy,} \\\\textit{model size}, \\\\textit{and}\\n\\\\textit{throughput}. This paper introduces a compact self-attention mechanism\\nthat is fundamental and highly generalizable. The proposed method reduces\\nredundancy and improves efficiency on top of the existing attention\\noptimizations. We show its drop-in applicability for both the regular attention\\nmechanism and some most recent variants in vision transformers. As a result, we\\nproduced smaller and faster models with the same or better accuracies.'},\n",
              "    {'title': 'ST-DAAE: An Enhanced Swin Transformer Attention Mechanism for Leaf Disease Classification',\n",
              "     'authors': ['Madhavi', 'T. Singh', 'Shailendra Pratap Singh'],\n",
              "     'abstract': \"This work presents a novel dual-technique method that uses Drop Attention (DA) for regularization and Attention Entropy Loss (AE) for optimization in the Swin Transformer framework to improve leaf disease classification. We have introduced sophisticated mathematical formulations that improve the model's foundations and performance, these methods provide notable improvements over previous approaches. The experimental findings show significant gains in accuracy: the Swin Transformer equipped with Drop Attention obtained a remarkable accuracy of 98.91%, whereas the variation using Attention Entropy Loss only managed 85%. These improvements increase the precision of leaf disease classification systems but also provide important tools for the progress of agricultural technology.\",\n",
              "     'year': 2024,\n",
              "     'url': 'https://www.semanticscholar.org/paper/7e109394990a54e7cb915f7ec891045bdeeab4c5',\n",
              "     'source': 'Semantic Scholar',\n",
              "     'citation_count': 0,\n",
              "     'venue': '2024 International Conference on Cybernation and Computation (CYBERCOM)',\n",
              "     'publication_date': '2024-11-15',\n",
              "     'content': \"This work presents a novel dual-technique method that uses Drop Attention (DA) for regularization and Attention Entropy Loss (AE) for optimization in the Swin Transformer framework to improve leaf disease classification. We have introduced sophisticated mathematical formulations that improve the model's foundations and performance, these methods provide notable improvements over previous approaches. The experimental findings show significant gains in accuracy: the Swin Transformer equipped with Drop Attention obtained a remarkable accuracy of 98.91%, whereas the variation using Attention Entropy Loss only managed 85%. These improvements increase the precision of leaf disease classification systems but also provide important tools for the progress of agricultural technology.\",\n",
              "     'external_ids': {'DOI': '10.1109/CYBERCOM63683.2024.10803175',\n",
              "      'CorpusId': 275014315}},\n",
              "    {'title': 'Multiple paddy disease recognition methods based on deformable transformer attention mechanism in complex scenarios',\n",
              "     'authors': ['Xinyu Zhang',\n",
              "      'Hang Dong',\n",
              "      'Liang Gong',\n",
              "      'Xin Cheng',\n",
              "      'Zhenghui Ge',\n",
              "      'Liangchao Guo'],\n",
              "     'abstract': 'Paddy disease recognition presents challenges in the agricultural industry, and existing algorithms struggle to accurately identify diseases in complex scenarios. In this paper, we propose a precise object detection framework to address the challenges of severe overlap, multi-disease detection, morphological irregularities, multi-scale object classification, and complex scenarios in real-world environments in paddy disease detection. The proposed model is based on an improved version of the DEtection TRansformer (Detr) algorithm. The enhanced network architecture fuses multi-scale features by adding a feature fusion module after the backbone network, which is able to retain more original information of the images and greatly improves the detection accuracy; the use of deformable attention module greatly reduces the computational complexity of the model. To evaluate the PDN, a dedicated paddy disease detection dataset with 1200 images is created. Experimental results demonstrate that the proposed model obtained a precision value of 100%, a recall value of 89.3%, F1-score of 94.3%, and a mean average precision (mAP) value of 60.2%. The model outperforms the existing state-of-the-art detection models in detection accuracy.',\n",
              "     'year': 2023,\n",
              "     'url': 'https://www.semanticscholar.org/paper/8f4b90c336a005978606d4c3eb8e8ca71ceca467',\n",
              "     'source': 'Semantic Scholar',\n",
              "     'citation_count': 2,\n",
              "     'venue': 'International Journal of Computer Applications',\n",
              "     'publication_date': '2023-10-03',\n",
              "     'content': 'Paddy disease recognition presents challenges in the agricultural industry, and existing algorithms struggle to accurately identify diseases in complex scenarios. In this paper, we propose a precise object detection framework to address the challenges of severe overlap, multi-disease detection, morphological irregularities, multi-scale object classification, and complex scenarios in real-world environments in paddy disease detection. The proposed model is based on an improved version of the DEtection TRansformer (Detr) algorithm. The enhanced network architecture fuses multi-scale features by adding a feature fusion module after the backbone network, which is able to retain more original information of the images and greatly improves the detection accuracy; the use of deformable attention module greatly reduces the computational complexity of the model. To evaluate the PDN, a dedicated paddy disease detection dataset with 1200 images is created. Experimental results demonstrate that the proposed model obtained a precision value of 100%, a recall value of 89.3%, F1-score of 94.3%, and a mean average precision (mAP) value of 60.2%. The model outperforms the existing state-of-the-art detection models in detection accuracy.',\n",
              "     'external_ids': {'DOI': '10.1080/1206212X.2023.2263254',\n",
              "      'CorpusId': 263823079}},\n",
              "    {'title': 'DHHG-TAC: Fusion of Dynamic Heterogeneous Hypergraphs and Transformer Attention Mechanism for Visual Question Answering Tasks',\n",
              "     'authors': ['Xuetao Liu', 'Ruiliang Dong', 'Hongyan Yang'],\n",
              "     'abstract': \"Amidst the burgeoning advancements in deep learning, traditional neural networks have demonstrated significant achievements in unimodal tasks such as image recognition. However, the handling of multimodal data, especially in visual question answering (VQA) tasks, presents challenges in processing the complex structural relationships among modalities. To address this issue, this article introduces a dynamic heterogeneous hypergraph neural network (HGNN) model that utilizes a Transformer-based combined attention mechanism and designs a hypergraph representation imaging network to enhance model inference without increasing parameter count. Initially, image scenes and textual questions are converted into pairs of hypergraphs with preliminary weights, which facilitate the capture of complex structural relationships through the HGNN. The hypergraph representation imaging network further aids the HGNN in learning and understanding the scene image modalities. Subsequently, a transformer-based combined attention mechanism is employed to adapt to the distinct characteristics of each modality and their intermodal interactions. This integration of multiple attention mechanisms helps identify critical structural information within the answer regions. Dynamic updates to the hyperedge weights of the hypergraph pairs, guided by the attention weights, enable the model to assimilate more relevant information progressively. Experiments on two public VQA datasets attest to the model's superior performance. Furthermore, this article envisions future advancements in model optimization and feature information extraction, extending the potential of HGNNs in multimodal fusion technology.\",\n",
              "     'year': 2025,\n",
              "     'url': 'https://www.semanticscholar.org/paper/019d474a22631ffd5dd10f60a0d9a0ba08f9a745',\n",
              "     'source': 'Semantic Scholar',\n",
              "     'citation_count': 1,\n",
              "     'venue': 'IEEE Transactions on Industrial Informatics',\n",
              "     'publication_date': '2025-01-01',\n",
              "     'content': \"Amidst the burgeoning advancements in deep learning, traditional neural networks have demonstrated significant achievements in unimodal tasks such as image recognition. However, the handling of multimodal data, especially in visual question answering (VQA) tasks, presents challenges in processing the complex structural relationships among modalities. To address this issue, this article introduces a dynamic heterogeneous hypergraph neural network (HGNN) model that utilizes a Transformer-based combined attention mechanism and designs a hypergraph representation imaging network to enhance model inference without increasing parameter count. Initially, image scenes and textual questions are converted into pairs of hypergraphs with preliminary weights, which facilitate the capture of complex structural relationships through the HGNN. The hypergraph representation imaging network further aids the HGNN in learning and understanding the scene image modalities. Subsequently, a transformer-based combined attention mechanism is employed to adapt to the distinct characteristics of each modality and their intermodal interactions. This integration of multiple attention mechanisms helps identify critical structural information within the answer regions. Dynamic updates to the hyperedge weights of the hypergraph pairs, guided by the attention weights, enable the model to assimilate more relevant information progressively. Experiments on two public VQA datasets attest to the model's superior performance. Furthermore, this article envisions future advancements in model optimization and feature information extraction, extending the potential of HGNNs in multimodal fusion technology.\",\n",
              "     'external_ids': {'DBLP': 'journals/tii/LiuDY25',\n",
              "      'DOI': '10.1109/TII.2024.3453919',\n",
              "      'CorpusId': 272999614}},\n",
              "    {'title': 'Dealing with a Data-limited Regime: Combining Transfer Learning And Transformer Attention Mechanism to Increase Aqueous Solubility Prediction Performance',\n",
              "     'authors': ['Magdalena Wiercioch', 'J. Kirchmair'],\n",
              "     'abstract': '',\n",
              "     'year': 2021,\n",
              "     'url': 'https://www.semanticscholar.org/paper/1c42be1b970d80383bb80fef1a9dc16e8c130e0e',\n",
              "     'source': 'Semantic Scholar',\n",
              "     'citation_count': 8,\n",
              "     'venue': 'Artificial Intelligence in the Life Sciences',\n",
              "     'publication_date': '2021-11-01',\n",
              "     'content': '',\n",
              "     'external_ids': {'DOI': '10.1016/j.ailsci.2021.100021',\n",
              "      'CorpusId': 244697219}},\n",
              "    {'title': 'Energy-efficient tugboat scheduling: A hybrid transformer-attention mechanism and artificial multiple intelligence system',\n",
              "     'authors': ['Rapeepan Pitakaso',\n",
              "      'Kanchana Sethanan',\n",
              "      'Sarayut Gonwirat',\n",
              "      'Chen-Fu Chien',\n",
              "      'M. K. Lim',\n",
              "      'Ming-Lang Tseng'],\n",
              "     'abstract': '',\n",
              "     'year': 2025,\n",
              "     'url': 'https://www.semanticscholar.org/paper/40205636069c955aadd1688a43ef86cf2bd160fb',\n",
              "     'source': 'Semantic Scholar',\n",
              "     'citation_count': 0,\n",
              "     'venue': 'Computers & industrial engineering',\n",
              "     'publication_date': '2025-04-01',\n",
              "     'content': '',\n",
              "     'external_ids': {'DBLP': 'journals/candie/PitakasoSGCLT25',\n",
              "      'DOI': '10.1016/j.cie.2025.111112',\n",
              "      'CorpusId': 277754496}},\n",
              "    {'title': 'Generalized Attention Mechanism and Relative Position for Transformer',\n",
              "     'authors': ['Raja Vikram Pandya'],\n",
              "     'abstract': '',\n",
              "     'year': None,\n",
              "     'url': 'https://doi.org/10.31224/2476',\n",
              "     'doi': '10.31224/2476',\n",
              "     'publisher': 'Open Engineering Inc',\n",
              "     'journal': '',\n",
              "     'source': 'CrossRef',\n",
              "     'content': ''},\n",
              "    {'title': 'Combining Transformer and Reverse Attention Mechanism for Polyp Segmentation',\n",
              "     'authors': ['Jianzhuang Lin', 'Wenzhong Yang', 'Sixiang Tan'],\n",
              "     'abstract': '',\n",
              "     'year': 2022,\n",
              "     'url': 'https://doi.org/10.5220/0012014800003633',\n",
              "     'doi': '10.5220/0012014800003633',\n",
              "     'publisher': 'SCITEPRESS - Science and Technology Publications',\n",
              "     'journal': 'Proceedings of the 4th International Conference on Biotechnology and Biomedicine',\n",
              "     'source': 'CrossRef',\n",
              "     'content': ''},\n",
              "    {'title': 'FCA-Transformer: A Feature Pyramid Time Series Forecasting Model Driven by Cross-Attention Mechanism',\n",
              "     'authors': ['* jiaoyu'],\n",
              "     'abstract': '',\n",
              "     'year': None,\n",
              "     'url': 'https://doi.org/10.22541/au.173772018.80603459/v1',\n",
              "     'doi': '10.22541/au.173772018.80603459/v1',\n",
              "     'publisher': 'Wiley',\n",
              "     'journal': '',\n",
              "     'source': 'CrossRef',\n",
              "     'content': ''},\n",
              "    {'title': 'Revolutionizing Wireless Traffic Usage Forecasting: Transformer with Attention Mechanism',\n",
              "     'authors': ['Bandu Uppalaiah', 'D. Mallikarjuna Reddy', 'A. Srilath'],\n",
              "     'abstract': '<jats:p id=\"p1\">Revolutionizing wireless traffic forecasting empowers proactive resource\\nallocation, optimizing network performance and ensuring efficient\\nutilization of resources in dynamic wireless environments. real-time\\ntraffic data from a business network with There are 470 APs.), this\\nresearch provides a thorough examination of the temporal and\\ngeographical dynamics of network traffic. Time series data forecasting\\nis given a new spin with the help of machine learning models built on\\nthe Transformer framework. This approach uses the brain‚Äôs attentional\\nprocesses to analyze time series data for hidden dynamics and complex\\npatterns. Notably, the analysis identifies high-traffic-utilization AP\\ngroups exhibiting robust seasonality patterns, alongside those devoid of\\nsuch patterns. Several different types of forecasting methods are used\\nand evaluated in this research, among them the Holt-Winters technique, a\\nSARIMA model, a GRU model, a CNN model, and a model based on\\nconvolutional neural networks. In conclusion, the research sheds light\\non the complex patterns underlying network traffic and presents an\\ninnovative forecasting approach, bolstering the potential for improved\\nwireless network resource management.</jats:p>',\n",
              "     'year': None,\n",
              "     'url': 'https://doi.org/10.22541/au.169769165.55793181/v1',\n",
              "     'doi': '10.22541/au.169769165.55793181/v1',\n",
              "     'publisher': 'Wiley',\n",
              "     'journal': '',\n",
              "     'source': 'CrossRef',\n",
              "     'content': '<jats:p id=\"p1\">Revolutionizing wireless traffic forecasting empowers proactive resource\\nallocation, optimizing network performance and ensuring efficient\\nutilization of resources in dynamic wireless environments. real-time\\ntraffic data from a business network with There are 470 APs.), this\\nresearch provides a thorough examination of the temporal and\\ngeographical dynamics of network traffic. Time series data forecasting\\nis given a new spin with the help of machine learning models built on\\nthe Transformer framework. This approach uses the brain‚Äôs attentional\\nprocesses to analyze time series data for hidden dynamics and complex\\npatterns. Notably, the analysis identifies high-traffic-utilization AP\\ngroups exhibiting robust seasonality patterns, alongside those devoid of\\nsuch patterns. Several different types of forecasting methods are used\\nand evaluated in this research, among them the Holt-Winters technique, a\\nSARIMA model, a GRU model, a CNN model, and a model based on\\nconvolutional neural networks. In conclusion, the research sheds light\\non the complex patterns underlying network traffic and presents an\\ninnovative forecasting approach, bolstering the potential for improved\\nwireless network resource management.</jats:p>'},\n",
              "    {'title': 'transformer: Implementation of Transformer Deep Neural Network with Vignettes',\n",
              "     'authors': ['Bastiaan Quast'],\n",
              "     'abstract': '',\n",
              "     'year': 2023,\n",
              "     'url': 'https://doi.org/10.32614/cran.package.transformer',\n",
              "     'doi': '10.32614/cran.package.transformer',\n",
              "     'publisher': 'The R Foundation',\n",
              "     'journal': 'CRAN: Contributed Packages',\n",
              "     'source': 'CrossRef',\n",
              "     'content': ''},\n",
              "    {'title': 'Convolutional channel modulator for transformer and LSTM networks in EEG-based emotion recognition.',\n",
              "     'authors': ['Hyunwook Kang', 'Jin Woo Choi', 'Byung Hyung Kim'],\n",
              "     'abstract': \"Electroencephalogram (EEG) signal is receiving much attention from recent studies since it is highly associated with intrinsic emotion. However, EEG signals contain underlying factors of variations across different sessions of the same subject, which make it difficult to learn temporal relationships between successive time steps. To disentangle invariant features, we propose a feature re-weighting mechanism on the extracted EEG features for temporal sequence modeling. Based on this method, our proposed model, called Convolutional Channel Modulator for Transformer and LSTM networks (CCMTL), extracts emotion-related inter-channel correlations using convolution operations and emphasizes important features by generating a channel attention map. This attention map is then used to perform matrix multiplication on the extracted features, which helps the subsequent Transformer to focus on important affective features. Furthermore, the sequential temporal modeling enhances the overall model's capability to understand temporal relationships both in global and sequential contexts. Experimental settings on public EEG emotion datasets demonstrate the superiority of the proposed CCMTL, surpassing six state-of-the-art models. Our code is publicly available at https://github.com/affctivai/CCMTL.\",\n",
              "     'year': 2025,\n",
              "     'url': 'https://pubmed.ncbi.nlm.nih.gov/40625552/',\n",
              "     'pmid': '40625552',\n",
              "     'source': 'PubMed',\n",
              "     'content': \"Electroencephalogram (EEG) signal is receiving much attention from recent studies since it is highly associated with intrinsic emotion. However, EEG signals contain underlying factors of variations across different sessions of the same subject, which make it difficult to learn temporal relationships between successive time steps. To disentangle invariant features, we propose a feature re-weighting mechanism on the extracted EEG features for temporal sequence modeling. Based on this method, our proposed model, called Convolutional Channel Modulator for Transformer and LSTM networks (CCMTL), extracts emotion-related inter-channel correlations using convolution operations and emphasizes important features by generating a channel attention map. This attention map is then used to perform matrix multiplication on the extracted features, which helps the subsequent Transformer to focus on important affective features. Furthermore, the sequential temporal modeling enhances the overall model's capability to understand temporal relationships both in global and sequential contexts. Experimental settings on public EEG emotion datasets demonstrate the superiority of the proposed CCMTL, surpassing six state-of-the-art models. Our code is publicly available at https://github.com/affctivai/CCMTL.\"},\n",
              "    {'title': 'Spatio-temporal transformer and graph convolutional networks based traffic flow prediction.',\n",
              "     'authors': ['Jin Zhang', 'Yimin Yang', 'Xiaoheng Wu', 'Sen Li'],\n",
              "     'abstract': 'Traffic flow prediction is a core component of intelligent transportation systems, providing accurate decision support for traffic management and urban planning. Traffic flow data exhibits highly complex spatiotemporal characteristics due to the intricate spatial correlations between nodes and the significant temporal dependencies across different time intervals. Despite substantial progress in this field, several challenges still remain. Firstly, most current methods rely on Graph Convolutional Networks (GCNs) to extract spatial correlations, typically using predefined adjacency matrices. However, these matrices are inadequate for dynamically capturing the complex and evolving spatial correlations within traffic networks. Secondly, traditional prediction methods predominantly focus on short-term forecasting, which is insufficient for long-term prediction needs. Additionally, many approaches fail to fully consider the local trend information in traffic flow data which reflects short-term temporal variations. To address these issues, a novel deep learning-based traffic flow prediction model, TDMGCN, is proposed. It integrates the Transformer and a multi-graph GCN to tackle the limitations of long-term prediction and the challenges of using the predefined adjacency matrices for spatial correlation extraction. Specifically, in the temporal dimension, a convolution-based multi-head self-attention module is designed. It can not only capture long-term temporal dependencies but also extract local trend information. In the spatial dimension, the model incorporates a spatial embedding module and a multi-graph convolutional module. The former is designed to learn traffic characteristics of different nodes, and the latter is used to extract spatial correlations effectively from multiple graphs. Additionally, the model integrates the periodic features of traffic flow data to further enhance prediction accuracy. Experimental results on five real-world traffic datasets demonstrate that TDMGCN outperforms the current most advanced baseline models.',\n",
              "     'year': 2025,\n",
              "     'url': 'https://pubmed.ncbi.nlm.nih.gov/40624240/',\n",
              "     'pmid': '40624240',\n",
              "     'source': 'PubMed',\n",
              "     'content': 'Traffic flow prediction is a core component of intelligent transportation systems, providing accurate decision support for traffic management and urban planning. Traffic flow data exhibits highly complex spatiotemporal characteristics due to the intricate spatial correlations between nodes and the significant temporal dependencies across different time intervals. Despite substantial progress in this field, several challenges still remain. Firstly, most current methods rely on Graph Convolutional Networks (GCNs) to extract spatial correlations, typically using predefined adjacency matrices. However, these matrices are inadequate for dynamically capturing the complex and evolving spatial correlations within traffic networks. Secondly, traditional prediction methods predominantly focus on short-term forecasting, which is insufficient for long-term prediction needs. Additionally, many approaches fail to fully consider the local trend information in traffic flow data which reflects short-term temporal variations. To address these issues, a novel deep learning-based traffic flow prediction model, TDMGCN, is proposed. It integrates the Transformer and a multi-graph GCN to tackle the limitations of long-term prediction and the challenges of using the predefined adjacency matrices for spatial correlation extraction. Specifically, in the temporal dimension, a convolution-based multi-head self-attention module is designed. It can not only capture long-term temporal dependencies but also extract local trend information. In the spatial dimension, the model incorporates a spatial embedding module and a multi-graph convolutional module. The former is designed to learn traffic characteristics of different nodes, and the latter is used to extract spatial correlations effectively from multiple graphs. Additionally, the model integrates the periodic features of traffic flow data to further enhance prediction accuracy. Experimental results on five real-world traffic datasets demonstrate that TDMGCN outperforms the current most advanced baseline models.'},\n",
              "    {'title': 'A recurrent multimodal sparse transformer framework for gastrointestinal disease classification.',\n",
              "     'authors': ['V Sharmila', 'S Geetha'],\n",
              "     'abstract': \"Accurate and early diagnosis of gastrointestinal (GI) tract diseases is essential for effective treatment planning and improved patient outcomes. However, existing diagnostic frameworks often face limitations due to modality imbalance, feature redundancy, and cross-modal inconsistencies, particularly when dealing with heterogeneous data such as medical text and endoscopic images. To bridge these gaps, this study proposes a novel recurrent multimodal principal gradient K-proximal sparse transformer (RMP-GKPS-transformer) framework for comprehensive GI disease classification. The approach integrates clinical text and WCE images using a robust multi-modal fusion strategy that incorporates Bio-RoBERTa for textual feature extraction, a graph vision spatial channel attention transformer network for image feature learning, and cross-attention mechanisms for modality alignment. Further, the model employs principal component analysis (PCA) for dimensionality reduction and gradient boosting machines (GBMs) for semantic conflict resolution. Classification is performed using an ensemble of random forest KNN, proximal policy optimization (PPO), and a sparse radial basis function (RBF) kernel to ensure accuracy and interpretability. Experimental evaluation on publicly available datasets achieved 99.82% accuracy, a Dice coefficient of 98.7%, and significantly lower execution time compared to state-of-the-art methods. The results confirm the framework's effectiveness in aligning and leveraging multi-modal data for precise classification of six GI diseases, offering a scalable and interpretable solution for enhanced clinical decision-making in gastroenterology.\",\n",
              "     'year': 2025,\n",
              "     'url': 'https://pubmed.ncbi.nlm.nih.gov/40624200/',\n",
              "     'pmid': '40624200',\n",
              "     'source': 'PubMed',\n",
              "     'content': \"Accurate and early diagnosis of gastrointestinal (GI) tract diseases is essential for effective treatment planning and improved patient outcomes. However, existing diagnostic frameworks often face limitations due to modality imbalance, feature redundancy, and cross-modal inconsistencies, particularly when dealing with heterogeneous data such as medical text and endoscopic images. To bridge these gaps, this study proposes a novel recurrent multimodal principal gradient K-proximal sparse transformer (RMP-GKPS-transformer) framework for comprehensive GI disease classification. The approach integrates clinical text and WCE images using a robust multi-modal fusion strategy that incorporates Bio-RoBERTa for textual feature extraction, a graph vision spatial channel attention transformer network for image feature learning, and cross-attention mechanisms for modality alignment. Further, the model employs principal component analysis (PCA) for dimensionality reduction and gradient boosting machines (GBMs) for semantic conflict resolution. Classification is performed using an ensemble of random forest KNN, proximal policy optimization (PPO), and a sparse radial basis function (RBF) kernel to ensure accuracy and interpretability. Experimental evaluation on publicly available datasets achieved 99.82% accuracy, a Dice coefficient of 98.7%, and significantly lower execution time compared to state-of-the-art methods. The results confirm the framework's effectiveness in aligning and leveraging multi-modal data for precise classification of six GI diseases, offering a scalable and interpretable solution for enhanced clinical decision-making in gastroenterology.\"},\n",
              "    {'title': 'Multi-Stage BiSTU Network Combining BiLSTM and Transformer for ABP Waveform Prediction from PPG Signals.',\n",
              "     'authors': ['Zheng Duanmu',\n",
              "      'Haojie Gong',\n",
              "      'Siyuan Lv',\n",
              "      'Wenyue Yan',\n",
              "      'Qianxi Cheng',\n",
              "      'Jinqiu Sang',\n",
              "      'Xilan Yang',\n",
              "      'Louqian Zhang'],\n",
              "     'abstract': 'Cardiovascular disease (CVD) remains a global health issue, and arterial blood pressure (ABP) waveforms provide critical physiological data that aid in the early diagnosis of CVD. However, existing pulse waveform evaluation methods are insufficient for accurately predicting ABP. This study aims to propose a novel U-net joint network architecture, the BiSTU Sequential Network, to predict high-quality ABP waveforms. The designed BiSTU Sequential Network integrates a Bidirectional Long Short-Term Memory (Bi-LSTM) model to capture temporal dependencies, a Transformer model with multi-head attention mechanisms to extract detailed features, and a MultiRes Convolutional Block Attention Module U-Net (MCBAMU-Net) for multi-scale feature extraction. The model was trained using 12,000 vital sign records from 942 ICU patients. Experimental results demonstrate that the predicted ABP waveforms closely align with the actual waveforms, achieving a mean absolute error (MAE) of 1.78 ¬± 2.15 mmHg, a root mean square error (RMSE) of 2.79 mmHg, and an R-squared (R   2  ) of 0.98. The model meets the standards of the Association for the Advancement of Medical Instrumentation (AAMI), with MAEs of 2.94 ¬± 3.43 mmHg for systolic blood pressure (SBP) and 4.22 ¬± 5.18 mmHg for diastolic blood pressure (DBP). Under the British Hypertension Society (BHS) standards, the accuracy rates within 5 mmHg are 85.3% for DBP and 72.4% for SBP and exceed 97% within 15 mmHg. The BiSTU Sequential Network exhibits significant potential for accurate, non-invasive prediction of arterial blood pressure. Its predictions closely match actual waveforms and comply with multiple clinical standards, indicating broad application prospects and contributing to the early diagnosis and monitoring of cardiovascular diseases.',\n",
              "     'year': 2025,\n",
              "     'url': 'https://pubmed.ncbi.nlm.nih.gov/40622504/',\n",
              "     'pmid': '40622504',\n",
              "     'source': 'PubMed',\n",
              "     'content': 'Cardiovascular disease (CVD) remains a global health issue, and arterial blood pressure (ABP) waveforms provide critical physiological data that aid in the early diagnosis of CVD. However, existing pulse waveform evaluation methods are insufficient for accurately predicting ABP. This study aims to propose a novel U-net joint network architecture, the BiSTU Sequential Network, to predict high-quality ABP waveforms. The designed BiSTU Sequential Network integrates a Bidirectional Long Short-Term Memory (Bi-LSTM) model to capture temporal dependencies, a Transformer model with multi-head attention mechanisms to extract detailed features, and a MultiRes Convolutional Block Attention Module U-Net (MCBAMU-Net) for multi-scale feature extraction. The model was trained using 12,000 vital sign records from 942 ICU patients. Experimental results demonstrate that the predicted ABP waveforms closely align with the actual waveforms, achieving a mean absolute error (MAE) of 1.78 ¬± 2.15 mmHg, a root mean square error (RMSE) of 2.79 mmHg, and an R-squared (R   2  ) of 0.98. The model meets the standards of the Association for the Advancement of Medical Instrumentation (AAMI), with MAEs of 2.94 ¬± 3.43 mmHg for systolic blood pressure (SBP) and 4.22 ¬± 5.18 mmHg for diastolic blood pressure (DBP). Under the British Hypertension Society (BHS) standards, the accuracy rates within 5 mmHg are 85.3% for DBP and 72.4% for SBP and exceed 97% within 15 mmHg. The BiSTU Sequential Network exhibits significant potential for accurate, non-invasive prediction of arterial blood pressure. Its predictions closely match actual waveforms and comply with multiple clinical standards, indicating broad application prospects and contributing to the early diagnosis and monitoring of cardiovascular diseases.'},\n",
              "    {'title': 'Leveraging heterogeneous tabular of EHRs with prompt learning for clinical prediction.',\n",
              "     'authors': ['Xuebing Yang',\n",
              "      'Longyu Li',\n",
              "      'Chutong Wang',\n",
              "      'Wensheng Zhang',\n",
              "      'Huizhou Liu',\n",
              "      'Wen Tang'],\n",
              "     'abstract': 'Electronic Health Records (EHRs) depict patient-related information and have significantly contributed to advancements in healthcare fields. The abundance of EHR data provides exceptional opportunities for developing clinical predictive models. However, the heterogeneity within multi-source EHR data raises a difficulty to organically leverage information from structured and unstructured features. In this paper, we focus on the heterogeneous EHR data in the tabular form, and propose a Prompt learning based data Fusion framework for Tabular (TabPF) to extract patient representations for clinical prediction. First, we design a text summary generator module to convert medical tabular into vector representations through long text embedding. Specifically, the tailored prompt learning is conducted for leading the Large Language Model (LLM) to respectively generate appropriate text summaries for different types of tabular data. Second, we design a novel attention mechanism of Transformer to effectively realize heterogeneous data fusion and generate more comprehensive patient representations for downstream predictions. The experiments are performed on the publicly available eICU-CRD dataset and the real-world CECMed dataset containing elderly patients diagnosed with chronic diseases, in comparison with representative baseline models. The results validate the superior performance of TabPF in predicting severity, mortality and Length of Stay (LoS). Furthermore, extensive ablation study and model variants evaluations demonstrate the effectiveness of the key component of the proposed framework.',\n",
              "     'year': 2025,\n",
              "     'url': 'https://pubmed.ncbi.nlm.nih.gov/40619074/',\n",
              "     'pmid': '40619074',\n",
              "     'source': 'PubMed',\n",
              "     'content': 'Electronic Health Records (EHRs) depict patient-related information and have significantly contributed to advancements in healthcare fields. The abundance of EHR data provides exceptional opportunities for developing clinical predictive models. However, the heterogeneity within multi-source EHR data raises a difficulty to organically leverage information from structured and unstructured features. In this paper, we focus on the heterogeneous EHR data in the tabular form, and propose a Prompt learning based data Fusion framework for Tabular (TabPF) to extract patient representations for clinical prediction. First, we design a text summary generator module to convert medical tabular into vector representations through long text embedding. Specifically, the tailored prompt learning is conducted for leading the Large Language Model (LLM) to respectively generate appropriate text summaries for different types of tabular data. Second, we design a novel attention mechanism of Transformer to effectively realize heterogeneous data fusion and generate more comprehensive patient representations for downstream predictions. The experiments are performed on the publicly available eICU-CRD dataset and the real-world CECMed dataset containing elderly patients diagnosed with chronic diseases, in comparison with representative baseline models. The results validate the superior performance of TabPF in predicting severity, mortality and Length of Stay (LoS). Furthermore, extensive ablation study and model variants evaluations demonstrate the effectiveness of the key component of the proposed framework.'}],\n",
              "   'count': 20,\n",
              "   'message': 'Found 20 papers'},\n",
              "  'question_answering': {'success': True,\n",
              "   'question': 'What is attention mechanism in transformers?',\n",
              "   'answer': 'The attention mechanism in transformers is a computational model that is claimed to implement attention mechanisms, but it actually performs a special class of relaxation labeling with similarity grouping effects. It does not exhibit similar effects as those known in human visual attention, which involves both feed-forward and feedback mechanisms. Instead, the attention mechanism in transformers groups figures in stimuli based on similarity in visual features such as color, and assigns more salience to distractors or the ground in saliency detection tasks. \\n\\nHowever, it\\'s also worth noting that there are issues associated with the conventional attention mechanism, such as rank-collapse and gradient vanishing, and researchers have proposed alternative attention mechanisms like the generalized probabilistic attention mechanism (GPAM) to address these issues. \\n\\nIn other words, the term \"attention mechanism\" in transformers can be misleading, as it doesn\\'t necessarily mimic human attention, but rather performs a specific type of computational operation.',\n",
              "   'sources': [{'title': 'Self-attention in Vision Transformers Performs Perceptual Grouping, Not Attention',\n",
              "     'authors': 'Paria Mehrani, John K. Tsotsos',\n",
              "     'source': 'arXiv',\n",
              "     'url': 'http://arxiv.org/abs/2303.01542v1',\n",
              "     'content_snippet': 'Recently, a considerable number of studies in computer vision involves deep\\nneural architectures called vision transformers. Visual processing in these\\nmodels incorporates computational models that ar...'},\n",
              "    {'title': 'Self-attention in Vision Transformers Performs Perceptual Grouping, Not Attention',\n",
              "     'authors': 'Paria Mehrani, John K. Tsotsos',\n",
              "     'source': 'arXiv',\n",
              "     'url': 'http://arxiv.org/abs/2303.01542v1',\n",
              "     'content_snippet': 'Recently, a considerable number of studies in computer vision involves deep\\nneural architectures called vision transformers. Visual processing in these\\nmodels incorporates computational models that ar...'},\n",
              "    {'title': 'Self-attention in Vision Transformers Performs Perceptual Grouping, Not Attention',\n",
              "     'authors': 'Paria Mehrani, John K. Tsotsos',\n",
              "     'source': 'arXiv',\n",
              "     'url': 'http://arxiv.org/abs/2303.01542v1',\n",
              "     'content_snippet': 'Recently, a considerable number of studies in computer vision involves deep\\nneural architectures called vision transformers. Visual processing in these\\nmodels incorporates computational models that ar...'},\n",
              "    {'title': 'Generalized Probabilistic Attention Mechanism in Transformers',\n",
              "     'authors': 'DongNyeong Heo, Heeyoul Choi',\n",
              "     'source': 'arXiv',\n",
              "     'url': 'http://arxiv.org/abs/2410.15578v1',\n",
              "     'content_snippet': 'The Transformer architecture has become widely adopted due to its\\ndemonstrated success, attributed to the attention mechanism at its core.\\nDespite these successes, the attention mechanism of Transform...'},\n",
              "    {'title': 'Generalized Probabilistic Attention Mechanism in Transformers',\n",
              "     'authors': 'DongNyeong Heo, Heeyoul Choi',\n",
              "     'source': 'arXiv',\n",
              "     'url': 'http://arxiv.org/abs/2410.15578v1',\n",
              "     'content_snippet': 'The Transformer architecture has become widely adopted due to its\\ndemonstrated success, attributed to the attention mechanism at its core.\\nDespite these successes, the attention mechanism of Transform...'}],\n",
              "   'timestamp': '2025-07-08T17:54:39.048581'},\n",
              "  'project_creation': {'success': True,\n",
              "   'project_id': 'proj_1751997279_3656',\n",
              "   'project': {'id': 'proj_1751997279_3656',\n",
              "    'name': 'Transformer Architecture Analysis',\n",
              "    'research_question': 'How do attention mechanisms improve transformer performance?',\n",
              "    'keywords': ['transformer', 'attention mechanism', 'neural networks'],\n",
              "    'scope': 'comprehensive',\n",
              "    'created_at': '2025-07-08T17:54:39.048782',\n",
              "    'status': 'active',\n",
              "    'papers': [{'title': 'The Xi-transform for conformally flat space-time',\n",
              "      'authors': ['George Sparling'],\n",
              "      'abstract': \"The Xi-transform is a new spinor transform arising naturally in Einstein's\\ngeneral relativity. Here the example of conformally flat space-time is\\ndiscussed in detail. In particular it is shown that for this case, the\\ntransform coincides with two other naturally defined transforms: one a\\ntwo-variable transform on the Lie group SU(2, C), the other a transform on the\\nspace of null split octaves. The key properties of the transform are developed.\",\n",
              "      'url': 'http://arxiv.org/abs/gr-qc/0612006v1',\n",
              "      'pdf_url': 'http://arxiv.org/pdf/gr-qc/0612006v1',\n",
              "      'published': '2006-12-01T03:22:29+00:00',\n",
              "      'updated': '2006-12-01T03:22:29+00:00',\n",
              "      'categories': ['gr-qc'],\n",
              "      'primary_category': 'gr-qc',\n",
              "      'source': 'arXiv',\n",
              "      'year': 2006,\n",
              "      'content': \"The Xi-transform is a new spinor transform arising naturally in Einstein's\\ngeneral relativity. Here the example of conformally flat space-time is\\ndiscussed in detail. In particular it is shown that for this case, the\\ntransform coincides with two other naturally defined transforms: one a\\ntwo-variable transform on the Lie group SU(2, C), the other a transform on the\\nspace of null split octaves. The key properties of the transform are developed.\"},\n",
              "     {'title': 'Multiple basic hypergeometric transformation formulas arising from the balanced duality transformation',\n",
              "      'authors': ['Yasushi Kajihara'],\n",
              "      'abstract': 'Some multiple hypergeometric transformation formulas arising from the\\nbalanced du- ality transformation formula are discussed through the symmetry.\\nDerivations of some transformation formulas with different dimensions are given\\nby taking certain limits of the balanced duality transformation. By combining\\nsome of them, some transformation formulas for $A_n$ basic hypergeometric\\nseries is given. They include some generalizations of Watson, Sears and ${}_8\\nW_7$ transformations.',\n",
              "      'url': 'http://arxiv.org/abs/1310.1984v2',\n",
              "      'pdf_url': 'http://arxiv.org/pdf/1310.1984v2',\n",
              "      'published': '2013-10-08T01:59:21+00:00',\n",
              "      'updated': '2014-03-10T01:06:01+00:00',\n",
              "      'categories': ['math.CA', 'math.QA', 'math.RT'],\n",
              "      'primary_category': 'math.CA',\n",
              "      'source': 'arXiv',\n",
              "      'year': 2013,\n",
              "      'content': 'Some multiple hypergeometric transformation formulas arising from the\\nbalanced du- ality transformation formula are discussed through the symmetry.\\nDerivations of some transformation formulas with different dimensions are given\\nby taking certain limits of the balanced duality transformation. By combining\\nsome of them, some transformation formulas for $A_n$ basic hypergeometric\\nseries is given. They include some generalizations of Watson, Sears and ${}_8\\nW_7$ transformations.'},\n",
              "     {'title': 'The Fourier and Hilbert transforms under the Bargmann transform',\n",
              "      'authors': ['Xing-Tang Dong', 'Kehe Zhu'],\n",
              "      'abstract': 'There is a canonical unitary transformation from $L^2(\\\\R)$ onto the Fock\\nspace $F^2$, called the Bargmann transform. We study the action of the Bargmann\\ntransform on several classical integral operators on $L^2(\\\\R)$, including the\\nfractional Fourier transform, the fractional Hilbert transform, and the wavelet\\ntransform.',\n",
              "      'url': 'http://arxiv.org/abs/1605.08683v1',\n",
              "      'pdf_url': 'http://arxiv.org/pdf/1605.08683v1',\n",
              "      'published': '2016-05-27T15:23:27+00:00',\n",
              "      'updated': '2016-05-27T15:23:27+00:00',\n",
              "      'categories': ['math.CV', 'math.FA'],\n",
              "      'primary_category': 'math.CV',\n",
              "      'source': 'arXiv',\n",
              "      'year': 2016,\n",
              "      'content': 'There is a canonical unitary transformation from $L^2(\\\\R)$ onto the Fock\\nspace $F^2$, called the Bargmann transform. We study the action of the Bargmann\\ntransform on several classical integral operators on $L^2(\\\\R)$, including the\\nfractional Fourier transform, the fractional Hilbert transform, and the wavelet\\ntransform.'},\n",
              "     {'title': 'Swin Transformer: Hierarchical Vision Transformer using Shifted Windows',\n",
              "      'authors': ['Ze Liu',\n",
              "       'Yutong Lin',\n",
              "       'Yue Cao',\n",
              "       'Han Hu',\n",
              "       'Yixuan Wei',\n",
              "       'Zheng Zhang',\n",
              "       'Stephen Lin',\n",
              "       'B. Guo'],\n",
              "      'abstract': 'This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with Shifted windows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at https://github.com/microsoft/Swin-Transformer.',\n",
              "      'year': 2021,\n",
              "      'url': 'https://www.semanticscholar.org/paper/c8b25fab5608c3e033d34b4483ec47e68ba109b7',\n",
              "      'source': 'Semantic Scholar',\n",
              "      'citation_count': 21778,\n",
              "      'venue': 'IEEE International Conference on Computer Vision',\n",
              "      'publication_date': '',\n",
              "      'content': 'This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with Shifted windows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at https://github.com/microsoft/Swin-Transformer.',\n",
              "      'external_ids': {'ArXiv': '2103.14030',\n",
              "       'DBLP': 'conf/iccv/LiuL00W0LG21',\n",
              "       'DOI': '10.1109/ICCV48922.2021.00986',\n",
              "       'CorpusId': 232352874}},\n",
              "     {'title': 'Longformer: The Long-Document Transformer',\n",
              "      'authors': ['Iz Beltagy', 'Matthew E. Peters', 'Arman Cohan'],\n",
              "      'abstract': \"Transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. Longformer's attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we evaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8. In contrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks. Our pretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on WikiHop and TriviaQA. We finally introduce the Longformer-Encoder-Decoder (LED), a Longformer variant for supporting long document generative sequence-to-sequence tasks, and demonstrate its effectiveness on the arXiv summarization dataset.\",\n",
              "      'year': 2020,\n",
              "      'url': 'https://www.semanticscholar.org/paper/925ad2897d1b5decbea320d07e99afa9110e09b2',\n",
              "      'source': 'Semantic Scholar',\n",
              "      'citation_count': 4109,\n",
              "      'venue': 'arXiv.org',\n",
              "      'publication_date': '2020-04-10',\n",
              "      'content': \"Transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. Longformer's attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we evaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8. In contrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks. Our pretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on WikiHop and TriviaQA. We finally introduce the Longformer-Encoder-Decoder (LED), a Longformer variant for supporting long document generative sequence-to-sequence tasks, and demonstrate its effectiveness on the arXiv summarization dataset.\",\n",
              "      'external_ids': {'DBLP': 'journals/corr/abs-2004-05150',\n",
              "       'MAG': '3015468748',\n",
              "       'ArXiv': '2004.05150',\n",
              "       'CorpusId': 215737171}},\n",
              "     {'title': 'Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer',\n",
              "      'authors': ['Colin Raffel',\n",
              "       'Noam M. Shazeer',\n",
              "       'Adam Roberts',\n",
              "       'Katherine Lee',\n",
              "       'Sharan Narang',\n",
              "       'Michael Matena',\n",
              "       'Yanqi Zhou',\n",
              "       'Wei Li',\n",
              "       'Peter J. Liu'],\n",
              "      'abstract': 'Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code.',\n",
              "      'year': 2019,\n",
              "      'url': 'https://www.semanticscholar.org/paper/6c4b76232bb72897685d19b3d264c6ee3005bc2b',\n",
              "      'source': 'Semantic Scholar',\n",
              "      'citation_count': 20431,\n",
              "      'venue': 'Journal of machine learning research',\n",
              "      'publication_date': '2019-10-23',\n",
              "      'content': 'Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code.',\n",
              "      'external_ids': {'MAG': '2981852735',\n",
              "       'DBLP': 'journals/corr/abs-1910-10683',\n",
              "       'ArXiv': '1910.10683',\n",
              "       'CorpusId': 204838007}},\n",
              "     {'title': 'Transformer ratings and transformer life',\n",
              "      'authors': ['E. Simonson'],\n",
              "      'abstract': '',\n",
              "      'year': 1998,\n",
              "      'url': 'https://doi.org/10.1049/ic:19981011',\n",
              "      'doi': '10.1049/ic:19981011',\n",
              "      'publisher': 'IEE',\n",
              "      'journal': 'IEE Colloquium Transformer Life Management',\n",
              "      'source': 'CrossRef',\n",
              "      'content': ''},\n",
              "     {'title': 'Figure 4: Hierarchical transformer module (H-Transformer).',\n",
              "      'authors': [],\n",
              "      'abstract': '',\n",
              "      'year': None,\n",
              "      'url': 'https://doi.org/10.7717/peerjcs.2417/fig-4',\n",
              "      'doi': '10.7717/peerjcs.2417/fig-4',\n",
              "      'publisher': 'PeerJ',\n",
              "      'journal': '',\n",
              "      'source': 'CrossRef',\n",
              "      'content': ''},\n",
              "     {'title': 'Transformer Testing',\n",
              "      'authors': [],\n",
              "      'abstract': '',\n",
              "      'year': 2007,\n",
              "      'url': 'https://doi.org/10.1201/9781420008715-20',\n",
              "      'doi': '10.1201/9781420008715-20',\n",
              "      'publisher': 'CRC Press',\n",
              "      'journal': 'Electric Power Transformer Engineering',\n",
              "      'source': 'CrossRef',\n",
              "      'content': ''},\n",
              "     {'title': 'Application of Transformer Neural Networks for Data Cleaning in Emergency Room Logs: A Case Study from the Bordeaux University Hospital.',\n",
              "      'authors': ['Dylan Russon',\n",
              "       'Cedric Gil-Jardine',\n",
              "       'Lara Marcel',\n",
              "       'Laure Chanel',\n",
              "       'Sylvain Faure',\n",
              "       'Bertrand Maury',\n",
              "       'Gabrielle Chenais',\n",
              "       'Emmanuel Lagarde'],\n",
              "      'abstract': \"Emergency Rooms (ERs) are at the center of various optimization research due to the growing number of visits in recent decades. The accurate logging of patient movement and time spent in ERs is essential for studying patient pathways and improving operations. Despite the innovative digital tracking system employed at the Bordeaux University Hospital, over 90% of these logs suffer from missing or incoherent values, primarily due to manual entry errors and software changes. This study explores the application of Transformer neural networks for cleaning ER logs at the Bordeaux University Hospital, addressing the challenge of inaccuracies in patient movement records accumulated over ten years. By leveraging a T5 Transformer model, we aim to demonstrate the model's ability to correct these errors by transforming unreliable datasets into valuable resources for optimizing ER operations. Our findings reveal that the Transformer model can achieve an overall accuracy of 95.79% in identifying and correcting faulty entries, showcasing the potential of advanced machine learning techniques in enhancing data integrity for healthcare applications. This research highlights the effectiveness of Transformer networks in data cleaning tasks and opens up opportunities for their application in various fields that are burdened with corrupt sequential data.\",\n",
              "      'year': 2025,\n",
              "      'url': 'https://pubmed.ncbi.nlm.nih.gov/40627474/',\n",
              "      'pmid': '40627474',\n",
              "      'source': 'PubMed',\n",
              "      'content': \"Emergency Rooms (ERs) are at the center of various optimization research due to the growing number of visits in recent decades. The accurate logging of patient movement and time spent in ERs is essential for studying patient pathways and improving operations. Despite the innovative digital tracking system employed at the Bordeaux University Hospital, over 90% of these logs suffer from missing or incoherent values, primarily due to manual entry errors and software changes. This study explores the application of Transformer neural networks for cleaning ER logs at the Bordeaux University Hospital, addressing the challenge of inaccuracies in patient movement records accumulated over ten years. By leveraging a T5 Transformer model, we aim to demonstrate the model's ability to correct these errors by transforming unreliable datasets into valuable resources for optimizing ER operations. Our findings reveal that the Transformer model can achieve an overall accuracy of 95.79% in identifying and correcting faulty entries, showcasing the potential of advanced machine learning techniques in enhancing data integrity for healthcare applications. This research highlights the effectiveness of Transformer networks in data cleaning tasks and opens up opportunities for their application in various fields that are burdened with corrupt sequential data.\"},\n",
              "     {'title': 'CNNViT-MILF-a: A Novel Architecture Leveraging the Synergy of CNN and ViT for Motor Imagery Classification.',\n",
              "      'authors': ['Zhenxi Zhao',\n",
              "       'Yingyu Cao',\n",
              "       'Hongbin Yu',\n",
              "       'Huixian Yu',\n",
              "       'Junfen Huang'],\n",
              "      'abstract': \"Accurate motor imagery (MI) classification in EEG-based brain-computer interfaces (BCIs) is essential for applications in engineering, medicine, and artificial intelligence. Due to the limitations of single-model approaches, hybrid model architectures have emerged as a promising direction. In particular, convolutional neural networks (CNNs) and vision transformers (ViTs) demonstrate strong complementary capabilities, leading to enhanced performance. This study proposes a series of novel models, termed as CNNViT-MI, to explore the synergy of CNNs and ViTs for MI classification. Specifically, five fusion strategies were defined: parallel integration, sequential integration, hierarchical integration, early fusion, and late fusion. Based on these strategies, eight candidate models were developed. Experiments were conducted on four datasets: BCI competition IV dataset 2a, BCI competition IV dataset 2b, high gamma dataset, and a self-collected MI-GS dataset. The results demonstrate that CNNViT-MILF-a achieves the best performance among all candidates by leveraging ViT as the backbone for global feature extraction and incorporating CNN-based local representations through a late fusion strategy. Compared to the best-performing state-ofthe-art (SOTA) methods, mean accuracy was improved by 2.27%, 2.31%, 0.74%, and 2.50% on the respective datasets, confirming the model's effectiveness and broad applicability, other metrics showed similar improvements. In addition, significance analysis, ablation studies, and visualization analysis were conducted, and corresponding clinical integration and rehabilitation protocols were developed to support practical use in healthcare.\",\n",
              "      'year': 2025,\n",
              "      'url': 'https://pubmed.ncbi.nlm.nih.gov/40627473/',\n",
              "      'pmid': '40627473',\n",
              "      'source': 'PubMed',\n",
              "      'content': \"Accurate motor imagery (MI) classification in EEG-based brain-computer interfaces (BCIs) is essential for applications in engineering, medicine, and artificial intelligence. Due to the limitations of single-model approaches, hybrid model architectures have emerged as a promising direction. In particular, convolutional neural networks (CNNs) and vision transformers (ViTs) demonstrate strong complementary capabilities, leading to enhanced performance. This study proposes a series of novel models, termed as CNNViT-MI, to explore the synergy of CNNs and ViTs for MI classification. Specifically, five fusion strategies were defined: parallel integration, sequential integration, hierarchical integration, early fusion, and late fusion. Based on these strategies, eight candidate models were developed. Experiments were conducted on four datasets: BCI competition IV dataset 2a, BCI competition IV dataset 2b, high gamma dataset, and a self-collected MI-GS dataset. The results demonstrate that CNNViT-MILF-a achieves the best performance among all candidates by leveraging ViT as the backbone for global feature extraction and incorporating CNN-based local representations through a late fusion strategy. Compared to the best-performing state-ofthe-art (SOTA) methods, mean accuracy was improved by 2.27%, 2.31%, 0.74%, and 2.50% on the respective datasets, confirming the model's effectiveness and broad applicability, other metrics showed similar improvements. In addition, significance analysis, ablation studies, and visualization analysis were conducted, and corresponding clinical integration and rehabilitation protocols were developed to support practical use in healthcare.\"},\n",
              "     {'title': 'Depression EEG classification based on multi-scale convolutional transformer network.',\n",
              "      'authors': ['Wan Chen',\n",
              "       'Yanping Cai',\n",
              "       'Aihua Li',\n",
              "       'Ke Jiang',\n",
              "       'Qisheng Yang',\n",
              "       'Xiao Zhong',\n",
              "       'Wei Zhang'],\n",
              "      'abstract': \"Depression electroencephalograph (EEG) classification based on machine learning is helpful for the auxiliary diagnosis of major depression disorder (MDD). Multi-channel EEG has abundant spatial information because EEG electrodes are distributed in different brain regions. However, existing methods arrange EEG features as feature vectors, which destroys the spatial structure of the features and may affect the model's performance. To improve the accuracy of MDD classification, we propose a novel EEG classification method for depression based on the brain topographic map and multi-scale convolutional transformer network (MCTNet). First, the power spectral density (PSD) features are extracted from EEG, and the one-dimensional feature vectors are converted into high-dimensional brain topographic maps according to the location information of EEG channels. Then, a multi-scale convolution with three parallel branches is designed to convert the brain topographic map into a deep feature map representation. Finally, image segmentation (IS) and the transformer encoder (TE) are used to learn the local and global features of the feature map, and the feature is input into the fully connected layer for classification. In addition, a joint loss function based on cross-entropy and center loss (CL) is designed to enable MCTNet to extract features with larger inter-class and smaller intra-class distances. Complete experimental verification is carried out on an open dataset. The accuracy, sensitivity and specificity of MCTNet are 97.24%, 97.20%, and 97.46%, respectively. The results show that the proposed method can achieve high-precision depression EEG classification and is superior to the state-of-the-art models.\",\n",
              "      'year': 2025,\n",
              "      'url': 'https://pubmed.ncbi.nlm.nih.gov/40627463/',\n",
              "      'pmid': '40627463',\n",
              "      'source': 'PubMed',\n",
              "      'content': \"Depression electroencephalograph (EEG) classification based on machine learning is helpful for the auxiliary diagnosis of major depression disorder (MDD). Multi-channel EEG has abundant spatial information because EEG electrodes are distributed in different brain regions. However, existing methods arrange EEG features as feature vectors, which destroys the spatial structure of the features and may affect the model's performance. To improve the accuracy of MDD classification, we propose a novel EEG classification method for depression based on the brain topographic map and multi-scale convolutional transformer network (MCTNet). First, the power spectral density (PSD) features are extracted from EEG, and the one-dimensional feature vectors are converted into high-dimensional brain topographic maps according to the location information of EEG channels. Then, a multi-scale convolution with three parallel branches is designed to convert the brain topographic map into a deep feature map representation. Finally, image segmentation (IS) and the transformer encoder (TE) are used to learn the local and global features of the feature map, and the feature is input into the fully connected layer for classification. In addition, a joint loss function based on cross-entropy and center loss (CL) is designed to enable MCTNet to extract features with larger inter-class and smaller intra-class distances. Complete experimental verification is carried out on an open dataset. The accuracy, sensitivity and specificity of MCTNet are 97.24%, 97.20%, and 97.46%, respectively. The results show that the proposed method can achieve high-precision depression EEG classification and is superior to the state-of-the-art models.\"},\n",
              "     {'title': 'A General Survey on Attention Mechanisms in Deep Learning',\n",
              "      'authors': ['Gianni Brauwers', 'Flavius Frasincar'],\n",
              "      'abstract': 'Attention is an important mechanism that can be employed for a variety of\\ndeep learning models across many different domains and tasks. This survey\\nprovides an overview of the most important attention mechanisms proposed in the\\nliterature. The various attention mechanisms are explained by means of a\\nframework consisting of a general attention model, uniform notation, and a\\ncomprehensive taxonomy of attention mechanisms. Furthermore, the various\\nmeasures for evaluating attention models are reviewed, and methods to\\ncharacterize the structure of attention models based on the proposed framework\\nare discussed. Last, future work in the field of attention models is\\nconsidered.',\n",
              "      'url': 'http://arxiv.org/abs/2203.14263v1',\n",
              "      'pdf_url': 'http://arxiv.org/pdf/2203.14263v1',\n",
              "      'published': '2022-03-27T10:06:23+00:00',\n",
              "      'updated': '2022-03-27T10:06:23+00:00',\n",
              "      'categories': ['cs.LG'],\n",
              "      'primary_category': 'cs.LG',\n",
              "      'source': 'arXiv',\n",
              "      'year': 2022,\n",
              "      'content': 'Attention is an important mechanism that can be employed for a variety of\\ndeep learning models across many different domains and tasks. This survey\\nprovides an overview of the most important attention mechanisms proposed in the\\nliterature. The various attention mechanisms are explained by means of a\\nframework consisting of a general attention model, uniform notation, and a\\ncomprehensive taxonomy of attention mechanisms. Furthermore, the various\\nmeasures for evaluating attention models are reviewed, and methods to\\ncharacterize the structure of attention models based on the proposed framework\\nare discussed. Last, future work in the field of attention models is\\nconsidered.'},\n",
              "     {'title': 'Generalized Probabilistic Attention Mechanism in Transformers',\n",
              "      'authors': ['DongNyeong Heo', 'Heeyoul Choi'],\n",
              "      'abstract': 'The Transformer architecture has become widely adopted due to its\\ndemonstrated success, attributed to the attention mechanism at its core.\\nDespite these successes, the attention mechanism of Transformers is associated\\nwith two well-known issues: rank-collapse and gradient vanishing. In this\\npaper, we present a theoretical analysis that it is inherently difficult to\\naddress both issues simultaneously in the conventional attention mechanism. To\\nhandle these issues, we introduce a novel class of attention mechanism,\\nreferred to as generalized probabilistic attention mechanism (GPAM), and its\\ndual-attention implementation within the Transformer architecture. Unlike\\nconventional attention mechanisms, GPAM allows for negative attention scores\\nwhile preserving a fixed total sum. We provide theoretical evidence that the\\nproposed dual-attention GPAM (daGPAM) effectively mitigates both the\\nrank-collapse and gradient vanishing issues which are difficult to resolve\\nsimultaneously with the conventional attention mechanisms. Furthermore, we\\nempirically validate this theoretical evidence, demonstrating the superiority\\nof daGPAM compared to other alternative attention mechanisms that were proposed\\nto address the same issues. Additionally, we demonstrate the practical benefits\\nof GPAM in natural language processing tasks, such as language modeling and\\nneural machine translation.',\n",
              "      'url': 'http://arxiv.org/abs/2410.15578v1',\n",
              "      'pdf_url': 'http://arxiv.org/pdf/2410.15578v1',\n",
              "      'published': '2024-10-21T01:55:52+00:00',\n",
              "      'updated': '2024-10-21T01:55:52+00:00',\n",
              "      'categories': ['cs.LG', 'cs.CL'],\n",
              "      'primary_category': 'cs.LG',\n",
              "      'source': 'arXiv',\n",
              "      'year': 2024,\n",
              "      'content': 'The Transformer architecture has become widely adopted due to its\\ndemonstrated success, attributed to the attention mechanism at its core.\\nDespite these successes, the attention mechanism of Transformers is associated\\nwith two well-known issues: rank-collapse and gradient vanishing. In this\\npaper, we present a theoretical analysis that it is inherently difficult to\\naddress both issues simultaneously in the conventional attention mechanism. To\\nhandle these issues, we introduce a novel class of attention mechanism,\\nreferred to as generalized probabilistic attention mechanism (GPAM), and its\\ndual-attention implementation within the Transformer architecture. Unlike\\nconventional attention mechanisms, GPAM allows for negative attention scores\\nwhile preserving a fixed total sum. We provide theoretical evidence that the\\nproposed dual-attention GPAM (daGPAM) effectively mitigates both the\\nrank-collapse and gradient vanishing issues which are difficult to resolve\\nsimultaneously with the conventional attention mechanisms. Furthermore, we\\nempirically validate this theoretical evidence, demonstrating the superiority\\nof daGPAM compared to other alternative attention mechanisms that were proposed\\nto address the same issues. Additionally, we demonstrate the practical benefits\\nof GPAM in natural language processing tasks, such as language modeling and\\nneural machine translation.'},\n",
              "     {'title': 'Tri-Attention: Explicit Context-Aware Attention Mechanism for Natural Language Processing',\n",
              "      'authors': ['Rui Yu', 'Yifeng Li', 'Wenpeng Lu', 'Longbing Cao'],\n",
              "      'abstract': 'In natural language processing (NLP), the context of a word or sentence plays\\nan essential role. Contextual information such as the semantic representation\\nof a passage or historical dialogue forms an essential part of a conversation\\nand a precise understanding of the present phrase or sentence. However, the\\nstandard attention mechanisms typically generate weights using query and key\\nbut ignore context, forming a Bi-Attention framework, despite their great\\nsuccess in modeling sequence alignment. This Bi-Attention mechanism does not\\nexplicitly model the interactions between the contexts, queries and keys of\\ntarget sequences, missing important contextual information and resulting in\\npoor attention performance. Accordingly, a novel and general triple-attention\\n(Tri-Attention) framework expands the standard Bi-Attention mechanism and\\nexplicitly interacts query, key, and context by incorporating context as the\\nthird dimension in calculating relevance scores. Four variants of Tri-Attention\\nare generated by expanding the two-dimensional vector-based additive,\\ndot-product, scaled dot-product, and bilinear operations in Bi-Attention to the\\ntensor operations for Tri-Attention. Extensive experiments on three NLP tasks\\ndemonstrate that Tri-Attention outperforms about 30 state-of-the-art\\nnon-attention, standard Bi-Attention, contextual Bi-Attention approaches and\\npretrained neural language models1.',\n",
              "      'url': 'http://arxiv.org/abs/2211.02899v1',\n",
              "      'pdf_url': 'http://arxiv.org/pdf/2211.02899v1',\n",
              "      'published': '2022-11-05T13:07:40+00:00',\n",
              "      'updated': '2022-11-05T13:07:40+00:00',\n",
              "      'categories': ['cs.CL', 'cs.AI'],\n",
              "      'primary_category': 'cs.CL',\n",
              "      'source': 'arXiv',\n",
              "      'year': 2022,\n",
              "      'content': 'In natural language processing (NLP), the context of a word or sentence plays\\nan essential role. Contextual information such as the semantic representation\\nof a passage or historical dialogue forms an essential part of a conversation\\nand a precise understanding of the present phrase or sentence. However, the\\nstandard attention mechanisms typically generate weights using query and key\\nbut ignore context, forming a Bi-Attention framework, despite their great\\nsuccess in modeling sequence alignment. This Bi-Attention mechanism does not\\nexplicitly model the interactions between the contexts, queries and keys of\\ntarget sequences, missing important contextual information and resulting in\\npoor attention performance. Accordingly, a novel and general triple-attention\\n(Tri-Attention) framework expands the standard Bi-Attention mechanism and\\nexplicitly interacts query, key, and context by incorporating context as the\\nthird dimension in calculating relevance scores. Four variants of Tri-Attention\\nare generated by expanding the two-dimensional vector-based additive,\\ndot-product, scaled dot-product, and bilinear operations in Bi-Attention to the\\ntensor operations for Tri-Attention. Extensive experiments on three NLP tasks\\ndemonstrate that Tri-Attention outperforms about 30 state-of-the-art\\nnon-attention, standard Bi-Attention, contextual Bi-Attention approaches and\\npretrained neural language models1.'},\n",
              "     {'title': 'A review on the attention mechanism of deep learning',\n",
              "      'authors': ['Zhaoyang Niu', 'G. Zhong', 'Hui Yu'],\n",
              "      'abstract': '',\n",
              "      'year': 2021,\n",
              "      'url': 'https://www.semanticscholar.org/paper/a56bf7ee9a56d8f84079684339a953c2df9ce76b',\n",
              "      'source': 'Semantic Scholar',\n",
              "      'citation_count': 2211,\n",
              "      'venue': 'Neurocomputing',\n",
              "      'publication_date': '2021-04-01',\n",
              "      'content': '',\n",
              "      'external_ids': {'DBLP': 'journals/ijon/NiuZY21',\n",
              "       'MAG': '3146366485',\n",
              "       'DOI': '10.1016/J.NEUCOM.2021.03.091',\n",
              "       'CorpusId': 233562906}},\n",
              "     {'title': 'DNNAM: Image inpainting algorithm via deep neural networks and attention mechanism',\n",
              "      'authors': ['Yuantao Chen', 'Runlong Xia', 'Kai Yang', 'Ke Zou'],\n",
              "      'abstract': '',\n",
              "      'year': 2024,\n",
              "      'url': 'https://www.semanticscholar.org/paper/56a5f9fecaaa268e6e452e4b5c3ff8d7ddb8fd68',\n",
              "      'source': 'Semantic Scholar',\n",
              "      'citation_count': 106,\n",
              "      'venue': 'Applied Soft Computing',\n",
              "      'publication_date': '2024-02-01',\n",
              "      'content': '',\n",
              "      'external_ids': {'DBLP': 'journals/asc/ChenXYZ24',\n",
              "       'DOI': '10.1016/j.asoc.2024.111392',\n",
              "       'CorpusId': 267676204}},\n",
              "     {'title': 'News Recommendation with Attention Mechanism',\n",
              "      'authors': ['Tianrui Liu',\n",
              "       'Changxin Xu',\n",
              "       'Yuxin Qiao',\n",
              "       'Chufeng Jiang',\n",
              "       'Weisheng Chen'],\n",
              "      'abstract': 'This paper explores the area of news recommendation, a key component of online information sharing. Initially, we provide a clear introduction to news recommendation, defining the core problem and summarizing current methods and notable recent algorithms. We then present our work on implementing the NRAM (News Recommendation with Attention Mechanism), an attention-based approach for news recommendation, and assess its effectiveness. Our evaluation shows that NRAM has the potential to significantly improve how news content is personalized for users on digital news platforms.',\n",
              "      'year': 2024,\n",
              "      'url': 'https://www.semanticscholar.org/paper/4cdb2fe14ac4d9d339564755f998d316e454f4db',\n",
              "      'source': 'Semantic Scholar',\n",
              "      'citation_count': 41,\n",
              "      'venue': 'arXiv.org',\n",
              "      'publication_date': '2024-02-12',\n",
              "      'content': 'This paper explores the area of news recommendation, a key component of online information sharing. Initially, we provide a clear introduction to news recommendation, defining the core problem and summarizing current methods and notable recent algorithms. We then present our work on implementing the NRAM (News Recommendation with Attention Mechanism), an attention-based approach for news recommendation, and assess its effectiveness. Our evaluation shows that NRAM has the potential to significantly improve how news content is personalized for users on digital news platforms.',\n",
              "      'external_ids': {'ArXiv': '2402.07422',\n",
              "       'DBLP': 'journals/corr/abs-2402-07422',\n",
              "       'DOI': '10.5281/zenodo.10635481',\n",
              "       'CorpusId': 267627716}},\n",
              "     {'title': 'Figure 3: The GAM attention mechanism includes two submodules: channel attention and spatial attention.',\n",
              "      'authors': [],\n",
              "      'abstract': '',\n",
              "      'year': None,\n",
              "      'url': 'https://doi.org/10.7717/peerjcs.2224/fig-3',\n",
              "      'doi': '10.7717/peerjcs.2224/fig-3',\n",
              "      'publisher': 'PeerJ',\n",
              "      'journal': '',\n",
              "      'source': 'CrossRef',\n",
              "      'content': ''},\n",
              "     {'title': 'Figure 2: Attention mechanism.',\n",
              "      'authors': [],\n",
              "      'abstract': '',\n",
              "      'year': None,\n",
              "      'url': 'https://doi.org/10.7717/peerjcs.948/fig-2',\n",
              "      'doi': '10.7717/peerjcs.948/fig-2',\n",
              "      'publisher': 'PeerJ',\n",
              "      'journal': '',\n",
              "      'source': 'CrossRef',\n",
              "      'content': ''},\n",
              "     {'title': 'Figure 3: Attention mechanism diagram.',\n",
              "      'authors': [],\n",
              "      'abstract': '',\n",
              "      'year': None,\n",
              "      'url': 'https://doi.org/10.7717/peerjcs.2332/fig-3',\n",
              "      'doi': '10.7717/peerjcs.2332/fig-3',\n",
              "      'publisher': 'PeerJ',\n",
              "      'journal': '',\n",
              "      'source': 'CrossRef',\n",
              "      'content': ''},\n",
              "     {'title': 'Assessment and management of portal hypertension in patients with MASLD: advances and caveats.',\n",
              "      'authors': ['Diego Rojo',\n",
              "       'Alba Jim√©nez-Masip',\n",
              "       'Laura Pag√®s',\n",
              "       'Juan Ba√±ares',\n",
              "       'Clara Sabiote',\n",
              "       'Mar√≠a Mart√≠nez-G√≥mez',\n",
              "       'Laia Aceituno',\n",
              "       'M Serra Cusid√≥',\n",
              "       'M Teresa Salcedo-Allende',\n",
              "       'Zyanya Calixto',\n",
              "       'M√≤nica Pons',\n",
              "       'Joan Genesc√†',\n",
              "       'Juan M Peric√†s'],\n",
              "      'abstract': 'Metabolic dysfunction-associated steatotic liver disease (MASLD) is increasingly gaining relevance both as a public health issue and a clinical challenge. The development of portal hypertension in MASLD challenges traditional paradigm that it arises only in the context of cirrhosis. This review explores the understanding of portal hypertension in MASLD, addressing recent advances in pathophysiology, diagnosis, and potential therapeutic approaches. Particular attention is given to studies that highlight non-cirrhotic contributors to increased portal pressure. Pathogenic mechanisms, as well as advances in noninvasive diagnostic tools are discussed, focusing on their utility in identifying early hemodynamic changes and stratifying the risk of clinical complications. A comprehensive literature search was conducted using Pubmed and major databases, including studies published up to April 2025. The future of MASLD management lies in early detection, improved noninvasive risk stratification, and personalized treatment strategies. Advances in technology and artificial intelligence will likely enhance diagnostic precision while ongoing research into molecular mechanisms and portal hypertension may enable earlier and effective therapeutic interventions.',\n",
              "      'year': 2025,\n",
              "      'url': 'https://pubmed.ncbi.nlm.nih.gov/40627432/',\n",
              "      'pmid': '40627432',\n",
              "      'source': 'PubMed',\n",
              "      'content': 'Metabolic dysfunction-associated steatotic liver disease (MASLD) is increasingly gaining relevance both as a public health issue and a clinical challenge. The development of portal hypertension in MASLD challenges traditional paradigm that it arises only in the context of cirrhosis. This review explores the understanding of portal hypertension in MASLD, addressing recent advances in pathophysiology, diagnosis, and potential therapeutic approaches. Particular attention is given to studies that highlight non-cirrhotic contributors to increased portal pressure. Pathogenic mechanisms, as well as advances in noninvasive diagnostic tools are discussed, focusing on their utility in identifying early hemodynamic changes and stratifying the risk of clinical complications. A comprehensive literature search was conducted using Pubmed and major databases, including studies published up to April 2025. The future of MASLD management lies in early detection, improved noninvasive risk stratification, and personalized treatment strategies. Advances in technology and artificial intelligence will likely enhance diagnostic precision while ongoing research into molecular mechanisms and portal hypertension may enable earlier and effective therapeutic interventions.'},\n",
              "     {'title': 'Excipients in pharmaceuticals: mechanisms of hypersensitivity and the role of global pharmacovigilance.',\n",
              "      'authors': ['Ruba Malkawi', 'Lora Altahrawi'],\n",
              "      'abstract': 'Excipients are important inactive components in drug formulations that ensure stability, bioavailability, and patient compliance. However, emerging evidence suggests that certain excipients, once considered inert, can cause hypersensitivity reactions in certain individuals. Such reactions include mild erythema due to systemic anaphylaxis and create clinical challenges that are difficult to handle. This review presents a systematic review of the existing literature on excipient hypersensitivity, with specific attention paid to commonly implicated excipients such as polyethylene glycol (PEG), parabens, and tartrazine. Hypersensitivity mechanisms (immune-mediated [IgE, T-cell] and non-immune) are discussed, along with their clinical features and diagnostic challenges. In addition, geographic variations in reporting are discussed, which in turn focus on the role of pharmacovigilance in the reduction of risk. Geographic variations in excipient hypersensitivity reporting are also discussed, highlighting disparities in pharmacovigilance efforts across different regions. This review also discusses recent work, regulatory issues, and desensitization protocols for the control of hypersensitivity reactions. Persistent surveillance and individual strategies are needed to enhance patient safety in the context of excipient-induced hypersensitivity.',\n",
              "      'year': 2025,\n",
              "      'url': 'https://pubmed.ncbi.nlm.nih.gov/40627271/',\n",
              "      'pmid': '40627271',\n",
              "      'source': 'PubMed',\n",
              "      'content': 'Excipients are important inactive components in drug formulations that ensure stability, bioavailability, and patient compliance. However, emerging evidence suggests that certain excipients, once considered inert, can cause hypersensitivity reactions in certain individuals. Such reactions include mild erythema due to systemic anaphylaxis and create clinical challenges that are difficult to handle. This review presents a systematic review of the existing literature on excipient hypersensitivity, with specific attention paid to commonly implicated excipients such as polyethylene glycol (PEG), parabens, and tartrazine. Hypersensitivity mechanisms (immune-mediated [IgE, T-cell] and non-immune) are discussed, along with their clinical features and diagnostic challenges. In addition, geographic variations in reporting are discussed, which in turn focus on the role of pharmacovigilance in the reduction of risk. Geographic variations in excipient hypersensitivity reporting are also discussed, highlighting disparities in pharmacovigilance efforts across different regions. This review also discusses recent work, regulatory issues, and desensitization protocols for the control of hypersensitivity reactions. Persistent surveillance and individual strategies are needed to enhance patient safety in the context of excipient-induced hypersensitivity.'},\n",
              "     {'title': 'Ononin induces ferroptosis in colorectal cancer cells via the PI3K/AKT/Nrf2 pathway to enhance anti-cancer immunotherapy.',\n",
              "      'authors': ['Yang Gui',\n",
              "       'Shuangjiao Deng',\n",
              "       'Jingjing Li',\n",
              "       'Dongmei Zuo',\n",
              "       'Heng Fan'],\n",
              "      'abstract': 'Colorectal cancer (CRC) represents one of the most prevalent forms of malignant neoplasms affecting the digestive tract. Recent studies have demonstrated that the induction of ferroptosis in tumor cells represents a novel therapeutic strategy. Ononin, an isoflavone glycoside compound derived from traditional Chinese medicinal plants, has garnered attention for its purported therapeutic efficacy. This study integrated network pharmacology and experimental studies to elucidate the underlying mechanism involved in the therapeutic action of ononin against CRC. The results demonstrated that ononin induced lipid peroxidation and a reduction in mitochondrial membrane potential (MMP), indicative of mitochondrial damage in CRC cells, accompanied by a pronounced decline in GPX4 expression. The combination of ononin and anti-PD-L1 therapy demonstrated a notable enhancement in tumor growth inhibition in the MC38 mouse CRC model, accompanied by a marked increase in the proportion of intratumoral IFNŒ≥+CD8+T cells. In summary, ononin triggers ferroptosis in colorectal cancer cells by increasing lipid ROS through the PI3K/AKT/Nrf2 pathway and reducing GPX4 expression. The combination of ononin and anti-PD-L1 therapy shows a synergistic anti-tumor effect, effectively inhibiting tumor growth and enhancing immune response. This study demonstrates that ononin is a promising therapeutic agent for CRC and may potentially serve as an immuno-adjuvant to enhance immunotherapy efficacy.',\n",
              "      'year': 2025,\n",
              "      'url': 'https://pubmed.ncbi.nlm.nih.gov/40627200/',\n",
              "      'pmid': '40627200',\n",
              "      'source': 'PubMed',\n",
              "      'content': 'Colorectal cancer (CRC) represents one of the most prevalent forms of malignant neoplasms affecting the digestive tract. Recent studies have demonstrated that the induction of ferroptosis in tumor cells represents a novel therapeutic strategy. Ononin, an isoflavone glycoside compound derived from traditional Chinese medicinal plants, has garnered attention for its purported therapeutic efficacy. This study integrated network pharmacology and experimental studies to elucidate the underlying mechanism involved in the therapeutic action of ononin against CRC. The results demonstrated that ononin induced lipid peroxidation and a reduction in mitochondrial membrane potential (MMP), indicative of mitochondrial damage in CRC cells, accompanied by a pronounced decline in GPX4 expression. The combination of ononin and anti-PD-L1 therapy demonstrated a notable enhancement in tumor growth inhibition in the MC38 mouse CRC model, accompanied by a marked increase in the proportion of intratumoral IFNŒ≥+CD8+T cells. In summary, ononin triggers ferroptosis in colorectal cancer cells by increasing lipid ROS through the PI3K/AKT/Nrf2 pathway and reducing GPX4 expression. The combination of ononin and anti-PD-L1 therapy shows a synergistic anti-tumor effect, effectively inhibiting tumor growth and enhancing immune response. This study demonstrates that ononin is a promising therapeutic agent for CRC and may potentially serve as an immuno-adjuvant to enhance immunotherapy efficacy.'},\n",
              "     {'title': 'Lecture Notes: Neural Network Architectures',\n",
              "      'authors': ['Evelyn Herberg'],\n",
              "      'abstract': 'These lecture notes provide an overview of Neural Network architectures from\\na mathematical point of view. Especially, Machine Learning with Neural Networks\\nis seen as an optimization problem. Covered are an introduction to Neural\\nNetworks and the following architectures: Feedforward Neural Network,\\nConvolutional Neural Network, ResNet, and Recurrent Neural Network.',\n",
              "      'url': 'http://arxiv.org/abs/2304.05133v2',\n",
              "      'pdf_url': 'http://arxiv.org/pdf/2304.05133v2',\n",
              "      'published': '2023-04-11T10:54:36+00:00',\n",
              "      'updated': '2023-04-18T15:57:29+00:00',\n",
              "      'categories': ['cs.LG', 'math.OC', '68T07'],\n",
              "      'primary_category': 'cs.LG',\n",
              "      'source': 'arXiv',\n",
              "      'year': 2023,\n",
              "      'content': 'These lecture notes provide an overview of Neural Network architectures from\\na mathematical point of view. Especially, Machine Learning with Neural Networks\\nis seen as an optimization problem. Covered are an introduction to Neural\\nNetworks and the following architectures: Feedforward Neural Network,\\nConvolutional Neural Network, ResNet, and Recurrent Neural Network.'},\n",
              "     {'title': 'Self-Organizing Multilayered Neural Networks of Optimal Complexity',\n",
              "      'authors': ['V. Schetinin'],\n",
              "      'abstract': 'The principles of self-organizing the neural networks of optimal complexity\\nis considered under the unrepresentative learning set. The method of\\nself-organizing the multi-layered neural networks is offered and used to train\\nthe logical neural networks which were applied to the medical diagnostics.',\n",
              "      'url': 'http://arxiv.org/abs/cs/0504056v1',\n",
              "      'pdf_url': 'http://arxiv.org/pdf/cs/0504056v1',\n",
              "      'published': '2005-04-13T13:59:55+00:00',\n",
              "      'updated': '2005-04-13T13:59:55+00:00',\n",
              "      'categories': ['cs.NE', 'cs.AI'],\n",
              "      'primary_category': 'cs.NE',\n",
              "      'source': 'arXiv',\n",
              "      'year': 2005,\n",
              "      'content': 'The principles of self-organizing the neural networks of optimal complexity\\nis considered under the unrepresentative learning set. The method of\\nself-organizing the multi-layered neural networks is offered and used to train\\nthe logical neural networks which were applied to the medical diagnostics.'},\n",
              "     {'title': 'Neural Network Processing Neural Networks: An efficient way to learn higher order functions',\n",
              "      'authors': ['Firat Tuna'],\n",
              "      'abstract': 'Functions are rich in meaning and can be interpreted in a variety of ways.\\nNeural networks were proven to be capable of approximating a large class of\\nfunctions[1]. In this paper, we propose a new class of neural networks called\\n\"Neural Network Processing Neural Networks\" (NNPNNs), which inputs neural\\nnetworks and numerical values, instead of just numerical values. Thus enabling\\nneural networks to represent and process rich structures.',\n",
              "      'url': 'http://arxiv.org/abs/1911.05640v2',\n",
              "      'pdf_url': 'http://arxiv.org/pdf/1911.05640v2',\n",
              "      'published': '2019-11-06T19:15:34+00:00',\n",
              "      'updated': '2020-01-14T23:11:27+00:00',\n",
              "      'categories': ['cs.LG', 'cs.NE'],\n",
              "      'primary_category': 'cs.LG',\n",
              "      'source': 'arXiv',\n",
              "      'year': 2019,\n",
              "      'content': 'Functions are rich in meaning and can be interpreted in a variety of ways.\\nNeural networks were proven to be capable of approximating a large class of\\nfunctions[1]. In this paper, we propose a new class of neural networks called\\n\"Neural Network Processing Neural Networks\" (NNPNNs), which inputs neural\\nnetworks and numerical values, instead of just numerical values. Thus enabling\\nneural networks to represent and process rich structures.'},\n",
              "     {'title': 'Finding Chaos in Noisy Systems',\n",
              "      'authors': ['Douglas Nychkatl',\n",
              "       'S. Ellner',\n",
              "       'Daniel Mccaftrey',\n",
              "       'A. R. Gallanl',\n",
              "       'Douglas Nychkat',\n",
              "       'S. Ellner',\n",
              "       'D. McCaffrey',\n",
              "       'A. Gallant',\n",
              "       'Neural Networks'],\n",
              "      'abstract': 'In the past twenty years there has been much interest in the physical and biological sciences in nonlinear dynamical systems that appear to have random, unpredictable behavior. One important parameter of a dynamic system is the dominant Lyapunov exponent (LE). When the behavior of the system is compared for two similar initial conditions, this exponent is related to the rate at which the subsequent trajectories diverge. A bounded system with a positive LE is one operational definition of chaotic behavior. Most methods for determining the LE have assumed thousands of observations generated from carefully controlled physical experiments. Less attention has been given to estimating the LE for biological and economic systems that are subjected to random perturbations and observed over a limited amount of time. Using nonparametric regression techniques (Neural Networks and Thin Plate Splines) it is possible to consistently estimate the LE. The properties of these methods have been studied using simulated data and are applied to a biological time series: marten fur returns for the Hudson Bay Company (1820-1900). Based on a nonparametric analysis there is little evidence for lowdimensional chaos in these data. Although these methods appear to work well for systems perturbed by small amounts of noise, finding chaos in a system with a significant stochastic component may be difficult.',\n",
              "      'year': 1992,\n",
              "      'url': 'https://www.semanticscholar.org/paper/0cadae8b546910d8a226954de73530e1e52131e4',\n",
              "      'source': 'Semantic Scholar',\n",
              "      'citation_count': 329,\n",
              "      'venue': '',\n",
              "      'publication_date': '',\n",
              "      'content': 'In the past twenty years there has been much interest in the physical and biological sciences in nonlinear dynamical systems that appear to have random, unpredictable behavior. One important parameter of a dynamic system is the dominant Lyapunov exponent (LE). When the behavior of the system is compared for two similar initial conditions, this exponent is related to the rate at which the subsequent trajectories diverge. A bounded system with a positive LE is one operational definition of chaotic behavior. Most methods for determining the LE have assumed thousands of observations generated from carefully controlled physical experiments. Less attention has been given to estimating the LE for biological and economic systems that are subjected to random perturbations and observed over a limited amount of time. Using nonparametric regression techniques (Neural Networks and Thin Plate Splines) it is possible to consistently estimate the LE. The properties of these methods have been studied using simulated data and are applied to a biological time series: marten fur returns for the Hudson Bay Company (1820-1900). Based on a nonparametric analysis there is little evidence for lowdimensional chaos in these data. Although these methods appear to work well for systems perturbed by small amounts of noise, finding chaos in a system with a significant stochastic component may be difficult.',\n",
              "      'external_ids': {'MAG': '182905047',\n",
              "       'DOI': '10.1111/J.2517-6161.1992.TB01889.X',\n",
              "       'CorpusId': 16621980}},\n",
              "     {'title': 'EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks',\n",
              "      'authors': ['Mingxing Tan', 'Quoc V. Le'],\n",
              "      'abstract': 'Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet. \\nTo go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.4% top-1 / 97.1% top-5 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flowers (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at this https URL.',\n",
              "      'year': 2019,\n",
              "      'url': 'https://www.semanticscholar.org/paper/4f2eda8077dc7a69bb2b4e0a1a086cf054adb3f9',\n",
              "      'source': 'Semantic Scholar',\n",
              "      'citation_count': 18321,\n",
              "      'venue': 'International Conference on Machine Learning',\n",
              "      'publication_date': '2019-05-24',\n",
              "      'content': 'Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet. \\nTo go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.4% top-1 / 97.1% top-5 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flowers (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at this https URL.',\n",
              "      'external_ids': {'DBLP': 'conf/icml/TanL19',\n",
              "       'MAG': '2946948417',\n",
              "       'ArXiv': '1905.11946',\n",
              "       'CorpusId': 167217261}},\n",
              "     {'title': 'How Powerful are Graph Neural Networks?',\n",
              "      'authors': ['Keyulu Xu', 'Weihua Hu', 'J. Leskovec', 'S. Jegelka'],\n",
              "      'abstract': 'Graph Neural Networks (GNNs) are an effective framework for representation learning of graphs. GNNs follow a neighborhood aggregation scheme, where the representation vector of a node is computed by recursively aggregating and transforming representation vectors of its neighboring nodes. Many GNN variants have been proposed and have achieved state-of-the-art results on both node and graph classification tasks. However, despite GNNs revolutionizing graph representation learning, there is limited understanding of their representational properties and limitations. Here, we present a theoretical framework for analyzing the expressive power of GNNs to capture different graph structures. Our results characterize the discriminative power of popular GNN variants, such as Graph Convolutional Networks and GraphSAGE, and show that they cannot learn to distinguish certain simple graph structures. We then develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism test. We empirically validate our theoretical findings on a number of graph classification benchmarks, and demonstrate that our model achieves state-of-the-art performance.',\n",
              "      'year': 2018,\n",
              "      'url': 'https://www.semanticscholar.org/paper/62ed9bf1d83c8db1f9cbf92ea2f57ea90ef683d9',\n",
              "      'source': 'Semantic Scholar',\n",
              "      'citation_count': 7728,\n",
              "      'venue': 'International Conference on Learning Representations',\n",
              "      'publication_date': '2018-10-01',\n",
              "      'content': 'Graph Neural Networks (GNNs) are an effective framework for representation learning of graphs. GNNs follow a neighborhood aggregation scheme, where the representation vector of a node is computed by recursively aggregating and transforming representation vectors of its neighboring nodes. Many GNN variants have been proposed and have achieved state-of-the-art results on both node and graph classification tasks. However, despite GNNs revolutionizing graph representation learning, there is limited understanding of their representational properties and limitations. Here, we present a theoretical framework for analyzing the expressive power of GNNs to capture different graph structures. Our results characterize the discriminative power of popular GNN variants, such as Graph Convolutional Networks and GraphSAGE, and show that they cannot learn to distinguish certain simple graph structures. We then develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism test. We empirically validate our theoretical findings on a number of graph classification benchmarks, and demonstrate that our model achieves state-of-the-art performance.',\n",
              "      'external_ids': {'MAG': '2950468517',\n",
              "       'ArXiv': '1810.00826',\n",
              "       'DBLP': 'journals/corr/abs-1810-00826',\n",
              "       'CorpusId': 52895589}},\n",
              "     {'title': 'Hybrid neural networks',\n",
              "      'authors': [],\n",
              "      'abstract': '',\n",
              "      'year': 1988,\n",
              "      'url': 'https://doi.org/10.1016/0893-6080(88)90047-0',\n",
              "      'doi': '10.1016/0893-6080(88)90047-0',\n",
              "      'publisher': 'Elsevier BV',\n",
              "      'journal': 'Neural Networks',\n",
              "      'source': 'CrossRef',\n",
              "      'content': ''},\n",
              "     {'title': 'Neural Networks',\n",
              "      'authors': [],\n",
              "      'abstract': '',\n",
              "      'year': 2016,\n",
              "      'url': 'https://doi.org/10.1016/s0893-6080(15)00246-4',\n",
              "      'doi': '10.1016/s0893-6080(15)00246-4',\n",
              "      'publisher': 'Elsevier BV',\n",
              "      'journal': 'Neural Networks',\n",
              "      'source': 'CrossRef',\n",
              "      'content': ''},\n",
              "     {'title': 'Protecting Deep Learning Model Copyrights With Adversarial Example-Free Reuse Detection.',\n",
              "      'authors': ['Xiaokun Luan', 'Xiyue Zhang', 'Jingyi Wang', 'Meng Sun'],\n",
              "      'abstract': \"Model reuse techniques can reduce the resource requirements for training high-performance deep neural networks (DNNs) by leveraging existing models. However, unauthorized reuse and replication of DNNs can lead to copyright infringement and economic loss to the model owner. This underscores the need to analyze the reuse relation between DNNs and develop copyright protection techniques to safeguard intellectual property rights. Existing DNN copyright protection approaches suffer from several inherent limitations hindering their effectiveness in practical scenarios. For instance, existing white-box fingerprinting approaches cannot address the common heterogeneous reuse case where the model architecture is changed, and DNN fingerprinting approaches heavily rely on generating adversarial examples with good transferability, which is known to be challenging in the black-box setting. To bridge the gap, we propose a neuron functionality analysis-based reuse detector (NFARD), a neuron functionality (NF) analysis-based reuse detector, which only requires normal test samples to detect reuse relations by measuring the models' differences on a newly proposed model characterization, i.e., NF. A set of NF-based distance metrics is designed to make NFARD applicable to both white-box and black-box settings. Moreover, we devise a linear transformation method to handle heterogeneous reuse cases by constructing the optimal projection matrix for dimension consistency, significantly extending the application scope of NFARD. To the best of our knowledge, this is the first adversarial example-free method that exploits NF for DNN copyright protection. As a side contribution, we constructed a reuse detection benchmark named Reuse Zoo that covers various practical reuse techniques and popular datasets. Extensive evaluations on this comprehensive benchmark show that NFARD achieves $F1$ scores of 0.984 and 1.0 for detecting reuse relationships in black-box and white-box settings, respectively, while generating test suites $2{\\\\sim } 99$ times faster than previous methods.\",\n",
              "      'year': 2025,\n",
              "      'url': 'https://pubmed.ncbi.nlm.nih.gov/40627481/',\n",
              "      'pmid': '40627481',\n",
              "      'source': 'PubMed',\n",
              "      'content': \"Model reuse techniques can reduce the resource requirements for training high-performance deep neural networks (DNNs) by leveraging existing models. However, unauthorized reuse and replication of DNNs can lead to copyright infringement and economic loss to the model owner. This underscores the need to analyze the reuse relation between DNNs and develop copyright protection techniques to safeguard intellectual property rights. Existing DNN copyright protection approaches suffer from several inherent limitations hindering their effectiveness in practical scenarios. For instance, existing white-box fingerprinting approaches cannot address the common heterogeneous reuse case where the model architecture is changed, and DNN fingerprinting approaches heavily rely on generating adversarial examples with good transferability, which is known to be challenging in the black-box setting. To bridge the gap, we propose a neuron functionality analysis-based reuse detector (NFARD), a neuron functionality (NF) analysis-based reuse detector, which only requires normal test samples to detect reuse relations by measuring the models' differences on a newly proposed model characterization, i.e., NF. A set of NF-based distance metrics is designed to make NFARD applicable to both white-box and black-box settings. Moreover, we devise a linear transformation method to handle heterogeneous reuse cases by constructing the optimal projection matrix for dimension consistency, significantly extending the application scope of NFARD. To the best of our knowledge, this is the first adversarial example-free method that exploits NF for DNN copyright protection. As a side contribution, we constructed a reuse detection benchmark named Reuse Zoo that covers various practical reuse techniques and popular datasets. Extensive evaluations on this comprehensive benchmark show that NFARD achieves $F1$ scores of 0.984 and 1.0 for detecting reuse relationships in black-box and white-box settings, respectively, while generating test suites $2{\\\\sim } 99$ times faster than previous methods.\"}],\n",
              "    'analyses': {},\n",
              "    'notes': [],\n",
              "    'progress': {'literature_search': 'completed',\n",
              "     'paper_analysis': 'pending',\n",
              "     'gap_analysis': 'pending',\n",
              "     'trend_analysis': 'pending',\n",
              "     'report_generation': 'pending'},\n",
              "    'research_plan': '**Comprehensive Research Plan: Transformer Architecture Analysis**\\n\\n**Introduction:**\\nThe transformer architecture has revolutionized the field of natural language processing (NLP) and beyond, with its ability to handle sequential data and parallelize computation. At the heart of the transformer lies the attention mechanism, which enables the model to focus on specific parts of the input data when generating outputs. This research plan aims to investigate the role of attention mechanisms in improving transformer performance, with a comprehensive analysis of the existing literature, key areas of investigation, methodology approach, expected outcomes, and timeline recommendations.\\n\\n**1. Literature Search Strategy:**\\n\\nTo conduct a thorough literature review, the following search strategy will be employed:\\n\\n* **Databases:** Search academic databases such as Google Scholar, IEEE Xplore, ACM Digital Library, and arXiv for relevant papers and articles.\\n* **Keywords:** Use a combination of keywords, including \"transformer,\" \"attention mechanism,\" \"neural networks,\" \"natural language processing,\" and \"deep learning.\"\\n* **Inclusion Criteria:** Include papers that:\\n\\t+ Focus on transformer architectures and attention mechanisms.\\n\\t+ Investigate the impact of attention mechanisms on transformer performance.\\n\\t+ Propose new attention mechanisms or variants.\\n\\t+ Evaluate transformer performance on various NLP tasks.\\n* **Exclusion Criteria:** Exclude papers that:\\n\\t+ Do not focus on transformer architectures or attention mechanisms.\\n\\t+ Are not related to NLP or deep learning.\\n\\t+ Are not published in reputable academic venues.\\n* **Search Tools:** Utilize search tools such as citation tracking, author searching, and keyword extraction to identify relevant papers and authors.\\n\\n**2. Key Areas to Investigate:**\\n\\nThe following key areas will be investigated to address the research question:\\n\\n* **Attention Mechanism Variants:** Investigate different attention mechanism variants, such as self-attention, multi-head attention, and hierarchical attention.\\n* **Transformer Architecture:** Analyze the transformer architecture, including the encoder-decoder structure, self-attention mechanisms, and feed-forward neural networks.\\n* **NLP Tasks:** Evaluate transformer performance on various NLP tasks, including machine translation, text classification, sentiment analysis, and question answering.\\n* **Performance Metrics:** Investigate the impact of attention mechanisms on transformer performance using metrics such as accuracy, F1-score, BLEU score, and ROUGE score.\\n* **Interpretability and Explainability:** Examine the interpretability and explainability of attention mechanisms in transformers, including visualization techniques and feature importance.\\n\\n**3. Methodology Approach:**\\n\\nThe following methodology approach will be employed:\\n\\n* **Systematic Review:** Conduct a systematic review of the existing literature on transformer architectures and attention mechanisms.\\n* **Experimental Evaluation:** Perform experimental evaluations of transformer models with different attention mechanisms on various NLP tasks.\\n* **Comparative Analysis:** Compare the performance of transformer models with different attention mechanisms, including self-attention, multi-head attention, and hierarchical attention.\\n* **Ablation Studies:** Conduct ablation studies to investigate the impact of individual components, such as attention mechanisms, on transformer performance.\\n* **Visualization and Interpretation:** Utilize visualization techniques, such as attention visualization, to interpret and understand the behavior of attention mechanisms in transformers.\\n\\n**4. Expected Outcomes:**\\n\\nThe expected outcomes of this research plan include:\\n\\n* **Comprehensive Literature Review:** A thorough understanding of the existing literature on transformer architectures and attention mechanisms.\\n* **Improved Understanding of Attention Mechanisms:** Insights into the role of attention mechanisms in improving transformer performance.\\n* **Performance Comparison:** A comparative analysis of transformer models with different attention mechanisms on various NLP tasks.\\n* **Interpretability and Explainability:** A deeper understanding of the interpretability and explainability of attention mechanisms in transformers.\\n* **Future Research Directions:** Identification of future research directions and potential applications of transformer architectures and attention mechanisms.\\n\\n**5. Timeline Recommendations:**\\n\\nThe following timeline recommendations are proposed:\\n\\n* **Literature Review (4 weeks):** Conduct a comprehensive literature review of transformer architectures and attention mechanisms.\\n* **Experimental Evaluation (8 weeks):** Perform experimental evaluations of transformer models with different attention mechanisms on various NLP tasks.\\n* **Data Analysis and Interpretation (4 weeks):** Analyze and interpret the results of the experimental evaluations.\\n* **Writing and Revisions (4 weeks):** Write and revise the research report, including the introduction, literature review, methodology, results, and discussion.\\n* **Finalization and Submission (2 weeks):** Finalize and submit the research report, including any necessary revisions and edits.\\n\\nOverall, this research plan provides a comprehensive framework for investigating the role of attention mechanisms in improving transformer performance. By following this plan, researchers can gain a deeper understanding of the transformer architecture and attention mechanisms, leading to improved performance and applications in NLP and beyond.'},\n",
              "   'message': 'Research project \"Transformer Architecture Analysis\" created successfully'},\n",
              "  'literature_search': {'success': True,\n",
              "   'papers_found': 33,\n",
              "   'search_results': {'transformer': 12,\n",
              "    'attention mechanism': 12,\n",
              "    'neural networks': 11},\n",
              "   'unique_papers': 33,\n",
              "   'message': 'Found 33 unique papers'},\n",
              "  'trend_analysis': {'query': 'large language models',\n",
              "   'trend_status': 'active',\n",
              "   'papers_found': 18,\n",
              "   'recent_papers': [{'title': 'All in One: Visual-Description-Guided Unified Point Cloud Segmentation',\n",
              "     'authors': ['Zongyan Han',\n",
              "      'Mohamed El Amine Boudjoghra',\n",
              "      'Jiahua Dong',\n",
              "      'Jinhong Wang',\n",
              "      'Rao Muhammad Anwer'],\n",
              "     'abstract': 'Unified segmentation of 3D point clouds is crucial for scene understanding,\\nbut is hindered by its sparse structure, limited annotations, and the challenge\\nof distinguishing fine-grained object classes in complex environments. Existing\\nmethods often struggle to capture rich semantic and contextual information due\\nto limited supervision and a lack of diverse multimodal cues, leading to\\nsuboptimal differentiation of classes and instances. To address these\\nchallenges, we propose VDG-Uni3DSeg, a novel framework that integrates\\npre-trained vision-language models (e.g., CLIP) and large language models\\n(LLMs) to enhance 3D segmentation. By leveraging LLM-generated textual\\ndescriptions and reference images from the internet, our method incorporates\\nrich multimodal cues, facilitating fine-grained class and instance separation.\\nWe further design a Semantic-Visual Contrastive Loss to align point features\\nwith multimodal queries and a Spatial Enhanced Module to model scene-wide\\nrelationships efficiently. Operating within a closed-set paradigm that utilizes\\nmultimodal knowledge generated offline, VDG-Uni3DSeg achieves state-of-the-art\\nresults in semantic, instance, and panoptic segmentation, offering a scalable\\nand practical solution for 3D understanding. Our code is available at\\nhttps://github.com/Hanzy1996/VDG-Uni3DSeg.',\n",
              "     'url': 'http://arxiv.org/abs/2507.05211v1',\n",
              "     'pdf_url': 'http://arxiv.org/pdf/2507.05211v1',\n",
              "     'published': '2025-07-07T17:22:00+00:00',\n",
              "     'categories': ['cs.CV', 'cs.AI'],\n",
              "     'primary_category': 'cs.CV',\n",
              "     'source': 'arXiv',\n",
              "     'year': 2025,\n",
              "     'content': 'Unified segmentation of 3D point clouds is crucial for scene understanding,\\nbut is hindered by its sparse structure, limited annotations, and the challenge\\nof distinguishing fine-grained object classes in complex environments. Existing\\nmethods often struggle to capture rich semantic and contextual information due\\nto limited supervision and a lack of diverse multimodal cues, leading to\\nsuboptimal differentiation of classes and instances. To address these\\nchallenges, we propose VDG-Uni3DSeg, a novel framework that integrates\\npre-trained vision-language models (e.g., CLIP) and large language models\\n(LLMs) to enhance 3D segmentation. By leveraging LLM-generated textual\\ndescriptions and reference images from the internet, our method incorporates\\nrich multimodal cues, facilitating fine-grained class and instance separation.\\nWe further design a Semantic-Visual Contrastive Loss to align point features\\nwith multimodal queries and a Spatial Enhanced Module to model scene-wide\\nrelationships efficiently. Operating within a closed-set paradigm that utilizes\\nmultimodal knowledge generated offline, VDG-Uni3DSeg achieves state-of-the-art\\nresults in semantic, instance, and panoptic segmentation, offering a scalable\\nand practical solution for 3D understanding. Our code is available at\\nhttps://github.com/Hanzy1996/VDG-Uni3DSeg.'},\n",
              "    {'title': 'OpenS2S: Advancing Open-Source End-to-End Empathetic Large Speech Language Model',\n",
              "     'authors': ['Chen Wang',\n",
              "      'Tianyu Peng',\n",
              "      'Wen Yang',\n",
              "      'Yinan Bai',\n",
              "      'Guangfu Wang',\n",
              "      'Jun Lin',\n",
              "      'Lanpeng Jia',\n",
              "      'Lingxiang Wu',\n",
              "      'Jinqiao Wang',\n",
              "      'Chengqing Zong',\n",
              "      'Jiajun Zhang'],\n",
              "     'abstract': 'Empathetic interaction is a cornerstone of human-machine communication, due\\nto the need for understanding speech enriched with paralinguistic cues and\\ngenerating emotional and expressive responses. However, the most powerful\\nempathetic LSLMs are increasingly closed off, leaving the crucial details about\\nthe architecture, data and development opaque to researchers. Given the\\ncritical need for transparent research into the LSLMs and empathetic behavior,\\nwe present OpenS2S, a fully open-source, transparent and end-to-end LSLM\\ndesigned to enable empathetic speech interactions. Based on our empathetic\\nspeech-to-text model BLSP-Emo, OpenS2S further employs a streaming interleaved\\ndecoding architecture to achieve low-latency speech generation. To facilitate\\nend-to-end training, OpenS2S incorporates an automated data construction\\npipeline that synthesizes diverse, high-quality empathetic speech dialogues at\\nlow cost. By leveraging large language models to generate empathetic content\\nand controllable text-to-speech systems to introduce speaker and emotional\\nvariation, we construct a scalable training corpus with rich paralinguistic\\ndiversity and minimal human supervision. We release the fully open-source\\nOpenS2S model, including the dataset, model weights, pre-training and\\nfine-tuning codes, to empower the broader research community and accelerate\\ninnovation in empathetic speech systems. The project webpage can be accessed at\\nhttps://casia-lm.github.io/OpenS2S',\n",
              "     'url': 'http://arxiv.org/abs/2507.05177v1',\n",
              "     'pdf_url': 'http://arxiv.org/pdf/2507.05177v1',\n",
              "     'published': '2025-07-07T16:31:37+00:00',\n",
              "     'categories': ['cs.CL', 'cs.AI', 'cs.SD', 'eess.AS'],\n",
              "     'primary_category': 'cs.CL',\n",
              "     'source': 'arXiv',\n",
              "     'year': 2025,\n",
              "     'content': 'Empathetic interaction is a cornerstone of human-machine communication, due\\nto the need for understanding speech enriched with paralinguistic cues and\\ngenerating emotional and expressive responses. However, the most powerful\\nempathetic LSLMs are increasingly closed off, leaving the crucial details about\\nthe architecture, data and development opaque to researchers. Given the\\ncritical need for transparent research into the LSLMs and empathetic behavior,\\nwe present OpenS2S, a fully open-source, transparent and end-to-end LSLM\\ndesigned to enable empathetic speech interactions. Based on our empathetic\\nspeech-to-text model BLSP-Emo, OpenS2S further employs a streaming interleaved\\ndecoding architecture to achieve low-latency speech generation. To facilitate\\nend-to-end training, OpenS2S incorporates an automated data construction\\npipeline that synthesizes diverse, high-quality empathetic speech dialogues at\\nlow cost. By leveraging large language models to generate empathetic content\\nand controllable text-to-speech systems to introduce speaker and emotional\\nvariation, we construct a scalable training corpus with rich paralinguistic\\ndiversity and minimal human supervision. We release the fully open-source\\nOpenS2S model, including the dataset, model weights, pre-training and\\nfine-tuning codes, to empower the broader research community and accelerate\\ninnovation in empathetic speech systems. The project webpage can be accessed at\\nhttps://casia-lm.github.io/OpenS2S'},\n",
              "    {'title': 'AI Generated Text Detection Using Instruction Fine-tuned Large Language and Transformer-Based Models',\n",
              "     'authors': ['Chinnappa Guggilla',\n",
              "      'Budhaditya Roy',\n",
              "      'Trupti Ramdas Chavan',\n",
              "      'Abdul Rahman',\n",
              "      'Edward Bowen'],\n",
              "     'abstract': 'Large Language Models (LLMs) possess an extraordinary capability to produce\\ntext that is not only coherent and contextually relevant but also strikingly\\nsimilar to human writing. They adapt to various styles and genres, producing\\ncontent that is both grammatically correct and semantically meaningful.\\nRecently, LLMs have been misused to create highly realistic phishing emails,\\nspread fake news, generate code to automate cyber crime, and write fraudulent\\nscientific articles. Additionally, in many real-world applications, the\\ngenerated content including style and topic and the generator model are not\\nknown beforehand. The increasing prevalence and sophistication of artificial\\nintelligence (AI)-generated texts have made their detection progressively more\\nchallenging. Various attempts have been made to distinguish machine-generated\\ntext from human-authored content using linguistic, statistical, machine\\nlearning, and ensemble-based approaches. This work focuses on two primary\\nobjectives Task-A, which involves distinguishing human-written text from\\nmachine-generated text, and Task-B, which attempts to identify the specific LLM\\nmodel responsible for the generation. Both of these tasks are based on fine\\ntuning of Generative Pre-trained Transformer (GPT_4o-mini), Large Language\\nModel Meta AI (LLaMA) 3 8B, and Bidirectional Encoder Representations from\\nTransformers (BERT). The fine-tuned version of GPT_4o-mini and the BERT model\\nhas achieved accuracies of 0.9547 for Task-A and 0.4698 for Task-B.',\n",
              "     'url': 'http://arxiv.org/abs/2507.05157v1',\n",
              "     'pdf_url': 'http://arxiv.org/pdf/2507.05157v1',\n",
              "     'published': '2025-07-07T16:13:13+00:00',\n",
              "     'categories': ['cs.CL', 'cs.AI'],\n",
              "     'primary_category': 'cs.CL',\n",
              "     'source': 'arXiv',\n",
              "     'year': 2025,\n",
              "     'content': 'Large Language Models (LLMs) possess an extraordinary capability to produce\\ntext that is not only coherent and contextually relevant but also strikingly\\nsimilar to human writing. They adapt to various styles and genres, producing\\ncontent that is both grammatically correct and semantically meaningful.\\nRecently, LLMs have been misused to create highly realistic phishing emails,\\nspread fake news, generate code to automate cyber crime, and write fraudulent\\nscientific articles. Additionally, in many real-world applications, the\\ngenerated content including style and topic and the generator model are not\\nknown beforehand. The increasing prevalence and sophistication of artificial\\nintelligence (AI)-generated texts have made their detection progressively more\\nchallenging. Various attempts have been made to distinguish machine-generated\\ntext from human-authored content using linguistic, statistical, machine\\nlearning, and ensemble-based approaches. This work focuses on two primary\\nobjectives Task-A, which involves distinguishing human-written text from\\nmachine-generated text, and Task-B, which attempts to identify the specific LLM\\nmodel responsible for the generation. Both of these tasks are based on fine\\ntuning of Generative Pre-trained Transformer (GPT_4o-mini), Large Language\\nModel Meta AI (LLaMA) 3 8B, and Bidirectional Encoder Representations from\\nTransformers (BERT). The fine-tuned version of GPT_4o-mini and the BERT model\\nhas achieved accuracies of 0.9547 for Task-A and 0.4698 for Task-B.'},\n",
              "    {'title': 'An Evaluation of Large Language Models on Text Summarization Tasks Using Prompt Engineering Techniques',\n",
              "     'authors': ['Walid Mohamed Aly',\n",
              "      'Taysir Hassan A. Soliman',\n",
              "      'Amr Mohamed AbdelAziz'],\n",
              "     'abstract': 'Large Language Models (LLMs) continue to advance natural language processing\\nwith their ability to generate human-like text across a range of tasks. Despite\\nthe remarkable success of LLMs in Natural Language Processing (NLP), their\\nperformance in text summarization across various domains and datasets has not\\nbeen comprehensively evaluated. At the same time, the ability to summarize text\\neffectively without relying on extensive training data has become a crucial\\nbottleneck. To address these issues, we present a systematic evaluation of six\\nLLMs across four datasets: CNN/Daily Mail and NewsRoom (news), SAMSum (dialog),\\nand ArXiv (scientific). By leveraging prompt engineering techniques including\\nzero-shot and in-context learning, our study evaluates the performance using\\nthe ROUGE and BERTScore metrics. In addition, a detailed analysis of inference\\ntimes is conducted to better understand the trade-off between summarization\\nquality and computational efficiency. For Long documents, introduce a\\nsentence-based chunking strategy that enables LLMs with shorter context windows\\nto summarize extended inputs in multiple stages. The findings reveal that while\\nLLMs perform competitively on news and dialog tasks, their performance on long\\nscientific documents improves significantly when aided by chunking strategies.\\nIn addition, notable performance variations were observed based on model\\nparameters, dataset properties, and prompt design. These results offer\\nactionable insights into how different LLMs behave across task types,\\ncontributing to ongoing research in efficient, instruction-based NLP systems.',\n",
              "     'url': 'http://arxiv.org/abs/2507.05123v1',\n",
              "     'pdf_url': 'http://arxiv.org/pdf/2507.05123v1',\n",
              "     'published': '2025-07-07T15:34:05+00:00',\n",
              "     'categories': ['cs.CL', 'cs.AI'],\n",
              "     'primary_category': 'cs.CL',\n",
              "     'source': 'arXiv',\n",
              "     'year': 2025,\n",
              "     'content': 'Large Language Models (LLMs) continue to advance natural language processing\\nwith their ability to generate human-like text across a range of tasks. Despite\\nthe remarkable success of LLMs in Natural Language Processing (NLP), their\\nperformance in text summarization across various domains and datasets has not\\nbeen comprehensively evaluated. At the same time, the ability to summarize text\\neffectively without relying on extensive training data has become a crucial\\nbottleneck. To address these issues, we present a systematic evaluation of six\\nLLMs across four datasets: CNN/Daily Mail and NewsRoom (news), SAMSum (dialog),\\nand ArXiv (scientific). By leveraging prompt engineering techniques including\\nzero-shot and in-context learning, our study evaluates the performance using\\nthe ROUGE and BERTScore metrics. In addition, a detailed analysis of inference\\ntimes is conducted to better understand the trade-off between summarization\\nquality and computational efficiency. For Long documents, introduce a\\nsentence-based chunking strategy that enables LLMs with shorter context windows\\nto summarize extended inputs in multiple stages. The findings reveal that while\\nLLMs perform competitively on news and dialog tasks, their performance on long\\nscientific documents improves significantly when aided by chunking strategies.\\nIn addition, notable performance variations were observed based on model\\nparameters, dataset properties, and prompt design. These results offer\\nactionable insights into how different LLMs behave across task types,\\ncontributing to ongoing research in efficient, instruction-based NLP systems.'},\n",
              "    {'title': 'VerifyLLM: LLM-Based Pre-Execution Task Plan Verification for Robots',\n",
              "     'authors': ['Danil S. Grigorev',\n",
              "      'Alexey K. Kovalev',\n",
              "      'Aleksandr I. Panov'],\n",
              "     'abstract': 'In the field of robotics, researchers face a critical challenge in ensuring\\nreliable and efficient task planning. Verifying high-level task plans before\\nexecution significantly reduces errors and enhance the overall performance of\\nthese systems. In this paper, we propose an architecture for automatically\\nverifying high-level task plans before their execution in simulator or\\nreal-world environments. Leveraging Large Language Models (LLMs), our approach\\nconsists of two key steps: first, the conversion of natural language\\ninstructions into Linear Temporal Logic (LTL), followed by a comprehensive\\nanalysis of action sequences. The module uses the reasoning capabilities of the\\nLLM to evaluate logical coherence and identify potential gaps in the plan.\\nRigorous testing on datasets of varying complexity demonstrates the broad\\napplicability of the module to household tasks. We contribute to improving the\\nreliability and efficiency of task planning and addresses the critical need for\\nrobust pre-execution verification in autonomous systems. The code is available\\nat https://verifyllm.github.io.',\n",
              "     'url': 'http://arxiv.org/abs/2507.05118v1',\n",
              "     'pdf_url': 'http://arxiv.org/pdf/2507.05118v1',\n",
              "     'published': '2025-07-07T15:31:36+00:00',\n",
              "     'categories': ['cs.RO', 'cs.AI'],\n",
              "     'primary_category': 'cs.RO',\n",
              "     'source': 'arXiv',\n",
              "     'year': 2025,\n",
              "     'content': 'In the field of robotics, researchers face a critical challenge in ensuring\\nreliable and efficient task planning. Verifying high-level task plans before\\nexecution significantly reduces errors and enhance the overall performance of\\nthese systems. In this paper, we propose an architecture for automatically\\nverifying high-level task plans before their execution in simulator or\\nreal-world environments. Leveraging Large Language Models (LLMs), our approach\\nconsists of two key steps: first, the conversion of natural language\\ninstructions into Linear Temporal Logic (LTL), followed by a comprehensive\\nanalysis of action sequences. The module uses the reasoning capabilities of the\\nLLM to evaluate logical coherence and identify potential gaps in the plan.\\nRigorous testing on datasets of varying complexity demonstrates the broad\\napplicability of the module to household tasks. We contribute to improving the\\nreliability and efficiency of task planning and addresses the critical need for\\nrobust pre-execution verification in autonomous systems. The code is available\\nat https://verifyllm.github.io.'}],\n",
              "   'trend_analysis': {'temporal_analysis': {'2025': {'paper_count': 8,\n",
              "      'top_keywords': ['language',\n",
              "       'models',\n",
              "       'model',\n",
              "       'human',\n",
              "       'large',\n",
              "       'text',\n",
              "       'llms',\n",
              "       'task',\n",
              "       'empathetic',\n",
              "       'fine',\n",
              "       'speech',\n",
              "       'evaluation',\n",
              "       'these',\n",
              "       'generated',\n",
              "       'content',\n",
              "       'systems',\n",
              "       'tasks',\n",
              "       'across',\n",
              "       'understanding',\n",
              "       'machine'],\n",
              "      'top_categories': {'cs.AI': 8,\n",
              "       'cs.CL': 3,\n",
              "       'cs.CV': 2,\n",
              "       'cs.SD': 1,\n",
              "       'eess.AS': 1,\n",
              "       'cs.RO': 1,\n",
              "       'cs.CR': 1,\n",
              "       'cs.LG': 1},\n",
              "      'sources': {'arXiv': 8},\n",
              "      'avg_citations': np.float64(0.0)},\n",
              "     '2022': {'paper_count': 4,\n",
              "      'top_keywords': ['models',\n",
              "       'language',\n",
              "       'reasoning',\n",
              "       'large',\n",
              "       'shot',\n",
              "       'llms',\n",
              "       'prompting',\n",
              "       'model',\n",
              "       'chain',\n",
              "       'thought',\n",
              "       'tasks',\n",
              "       'zero',\n",
              "       'palm',\n",
              "       'exemplars',\n",
              "       'state',\n",
              "       'step',\n",
              "       'clinical',\n",
              "       'abilities',\n",
              "       'parameter',\n",
              "       'accuracy'],\n",
              "      'top_categories': {},\n",
              "      'sources': {'Semantic Scholar': 4},\n",
              "      'avg_citations': np.float64(4819.0)},\n",
              "     '2023': {'paper_count': 4,\n",
              "      'top_keywords': ['language',\n",
              "       'models',\n",
              "       'model',\n",
              "       'large',\n",
              "       'image',\n",
              "       'training',\n",
              "       'frozen',\n",
              "       'vision',\n",
              "       'which',\n",
              "       'tasks',\n",
              "       'scale',\n",
              "       'trained',\n",
              "       'generation',\n",
              "       'advanced',\n",
              "       'problem',\n",
              "       'solving',\n",
              "       'llms',\n",
              "       'blip',\n",
              "       'also',\n",
              "       'capabilities'],\n",
              "      'top_categories': {},\n",
              "      'sources': {'Semantic Scholar': 4},\n",
              "      'avg_citations': np.float64(2897.25)},\n",
              "     '2021': {'paper_count': 2,\n",
              "      'top_keywords': ['model',\n",
              "       'lora',\n",
              "       'language',\n",
              "       'models',\n",
              "       'fine',\n",
              "       'parameters',\n",
              "       'rank',\n",
              "       'adaptation',\n",
              "       'trainable',\n",
              "       'code',\n",
              "       'large',\n",
              "       'which',\n",
              "       'tuned',\n",
              "       'github',\n",
              "       'solves',\n",
              "       'training',\n",
              "       'tasks',\n",
              "       'tuning',\n",
              "       'using',\n",
              "       'deploying'],\n",
              "      'top_categories': {},\n",
              "      'sources': {'Semantic Scholar': 2},\n",
              "      'avg_citations': np.float64(8164.0)}},\n",
              "    'emerging_trends': [{'type': 'new_keywords',\n",
              "      'keywords': ['human', 'llms', 'task', 'empathetic', 'fine', 'text'],\n",
              "      'period': '2025'},\n",
              "     {'type': 'growing_categories',\n",
              "      'categories': [{'category': 'cs.AI',\n",
              "        'growth_rate': 8.0,\n",
              "        'recent_count': 8,\n",
              "        'previous_count': 0},\n",
              "       {'category': 'cs.CL',\n",
              "        'growth_rate': 3.0,\n",
              "        'recent_count': 3,\n",
              "        'previous_count': 0},\n",
              "       {'category': 'cs.CV',\n",
              "        'growth_rate': 2.0,\n",
              "        'recent_count': 2,\n",
              "        'previous_count': 0},\n",
              "       {'category': 'cs.SD',\n",
              "        'growth_rate': 1.0,\n",
              "        'recent_count': 1,\n",
              "        'previous_count': 0},\n",
              "       {'category': 'eess.AS',\n",
              "        'growth_rate': 1.0,\n",
              "        'recent_count': 1,\n",
              "        'previous_count': 0},\n",
              "       {'category': 'cs.RO',\n",
              "        'growth_rate': 1.0,\n",
              "        'recent_count': 1,\n",
              "        'previous_count': 0},\n",
              "       {'category': 'cs.CR',\n",
              "        'growth_rate': 1.0,\n",
              "        'recent_count': 1,\n",
              "        'previous_count': 0},\n",
              "       {'category': 'cs.LG',\n",
              "        'growth_rate': 1.0,\n",
              "        'recent_count': 1,\n",
              "        'previous_count': 0}],\n",
              "      'period': '2025'}],\n",
              "    'timeframe': 'yearly',\n",
              "    'total_papers': 18,\n",
              "    'analysis_timestamp': '2025-07-08T17:55:30.116580'},\n",
              "   'ai_insights': '**Analysis of Recent Research Papers on \"Large Language Models\"**\\n\\n1. **Current research momentum: High** (18 recent papers, growing categories, and new keywords indicate a surge in research activity)\\n2. **Key developments in the last 30 days:** Introduction of new keywords like \"empathetic\" and \"fine\" text processing, and growth in categories like cs.AI, cs.CL, and cs.CV, suggesting advancements in AI, natural language processing, and computer vision applications.\\n3. **Emerging sub-topics or applications:** Human-LLM interaction, empathetic language understanding, and fine-text processing are emerging sub-topics. Applications in computer vision (cs.CV) and robotics (cs.RO) are also gaining traction.\\n4. **Potential future directions:** Researchers may explore developing more empathetic and human-like LLMs, integrating LLMs with computer vision and robotics, and fine-tuning LLMs for specific tasks and applications, such as text processing and human-LLM collaboration.',\n",
              "   'monitoring_timestamp': '2025-07-08T17:55:31.450316'}},\n",
              " 'message': 'All demo components executed successfully!'}"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        " research_mate.demo()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "aNRKf8O3NoWT",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNRKf8O3NoWT",
        "outputId": "31c00ead-03e3-43aa-f44d-e8217eb82470"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç Quick search: 'attention'\n",
            "üîç Searching all sources for: 'attention'\n",
            "üîç Searching arXiv...\n",
            "‚úÖ Found 10 papers on arXiv\n",
            "üîç Searching Semantic Scholar...\n",
            "‚úÖ Found 10 papers on Semantic Scholar\n",
            "üîç Searching CrossRef...\n",
            "‚úÖ Found 10 papers on CrossRef\n",
            "üîç Searching PubMed...\n",
            "‚úÖ Found 10 papers on PubMed\n",
            "‚úÖ Total unique papers found: 37\n",
            "‚úÖ Added 32 document chunks to vectorstore\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'success': True,\n",
              " 'query': 'attention',\n",
              " 'results': [{'title': 'Exploring Human-like Attention Supervision in Visual Question Answering',\n",
              "   'authors': ['Tingting Qiao', 'Jianfeng Dong', 'Duanqing Xu'],\n",
              "   'abstract': 'Attention mechanisms have been widely applied in the Visual Question\\nAnswering (VQA) task, as they help to focus on the area-of-interest of both\\nvisual and textual information. To answer the questions correctly, the model\\nneeds to selectively target different areas of an image, which suggests that an\\nattention-based model may benefit from an explicit attention supervision. In\\nthis work, we aim to address the problem of adding attention supervision to VQA\\nmodels. Since there is a lack of human attention data, we first propose a Human\\nAttention Network (HAN) to generate human-like attention maps, training on a\\nrecently released dataset called Human ATtention Dataset (VQA-HAT). Then, we\\napply the pre-trained HAN on the VQA v2.0 dataset to automatically produce the\\nhuman-like attention maps for all image-question pairs. The generated\\nhuman-like attention map dataset for the VQA v2.0 dataset is named as\\nHuman-Like ATtention (HLAT) dataset. Finally, we apply human-like attention\\nsupervision to an attention-based VQA model. The experiments show that adding\\nhuman-like supervision yields a more accurate attention together with a better\\nperformance, showing a promising future for human-like attention supervision in\\nVQA.',\n",
              "   'url': 'http://arxiv.org/abs/1709.06308v1',\n",
              "   'pdf_url': 'http://arxiv.org/pdf/1709.06308v1',\n",
              "   'published': '2017-09-19T09:19:08+00:00',\n",
              "   'updated': '2017-09-19T09:19:08+00:00',\n",
              "   'categories': ['cs.CV'],\n",
              "   'primary_category': 'cs.CV',\n",
              "   'source': 'arXiv',\n",
              "   'year': 2017,\n",
              "   'content': 'Attention mechanisms have been widely applied in the Visual Question\\nAnswering (VQA) task, as they help to focus on the area-of-interest of both\\nvisual and textual information. To answer the questions correctly, the model\\nneeds to selectively target different areas of an image, which suggests that an\\nattention-based model may benefit from an explicit attention supervision. In\\nthis work, we aim to address the problem of adding attention supervision to VQA\\nmodels. Since there is a lack of human attention data, we first propose a Human\\nAttention Network (HAN) to generate human-like attention maps, training on a\\nrecently released dataset called Human ATtention Dataset (VQA-HAT). Then, we\\napply the pre-trained HAN on the VQA v2.0 dataset to automatically produce the\\nhuman-like attention maps for all image-question pairs. The generated\\nhuman-like attention map dataset for the VQA v2.0 dataset is named as\\nHuman-Like ATtention (HLAT) dataset. Finally, we apply human-like attention\\nsupervision to an attention-based VQA model. The experiments show that adding\\nhuman-like supervision yields a more accurate attention together with a better\\nperformance, showing a promising future for human-like attention supervision in\\nVQA.'},\n",
              "  {'title': 'Simulating Hard Attention Using Soft Attention',\n",
              "   'authors': ['Andy Yang', 'Lena Strobl', 'David Chiang', 'Dana Angluin'],\n",
              "   'abstract': 'We study conditions under which transformers using soft attention can\\nsimulate hard attention, that is, effectively focus all attention on a subset\\nof positions. First, we examine several subclasses of languages recognized by\\nhard-attention transformers, which can be defined in variants of linear\\ntemporal logic. We demonstrate how soft-attention transformers can compute\\nformulas of these logics using unbounded positional embeddings or temperature\\nscaling. Second, we demonstrate how temperature scaling allows softmax\\ntransformers to simulate general hard-attention transformers, using a\\ntemperature that depends on the minimum gap between the maximum attention\\nscores and other attention scores.',\n",
              "   'url': 'http://arxiv.org/abs/2412.09925v2',\n",
              "   'pdf_url': 'http://arxiv.org/pdf/2412.09925v2',\n",
              "   'published': '2024-12-13T07:27:42+00:00',\n",
              "   'updated': '2025-06-26T13:41:24+00:00',\n",
              "   'categories': ['cs.LG', 'cs.CL', 'cs.FL'],\n",
              "   'primary_category': 'cs.LG',\n",
              "   'source': 'arXiv',\n",
              "   'year': 2024,\n",
              "   'content': 'We study conditions under which transformers using soft attention can\\nsimulate hard attention, that is, effectively focus all attention on a subset\\nof positions. First, we examine several subclasses of languages recognized by\\nhard-attention transformers, which can be defined in variants of linear\\ntemporal logic. We demonstrate how soft-attention transformers can compute\\nformulas of these logics using unbounded positional embeddings or temperature\\nscaling. Second, we demonstrate how temperature scaling allows softmax\\ntransformers to simulate general hard-attention transformers, using a\\ntemperature that depends on the minimum gap between the maximum attention\\nscores and other attention scores.'},\n",
              "  {'title': 'Agent Attention: On the Integration of Softmax and Linear Attention',\n",
              "   'authors': ['Dongchen Han',\n",
              "    'Tianzhu Ye',\n",
              "    'Yizeng Han',\n",
              "    'Zhuofan Xia',\n",
              "    'Siyuan Pan',\n",
              "    'Pengfei Wan',\n",
              "    'Shiji Song',\n",
              "    'Gao Huang'],\n",
              "   'abstract': 'The attention module is the key component in Transformers. While the global\\nattention mechanism offers high expressiveness, its excessive computational\\ncost restricts its applicability in various scenarios. In this paper, we\\npropose a novel attention paradigm, Agent Attention, to strike a favorable\\nbalance between computational efficiency and representation power.\\nSpecifically, the Agent Attention, denoted as a quadruple $(Q, A, K, V)$,\\nintroduces an additional set of agent tokens $A$ into the conventional\\nattention module. The agent tokens first act as the agent for the query tokens\\n$Q$ to aggregate information from $K$ and $V$, and then broadcast the\\ninformation back to $Q$. Given the number of agent tokens can be designed to be\\nmuch smaller than the number of query tokens, the agent attention is\\nsignificantly more efficient than the widely adopted Softmax attention, while\\npreserving global context modelling capability. Interestingly, we show that the\\nproposed agent attention is equivalent to a generalized form of linear\\nattention. Therefore, agent attention seamlessly integrates the powerful\\nSoftmax attention and the highly efficient linear attention. Extensive\\nexperiments demonstrate the effectiveness of agent attention with various\\nvision Transformers and across diverse vision tasks, including image\\nclassification, object detection, semantic segmentation and image generation.\\nNotably, agent attention has shown remarkable performance in high-resolution\\nscenarios, owning to its linear attention nature. For instance, when applied to\\nStable Diffusion, our agent attention accelerates generation and substantially\\nenhances image generation quality without any additional training. Code is\\navailable at https://github.com/LeapLabTHU/Agent-Attention.',\n",
              "   'url': 'http://arxiv.org/abs/2312.08874v3',\n",
              "   'pdf_url': 'http://arxiv.org/pdf/2312.08874v3',\n",
              "   'published': '2023-12-14T16:26:29+00:00',\n",
              "   'updated': '2024-07-15T09:42:48+00:00',\n",
              "   'categories': ['cs.CV'],\n",
              "   'primary_category': 'cs.CV',\n",
              "   'source': 'arXiv',\n",
              "   'year': 2023,\n",
              "   'content': 'The attention module is the key component in Transformers. While the global\\nattention mechanism offers high expressiveness, its excessive computational\\ncost restricts its applicability in various scenarios. In this paper, we\\npropose a novel attention paradigm, Agent Attention, to strike a favorable\\nbalance between computational efficiency and representation power.\\nSpecifically, the Agent Attention, denoted as a quadruple $(Q, A, K, V)$,\\nintroduces an additional set of agent tokens $A$ into the conventional\\nattention module. The agent tokens first act as the agent for the query tokens\\n$Q$ to aggregate information from $K$ and $V$, and then broadcast the\\ninformation back to $Q$. Given the number of agent tokens can be designed to be\\nmuch smaller than the number of query tokens, the agent attention is\\nsignificantly more efficient than the widely adopted Softmax attention, while\\npreserving global context modelling capability. Interestingly, we show that the\\nproposed agent attention is equivalent to a generalized form of linear\\nattention. Therefore, agent attention seamlessly integrates the powerful\\nSoftmax attention and the highly efficient linear attention. Extensive\\nexperiments demonstrate the effectiveness of agent attention with various\\nvision Transformers and across diverse vision tasks, including image\\nclassification, object detection, semantic segmentation and image generation.\\nNotably, agent attention has shown remarkable performance in high-resolution\\nscenarios, owning to its linear attention nature. For instance, when applied to\\nStable Diffusion, our agent attention accelerates generation and substantially\\nenhances image generation quality without any additional training. Code is\\navailable at https://github.com/LeapLabTHU/Agent-Attention.'},\n",
              "  {'title': 'Tri-Attention: Explicit Context-Aware Attention Mechanism for Natural Language Processing',\n",
              "   'authors': ['Rui Yu', 'Yifeng Li', 'Wenpeng Lu', 'Longbing Cao'],\n",
              "   'abstract': 'In natural language processing (NLP), the context of a word or sentence plays\\nan essential role. Contextual information such as the semantic representation\\nof a passage or historical dialogue forms an essential part of a conversation\\nand a precise understanding of the present phrase or sentence. However, the\\nstandard attention mechanisms typically generate weights using query and key\\nbut ignore context, forming a Bi-Attention framework, despite their great\\nsuccess in modeling sequence alignment. This Bi-Attention mechanism does not\\nexplicitly model the interactions between the contexts, queries and keys of\\ntarget sequences, missing important contextual information and resulting in\\npoor attention performance. Accordingly, a novel and general triple-attention\\n(Tri-Attention) framework expands the standard Bi-Attention mechanism and\\nexplicitly interacts query, key, and context by incorporating context as the\\nthird dimension in calculating relevance scores. Four variants of Tri-Attention\\nare generated by expanding the two-dimensional vector-based additive,\\ndot-product, scaled dot-product, and bilinear operations in Bi-Attention to the\\ntensor operations for Tri-Attention. Extensive experiments on three NLP tasks\\ndemonstrate that Tri-Attention outperforms about 30 state-of-the-art\\nnon-attention, standard Bi-Attention, contextual Bi-Attention approaches and\\npretrained neural language models1.',\n",
              "   'url': 'http://arxiv.org/abs/2211.02899v1',\n",
              "   'pdf_url': 'http://arxiv.org/pdf/2211.02899v1',\n",
              "   'published': '2022-11-05T13:07:40+00:00',\n",
              "   'updated': '2022-11-05T13:07:40+00:00',\n",
              "   'categories': ['cs.CL', 'cs.AI'],\n",
              "   'primary_category': 'cs.CL',\n",
              "   'source': 'arXiv',\n",
              "   'year': 2022,\n",
              "   'content': 'In natural language processing (NLP), the context of a word or sentence plays\\nan essential role. Contextual information such as the semantic representation\\nof a passage or historical dialogue forms an essential part of a conversation\\nand a precise understanding of the present phrase or sentence. However, the\\nstandard attention mechanisms typically generate weights using query and key\\nbut ignore context, forming a Bi-Attention framework, despite their great\\nsuccess in modeling sequence alignment. This Bi-Attention mechanism does not\\nexplicitly model the interactions between the contexts, queries and keys of\\ntarget sequences, missing important contextual information and resulting in\\npoor attention performance. Accordingly, a novel and general triple-attention\\n(Tri-Attention) framework expands the standard Bi-Attention mechanism and\\nexplicitly interacts query, key, and context by incorporating context as the\\nthird dimension in calculating relevance scores. Four variants of Tri-Attention\\nare generated by expanding the two-dimensional vector-based additive,\\ndot-product, scaled dot-product, and bilinear operations in Bi-Attention to the\\ntensor operations for Tri-Attention. Extensive experiments on three NLP tasks\\ndemonstrate that Tri-Attention outperforms about 30 state-of-the-art\\nnon-attention, standard Bi-Attention, contextual Bi-Attention approaches and\\npretrained neural language models1.'},\n",
              "  {'title': 'SCCA: Shifted Cross Chunk Attention for long contextual semantic expansion',\n",
              "   'authors': ['Yuxiang Guo'],\n",
              "   'abstract': 'Sparse attention as a efficient method can significantly decrease the\\ncomputation cost, but current sparse attention tend to rely on window self\\nattention which block the global information flow. For this problem, we present\\nShifted Cross Chunk Attention (SCCA), using different KV shifting strategy to\\nextend respective field in each attention layer. Except, we combine Dilated\\nAttention(DA) and Dilated Neighborhood Attention(DNA) to present Shifted\\nDilated Attention(SDA). Both SCCA and SDA can accumulate attention results in\\nmulti head attention to obtain approximate respective field in full attention.\\nIn this paper, we conduct language modeling experiments using different pattern\\nof SCCA and combination of SCCA and SDA. The proposed shifted cross chunk\\nattention (SCCA) can effectively extend large language models (LLMs) to longer\\ncontext combined with Positional interpolation(PI) and LoRA than current sparse\\nattention. Notably, SCCA adopts LLaMA2 7B from 4k context to 8k in single V100.\\nThis attention pattern can provide a Plug-and-play fine-tuning method to extend\\nmodel context while retaining their original architectures, and is compatible\\nwith most existing techniques.',\n",
              "   'url': 'http://arxiv.org/abs/2312.07305v1',\n",
              "   'pdf_url': 'http://arxiv.org/pdf/2312.07305v1',\n",
              "   'published': '2023-12-12T14:24:54+00:00',\n",
              "   'updated': '2023-12-12T14:24:54+00:00',\n",
              "   'categories': ['cs.CL', 'cs.AI'],\n",
              "   'primary_category': 'cs.CL',\n",
              "   'source': 'arXiv',\n",
              "   'year': 2023,\n",
              "   'content': 'Sparse attention as a efficient method can significantly decrease the\\ncomputation cost, but current sparse attention tend to rely on window self\\nattention which block the global information flow. For this problem, we present\\nShifted Cross Chunk Attention (SCCA), using different KV shifting strategy to\\nextend respective field in each attention layer. Except, we combine Dilated\\nAttention(DA) and Dilated Neighborhood Attention(DNA) to present Shifted\\nDilated Attention(SDA). Both SCCA and SDA can accumulate attention results in\\nmulti head attention to obtain approximate respective field in full attention.\\nIn this paper, we conduct language modeling experiments using different pattern\\nof SCCA and combination of SCCA and SDA. The proposed shifted cross chunk\\nattention (SCCA) can effectively extend large language models (LLMs) to longer\\ncontext combined with Positional interpolation(PI) and LoRA than current sparse\\nattention. Notably, SCCA adopts LLaMA2 7B from 4k context to 8k in single V100.\\nThis attention pattern can provide a Plug-and-play fine-tuning method to extend\\nmodel context while retaining their original architectures, and is compatible\\nwith most existing techniques.'},\n",
              "  {'title': 'A General Survey on Attention Mechanisms in Deep Learning',\n",
              "   'authors': ['Gianni Brauwers', 'Flavius Frasincar'],\n",
              "   'abstract': 'Attention is an important mechanism that can be employed for a variety of\\ndeep learning models across many different domains and tasks. This survey\\nprovides an overview of the most important attention mechanisms proposed in the\\nliterature. The various attention mechanisms are explained by means of a\\nframework consisting of a general attention model, uniform notation, and a\\ncomprehensive taxonomy of attention mechanisms. Furthermore, the various\\nmeasures for evaluating attention models are reviewed, and methods to\\ncharacterize the structure of attention models based on the proposed framework\\nare discussed. Last, future work in the field of attention models is\\nconsidered.',\n",
              "   'url': 'http://arxiv.org/abs/2203.14263v1',\n",
              "   'pdf_url': 'http://arxiv.org/pdf/2203.14263v1',\n",
              "   'published': '2022-03-27T10:06:23+00:00',\n",
              "   'updated': '2022-03-27T10:06:23+00:00',\n",
              "   'categories': ['cs.LG'],\n",
              "   'primary_category': 'cs.LG',\n",
              "   'source': 'arXiv',\n",
              "   'year': 2022,\n",
              "   'content': 'Attention is an important mechanism that can be employed for a variety of\\ndeep learning models across many different domains and tasks. This survey\\nprovides an overview of the most important attention mechanisms proposed in the\\nliterature. The various attention mechanisms are explained by means of a\\nframework consisting of a general attention model, uniform notation, and a\\ncomprehensive taxonomy of attention mechanisms. Furthermore, the various\\nmeasures for evaluating attention models are reviewed, and methods to\\ncharacterize the structure of attention models based on the proposed framework\\nare discussed. Last, future work in the field of attention models is\\nconsidered.'},\n",
              "  {'title': 'Attention-Only Transformers and Implementing MLPs with Attention Heads',\n",
              "   'authors': ['Robert Huben', 'Valerie Morris'],\n",
              "   'abstract': \"The transformer architecture is widely used in machine learning models and\\nconsists of two alternating sublayers: attention heads and MLPs. We prove that\\nan MLP neuron can be implemented by a masked attention head with internal\\ndimension 1 so long as the MLP's activation function comes from a restricted\\nclass including SiLU and close approximations of ReLU and GeLU. This allows one\\nto convert an MLP-and-attention transformer into an attention-only transformer\\nat the cost of greatly increasing the number of attention heads. We also prove\\nthat attention heads can perform the components of an MLP (linear\\ntransformations and activation functions) separately. Finally, we prove that\\nattention heads can encode arbitrary masking patterns in their weight matrices\\nto within arbitrarily small error.\",\n",
              "   'url': 'http://arxiv.org/abs/2309.08593v1',\n",
              "   'pdf_url': 'http://arxiv.org/pdf/2309.08593v1',\n",
              "   'published': '2023-09-15T17:47:45+00:00',\n",
              "   'updated': '2023-09-15T17:47:45+00:00',\n",
              "   'categories': ['cs.LG'],\n",
              "   'primary_category': 'cs.LG',\n",
              "   'source': 'arXiv',\n",
              "   'year': 2023,\n",
              "   'content': \"The transformer architecture is widely used in machine learning models and\\nconsists of two alternating sublayers: attention heads and MLPs. We prove that\\nan MLP neuron can be implemented by a masked attention head with internal\\ndimension 1 so long as the MLP's activation function comes from a restricted\\nclass including SiLU and close approximations of ReLU and GeLU. This allows one\\nto convert an MLP-and-attention transformer into an attention-only transformer\\nat the cost of greatly increasing the number of attention heads. We also prove\\nthat attention heads can perform the components of an MLP (linear\\ntransformations and activation functions) separately. Finally, we prove that\\nattention heads can encode arbitrary masking patterns in their weight matrices\\nto within arbitrarily small error.\"},\n",
              "  {'title': 'Peer attention enhances student learning',\n",
              "   'authors': ['Songlin Xu', 'Dongyin Hu', 'Ru Wang', 'Xinyu Zhang'],\n",
              "   'abstract': \"Human visual attention is susceptible to social influences. In education,\\npeer effects impact student learning, but their precise role in modulating\\nattention remains unclear. Our experiment (N=311) demonstrates that displaying\\npeer visual attention regions when students watch online course videos enhances\\ntheir focus and engagement. However, students retain adaptability in following\\npeer attention cues. Overall, guided peer attention improves learning\\nexperiences and outcomes. These findings elucidate how peer visual attention\\nshapes students' gaze patterns, deepening understanding of peer influence on\\nlearning. They also offer insights into designing adaptive online learning\\ninterventions leveraging peer attention modelling to optimize student\\nattentiveness and success.\",\n",
              "   'url': 'http://arxiv.org/abs/2312.02358v1',\n",
              "   'pdf_url': 'http://arxiv.org/pdf/2312.02358v1',\n",
              "   'published': '2023-12-04T21:36:58+00:00',\n",
              "   'updated': '2023-12-04T21:36:58+00:00',\n",
              "   'categories': ['cs.HC', 'cs.AI'],\n",
              "   'primary_category': 'cs.HC',\n",
              "   'source': 'arXiv',\n",
              "   'year': 2023,\n",
              "   'content': \"Human visual attention is susceptible to social influences. In education,\\npeer effects impact student learning, but their precise role in modulating\\nattention remains unclear. Our experiment (N=311) demonstrates that displaying\\npeer visual attention regions when students watch online course videos enhances\\ntheir focus and engagement. However, students retain adaptability in following\\npeer attention cues. Overall, guided peer attention improves learning\\nexperiences and outcomes. These findings elucidate how peer visual attention\\nshapes students' gaze patterns, deepening understanding of peer influence on\\nlearning. They also offer insights into designing adaptive online learning\\ninterventions leveraging peer attention modelling to optimize student\\nattentiveness and success.\"},\n",
              "  {'title': 'GLU Attention Improve Transformer',\n",
              "   'authors': ['Zehao Wang'],\n",
              "   'abstract': 'Gated Linear Units (GLU) have shown great potential in enhancing neural\\nnetwork performance. In this paper, I introduce a novel attention mechanism\\ncalled GLU Attention, which introduces nonlinearity into the values of\\nAttention. My experiments demonstrate that GLU Attention improves both model\\nperformance and convergence speed across text and vision modalities with zero\\nadditional parameters and negligible computational costs. GLU Attention is\\nlightweight and can seamlessly integrate with other technologies, such as Flash\\nAttention, Rotary Position Embedding (RoPE), and various Multi-Head Attention\\n(MHA) variants such as Grouped-Query Attention (GQA). This project is\\nopen-sourced at github.',\n",
              "   'url': 'http://arxiv.org/abs/2507.00022v2',\n",
              "   'pdf_url': 'http://arxiv.org/pdf/2507.00022v2',\n",
              "   'published': '2025-06-16T18:38:56+00:00',\n",
              "   'updated': '2025-07-06T05:43:48+00:00',\n",
              "   'categories': ['cs.LG', 'cs.AI', 'cs.CL', 'cs.NE'],\n",
              "   'primary_category': 'cs.LG',\n",
              "   'source': 'arXiv',\n",
              "   'year': 2025,\n",
              "   'content': 'Gated Linear Units (GLU) have shown great potential in enhancing neural\\nnetwork performance. In this paper, I introduce a novel attention mechanism\\ncalled GLU Attention, which introduces nonlinearity into the values of\\nAttention. My experiments demonstrate that GLU Attention improves both model\\nperformance and convergence speed across text and vision modalities with zero\\nadditional parameters and negligible computational costs. GLU Attention is\\nlightweight and can seamlessly integrate with other technologies, such as Flash\\nAttention, Rotary Position Embedding (RoPE), and various Multi-Head Attention\\n(MHA) variants such as Grouped-Query Attention (GQA). This project is\\nopen-sourced at github.'},\n",
              "  {'title': 'HAR-Net: Joint Learning of Hybrid Attention for Single-stage Object Detection',\n",
              "   'authors': ['Ya-Li Li', 'Shengjin Wang'],\n",
              "   'abstract': 'Object detection has been a challenging task in computer vision. Although\\nsignificant progress has been made in object detection with deep neural\\nnetworks, the attention mechanism is far from development. In this paper, we\\npropose the hybrid attention mechanism for single-stage object detection.\\nFirst, we present the modules of spatial attention, channel attention and\\naligned attention for single-stage object detection. In particular, stacked\\ndilated convolution layers with symmetrically fixed rates are constructed to\\nlearn spatial attention. The channel attention is proposed with the cross-level\\ngroup normalization and squeeze-and-excitation module. Aligned attention is\\nconstructed with organized deformable filters. Second, the three kinds of\\nattention are unified to construct the hybrid attention mechanism. We then\\nembed the hybrid attention into Retina-Net and propose the efficient\\nsingle-stage HAR-Net for object detection. The attention modules and the\\nproposed HAR-Net are evaluated on the COCO detection dataset. Experiments\\ndemonstrate that hybrid attention can significantly improve the detection\\naccuracy and the HAR-Net can achieve the state-of-the-art 45.8\\\\% mAP,\\noutperform existing single-stage object detectors.',\n",
              "   'url': 'http://arxiv.org/abs/1904.11141v1',\n",
              "   'pdf_url': 'http://arxiv.org/pdf/1904.11141v1',\n",
              "   'published': '2019-04-25T03:37:19+00:00',\n",
              "   'updated': '2019-04-25T03:37:19+00:00',\n",
              "   'categories': ['cs.CV'],\n",
              "   'primary_category': 'cs.CV',\n",
              "   'source': 'arXiv',\n",
              "   'year': 2019,\n",
              "   'content': 'Object detection has been a challenging task in computer vision. Although\\nsignificant progress has been made in object detection with deep neural\\nnetworks, the attention mechanism is far from development. In this paper, we\\npropose the hybrid attention mechanism for single-stage object detection.\\nFirst, we present the modules of spatial attention, channel attention and\\naligned attention for single-stage object detection. In particular, stacked\\ndilated convolution layers with symmetrically fixed rates are constructed to\\nlearn spatial attention. The channel attention is proposed with the cross-level\\ngroup normalization and squeeze-and-excitation module. Aligned attention is\\nconstructed with organized deformable filters. Second, the three kinds of\\nattention are unified to construct the hybrid attention mechanism. We then\\nembed the hybrid attention into Retina-Net and propose the efficient\\nsingle-stage HAR-Net for object detection. The attention modules and the\\nproposed HAR-Net are evaluated on the COCO detection dataset. Experiments\\ndemonstrate that hybrid attention can significantly improve the detection\\naccuracy and the HAR-Net can achieve the state-of-the-art 45.8\\\\% mAP,\\noutperform existing single-stage object detectors.'},\n",
              "  {'title': 'CBAM: Convolutional Block Attention Module',\n",
              "   'authors': ['Sanghyun Woo',\n",
              "    'Jongchan Park',\n",
              "    'Joon-Young Lee',\n",
              "    'In-So Kweon'],\n",
              "   'abstract': 'We propose Convolutional Block Attention Module (CBAM), a simple yet effective attention module for feed-forward convolutional neural networks. Given an intermediate feature map, our module sequentially infers attention maps along two separate dimensions, channel and spatial, then the attention maps are multiplied to the input feature map for adaptive feature refinement. Because CBAM is a lightweight and general module, it can be integrated into any CNN architectures seamlessly with negligible overheads and is end-to-end trainable along with base CNNs. We validate our CBAM through extensive experiments on ImageNet-1K, MS COCO detection, and VOC 2007 detection datasets. Our experiments show consistent improvements in classification and detection performances with various models, demonstrating the wide applicability of CBAM. The code and models will be publicly available.',\n",
              "   'year': 2018,\n",
              "   'url': 'https://www.semanticscholar.org/paper/de95601d9e3b20ec51aa33e1f27b1880d2c44ef2',\n",
              "   'source': 'Semantic Scholar',\n",
              "   'citation_count': 16734,\n",
              "   'venue': 'European Conference on Computer Vision',\n",
              "   'publication_date': '2018-07-17',\n",
              "   'content': 'We propose Convolutional Block Attention Module (CBAM), a simple yet effective attention module for feed-forward convolutional neural networks. Given an intermediate feature map, our module sequentially infers attention maps along two separate dimensions, channel and spatial, then the attention maps are multiplied to the input feature map for adaptive feature refinement. Because CBAM is a lightweight and general module, it can be integrated into any CNN architectures seamlessly with negligible overheads and is end-to-end trainable along with base CNNs. We validate our CBAM through extensive experiments on ImageNet-1K, MS COCO detection, and VOC 2007 detection datasets. Our experiments show consistent improvements in classification and detection performances with various models, demonstrating the wide applicability of CBAM. The code and models will be publicly available.',\n",
              "   'external_ids': {'MAG': '2948744165',\n",
              "    'ArXiv': '1807.06521',\n",
              "    'DBLP': 'conf/eccv/WooPLK18',\n",
              "    'DOI': '10.1007/978-3-030-01234-2_1',\n",
              "    'CorpusId': 49867180}},\n",
              "  {'title': 'Training data-efficient image transformers & distillation through attention',\n",
              "   'authors': ['Hugo Touvron',\n",
              "    'M. Cord',\n",
              "    'Matthijs Douze',\n",
              "    'Francisco Massa',\n",
              "    'Alexandre Sablayrolles',\n",
              "    \"Herv'e J'egou\"],\n",
              "   'abstract': 'Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. However, these visual transformers are pre-trained with hundreds of millions of images using an expensive infrastructure, thereby limiting their adoption. In this work, we produce a competitive convolution-free transformer by training on Imagenet only. We train them on a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1% (single-crop evaluation) on ImageNet with no external data. More importantly, we introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention. We show the interest of this token-based distillation, especially when using a convnet as a teacher. This leads us to report results competitive with convnets for both Imagenet (where we obtain up to 85.2% accuracy) and when transferring to other tasks. We share our code and models.',\n",
              "   'year': 2020,\n",
              "   'url': 'https://www.semanticscholar.org/paper/ad7ddcc14984caae308c397f1a589aae75d4ab71',\n",
              "   'source': 'Semantic Scholar',\n",
              "   'citation_count': 6848,\n",
              "   'venue': 'International Conference on Machine Learning',\n",
              "   'publication_date': '2020-12-23',\n",
              "   'content': 'Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. However, these visual transformers are pre-trained with hundreds of millions of images using an expensive infrastructure, thereby limiting their adoption. In this work, we produce a competitive convolution-free transformer by training on Imagenet only. We train them on a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1% (single-crop evaluation) on ImageNet with no external data. More importantly, we introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention. We show the interest of this token-based distillation, especially when using a convnet as a teacher. This leads us to report results competitive with convnets for both Imagenet (where we obtain up to 85.2% accuracy) and when transferring to other tasks. We share our code and models.',\n",
              "   'external_ids': {'ArXiv': '2012.12877',\n",
              "    'DBLP': 'journals/corr/abs-2012-12877',\n",
              "    'CorpusId': 229363322}},\n",
              "  {'title': 'Image Super-Resolution Using Very Deep Residual Channel Attention Networks',\n",
              "   'authors': ['Yulun Zhang',\n",
              "    'Kunpeng Li',\n",
              "    'Kai Li',\n",
              "    'Lichen Wang',\n",
              "    'Bineng Zhong',\n",
              "    'Y. Fu'],\n",
              "   'abstract': 'Convolutional neural network (CNN) depth is of crucial importance for image super-resolution (SR). However, we observe that deeper networks for image SR are more difficult to train. The low-resolution inputs and features contain abundant low-frequency information, which is treated equally across channels, hence hindering the representational ability of CNNs. To solve these problems, we propose the very deep residual channel attention networks (RCAN). Specifically, we propose a residual in residual (RIR) structure to form very deep network, which consists of several residual groups with long skip connections. Each residual group contains some residual blocks with short skip connections. Meanwhile, RIR allows abundant low-frequency information to be bypassed through multiple skip connections, making the main network focus on learning high-frequency information. Furthermore, we propose a channel attention mechanism to adaptively rescale channel-wise features by considering interdependencies among channels. Extensive experiments show that our RCAN achieves better accuracy and visual improvements against state-of-the-art methods.',\n",
              "   'year': 2018,\n",
              "   'url': 'https://www.semanticscholar.org/paper/9775f8964a2eea1c9e35a02b1b906487396ea1f5',\n",
              "   'source': 'Semantic Scholar',\n",
              "   'citation_count': 4354,\n",
              "   'venue': 'European Conference on Computer Vision',\n",
              "   'publication_date': '2018-07-08',\n",
              "   'content': 'Convolutional neural network (CNN) depth is of crucial importance for image super-resolution (SR). However, we observe that deeper networks for image SR are more difficult to train. The low-resolution inputs and features contain abundant low-frequency information, which is treated equally across channels, hence hindering the representational ability of CNNs. To solve these problems, we propose the very deep residual channel attention networks (RCAN). Specifically, we propose a residual in residual (RIR) structure to form very deep network, which consists of several residual groups with long skip connections. Each residual group contains some residual blocks with short skip connections. Meanwhile, RIR allows abundant low-frequency information to be bypassed through multiple skip connections, making the main network focus on learning high-frequency information. Furthermore, we propose a channel attention mechanism to adaptively rescale channel-wise features by considering interdependencies among channels. Extensive experiments show that our RCAN achieves better accuracy and visual improvements against state-of-the-art methods.',\n",
              "   'external_ids': {'ArXiv': '1807.02758',\n",
              "    'MAG': '2866634454',\n",
              "    'DBLP': 'conf/eccv/ZhangLLWZF18',\n",
              "    'DOI': '10.1007/978-3-030-01234-2_18',\n",
              "    'CorpusId': 49657846}},\n",
              "  {'title': 'Attention U-Net: Learning Where to Look for the Pancreas',\n",
              "   'authors': ['O. Oktay',\n",
              "    'Jo Schlemper',\n",
              "    'L. L. Folgoc',\n",
              "    'M. J. Lee',\n",
              "    'M. Heinrich',\n",
              "    'K. Misawa',\n",
              "    'K. Mori',\n",
              "    'Steven G. McDonagh',\n",
              "    'Nils Y. Hammerla',\n",
              "    'Bernhard Kainz',\n",
              "    'Ben Glocker',\n",
              "    'D. Rueckert'],\n",
              "   'abstract': 'We propose a novel attention gate (AG) model for medical imaging that automatically learns to focus on target structures of varying shapes and sizes. Models trained with AGs implicitly learn to suppress irrelevant regions in an input image while highlighting salient features useful for a specific task. This enables us to eliminate the necessity of using explicit external tissue/organ localisation modules of cascaded convolutional neural networks (CNNs). AGs can be easily integrated into standard CNN architectures such as the U-Net model with minimal computational overhead while increasing the model sensitivity and prediction accuracy. The proposed Attention U-Net architecture is evaluated on two large CT abdominal datasets for multi-class image segmentation. Experimental results show that AGs consistently improve the prediction performance of U-Net across different datasets and training sizes while preserving computational efficiency. The code for the proposed architecture is publicly available.',\n",
              "   'year': 2018,\n",
              "   'url': 'https://www.semanticscholar.org/paper/ae1c89817a3a239e5344293138bdd80293983460',\n",
              "   'source': 'Semantic Scholar',\n",
              "   'citation_count': 5107,\n",
              "   'venue': 'arXiv.org',\n",
              "   'publication_date': '2018-04-11',\n",
              "   'content': 'We propose a novel attention gate (AG) model for medical imaging that automatically learns to focus on target structures of varying shapes and sizes. Models trained with AGs implicitly learn to suppress irrelevant regions in an input image while highlighting salient features useful for a specific task. This enables us to eliminate the necessity of using explicit external tissue/organ localisation modules of cascaded convolutional neural networks (CNNs). AGs can be easily integrated into standard CNN architectures such as the U-Net model with minimal computational overhead while increasing the model sensitivity and prediction accuracy. The proposed Attention U-Net architecture is evaluated on two large CT abdominal datasets for multi-class image segmentation. Experimental results show that AGs consistently improve the prediction performance of U-Net across different datasets and training sizes while preserving computational efficiency. The code for the proposed architecture is publicly available.',\n",
              "   'external_ids': {'MAG': '2798122215',\n",
              "    'DBLP': 'journals/corr/abs-1804-03999',\n",
              "    'ArXiv': '1804.03999',\n",
              "    'CorpusId': 4861068}},\n",
              "  {'title': 'Attention is All you Need',\n",
              "   'authors': ['Ashish Vaswani',\n",
              "    'Noam M. Shazeer',\n",
              "    'Niki Parmar',\n",
              "    'Jakob Uszkoreit',\n",
              "    'Llion Jones',\n",
              "    'Aidan N. Gomez',\n",
              "    'Lukasz Kaiser',\n",
              "    'I. Polosukhin'],\n",
              "   'abstract': 'The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.',\n",
              "   'year': 2017,\n",
              "   'url': 'https://www.semanticscholar.org/paper/204e3073870fae3d05bcbc2f6a8e263d9b72e776',\n",
              "   'source': 'Semantic Scholar',\n",
              "   'citation_count': 133288,\n",
              "   'venue': 'Neural Information Processing Systems',\n",
              "   'publication_date': '2017-06-12',\n",
              "   'content': 'The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.',\n",
              "   'external_ids': {'MAG': '2963403868',\n",
              "    'DBLP': 'conf/nips/VaswaniSPUJGKP17',\n",
              "    'ArXiv': '1706.03762',\n",
              "    'CorpusId': 13756489}},\n",
              "  {'title': 'Graph Attention Networks',\n",
              "   'authors': ['Petar Velickovic',\n",
              "    'Guillem Cucurull',\n",
              "    'Arantxa Casanova',\n",
              "    'Adriana Romero',\n",
              "    'P. Lio‚Äô',\n",
              "    'Yoshua Bengio'],\n",
              "   'abstract': \"We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training).\",\n",
              "   'year': 2017,\n",
              "   'url': 'https://www.semanticscholar.org/paper/33998aff64ce51df8dee45989cdca4b6b1329ec4',\n",
              "   'source': 'Semantic Scholar',\n",
              "   'citation_count': 20357,\n",
              "   'venue': 'International Conference on Learning Representations',\n",
              "   'publication_date': '2017-10-30',\n",
              "   'content': \"We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training).\",\n",
              "   'external_ids': {'DBLP': 'journals/corr/abs-1710-10903',\n",
              "    'ArXiv': '1710.10903',\n",
              "    'MAG': '2766453196',\n",
              "    'DOI': '10.17863/CAM.48429',\n",
              "    'CorpusId': 3292002}},\n",
              "  {'title': 'Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering',\n",
              "   'authors': ['Peter Anderson',\n",
              "    'Xiaodong He',\n",
              "    'Chris Buehler',\n",
              "    'Damien Teney',\n",
              "    'Mark Johnson',\n",
              "    'Stephen Gould',\n",
              "    'Lei Zhang'],\n",
              "   'abstract': 'Top-down visual attention mechanisms have been used extensively in image captioning and visual question answering (VQA) to enable deeper image understanding through fine-grained analysis and even multiple steps of reasoning. In this work, we propose a combined bottom-up and top-down attention mechanism that enables attention to be calculated at the level of objects and other salient image regions. This is the natural basis for attention to be considered. Within our approach, the bottom-up mechanism (based on Faster R-CNN) proposes image regions, each with an associated feature vector, while the top-down mechanism determines feature weightings. Applying this approach to image captioning, our results on the MSCOCO test server establish a new state-of-the-art for the task, achieving CIDEr / SPICE / BLEU-4 scores of 117.9, 21.5 and 36.9, respectively. Demonstrating the broad applicability of the method, applying the same approach to VQA we obtain first place in the 2017 VQA Challenge.',\n",
              "   'year': 2017,\n",
              "   'url': 'https://www.semanticscholar.org/paper/a82c1d1ccaa3a3d1d6ee6677de0eed2e93ddb6e8',\n",
              "   'source': 'Semantic Scholar',\n",
              "   'citation_count': 4231,\n",
              "   'venue': '2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition',\n",
              "   'publication_date': '2017-07-25',\n",
              "   'content': 'Top-down visual attention mechanisms have been used extensively in image captioning and visual question answering (VQA) to enable deeper image understanding through fine-grained analysis and even multiple steps of reasoning. In this work, we propose a combined bottom-up and top-down attention mechanism that enables attention to be calculated at the level of objects and other salient image regions. This is the natural basis for attention to be considered. Within our approach, the bottom-up mechanism (based on Faster R-CNN) proposes image regions, each with an associated feature vector, while the top-down mechanism determines feature weightings. Applying this approach to image captioning, our results on the MSCOCO test server establish a new state-of-the-art for the task, achieving CIDEr / SPICE / BLEU-4 scores of 117.9, 21.5 and 36.9, respectively. Demonstrating the broad applicability of the method, applying the same approach to VQA we obtain first place in the 2017 VQA Challenge.',\n",
              "   'external_ids': {'MAG': '2745461083',\n",
              "    'DBLP': 'conf/cvpr/00010BT0GZ18',\n",
              "    'ArXiv': '1707.07998',\n",
              "    'DOI': '10.1109/CVPR.2018.00636',\n",
              "    'CorpusId': 3753452}},\n",
              "  {'title': 'ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks',\n",
              "   'authors': ['Qilong Wang',\n",
              "    'Banggu Wu',\n",
              "    'Peng Fei Zhu',\n",
              "    'P. Li',\n",
              "    'W. Zuo',\n",
              "    'Q. Hu'],\n",
              "   'abstract': 'Recently, channel attention mechanism has demonstrated to offer great potential in improving the performance of deep convolutional neural networks (CNNs). However, most existing methods dedicate to developing more sophisticated attention modules for achieving better performance, which inevitably increase model complexity. To overcome the paradox of performance and complexity trade-off, this paper proposes an Efficient Channel Attention (ECA) module, which only involves a handful of parameters while bringing clear performance gain. By dissecting the channel attention module in SENet, we empirically show avoiding dimensionality reduction is important for learning channel attention, and appropriate cross-channel interaction can preserve performance while significantly decreasing model complexity. Therefore, we propose a local cross-channel interaction strategy without dimensionality reduction, which can be efficiently implemented via 1D convolution. Furthermore, we develop a method to adaptively select kernel size of 1D convolution, determining coverage of local cross-channel interaction. The proposed ECA module is both efficient and effective, e.g., the parameters and computations of our modules against backbone of ResNet50 are 80 vs. 24.37M and 4.7e-4 GFlops vs. 3.86 GFlops, respectively, and the performance boost is more than 2% in terms of Top-1 accuracy. We extensively evaluate our ECA module on image classification, object detection and instance segmentation with backbones of ResNets and MobileNetV2. The experimental results show our module is more efficient while performing favorably against its counterparts.',\n",
              "   'year': 2019,\n",
              "   'url': 'https://www.semanticscholar.org/paper/8cb34cbdcf65c23ef98430441b14a648c4e8d992',\n",
              "   'source': 'Semantic Scholar',\n",
              "   'citation_count': 4085,\n",
              "   'venue': 'Computer Vision and Pattern Recognition',\n",
              "   'publication_date': '2019-10-08',\n",
              "   'content': 'Recently, channel attention mechanism has demonstrated to offer great potential in improving the performance of deep convolutional neural networks (CNNs). However, most existing methods dedicate to developing more sophisticated attention modules for achieving better performance, which inevitably increase model complexity. To overcome the paradox of performance and complexity trade-off, this paper proposes an Efficient Channel Attention (ECA) module, which only involves a handful of parameters while bringing clear performance gain. By dissecting the channel attention module in SENet, we empirically show avoiding dimensionality reduction is important for learning channel attention, and appropriate cross-channel interaction can preserve performance while significantly decreasing model complexity. Therefore, we propose a local cross-channel interaction strategy without dimensionality reduction, which can be efficiently implemented via 1D convolution. Furthermore, we develop a method to adaptively select kernel size of 1D convolution, determining coverage of local cross-channel interaction. The proposed ECA module is both efficient and effective, e.g., the parameters and computations of our modules against backbone of ResNet50 are 80 vs. 24.37M and 4.7e-4 GFlops vs. 3.86 GFlops, respectively, and the performance boost is more than 2% in terms of Top-1 accuracy. We extensively evaluate our ECA module on image classification, object detection and instance segmentation with backbones of ResNets and MobileNetV2. The experimental results show our module is more efficient while performing favorably against its counterparts.',\n",
              "   'external_ids': {'ArXiv': '1910.03151',\n",
              "    'MAG': '2979983945',\n",
              "    'DBLP': 'journals/corr/abs-1910-03151',\n",
              "    'DOI': '10.1109/CVPR42600.2020.01155',\n",
              "    'CorpusId': 203902337}},\n",
              "  {'title': 'Attention deficit hyperactivity disorder.',\n",
              "   'authors': ['Muhammed Ather', 'G. Salmon'],\n",
              "   'abstract': 'Attention deficit hyperactivity disorder is a highly heritable medical condition which mainly affects school-aged children although it is increasingly being recognized in adults. The exact cause remains unknown, but the condition responds well to evidence-based interventions.',\n",
              "   'year': 2018,\n",
              "   'url': 'https://www.semanticscholar.org/paper/961722bc29963f3b11600d0d10c233c4a11b4573',\n",
              "   'source': 'Semantic Scholar',\n",
              "   'citation_count': 4449,\n",
              "   'venue': 'British journal of hospital medicine',\n",
              "   'publication_date': '2018-05-01',\n",
              "   'content': 'Attention deficit hyperactivity disorder is a highly heritable medical condition which mainly affects school-aged children although it is increasingly being recognized in adults. The exact cause remains unknown, but the condition responds well to evidence-based interventions.',\n",
              "   'external_ids': {'DOI': '10.12968/hmed.2010.71.11.79663',\n",
              "    'CorpusId': 807553,\n",
              "    'PubMed': '21063258'}},\n",
              "  {'title': 'Dual Attention Network for Scene Segmentation',\n",
              "   'authors': ['J. Fu', 'J. Liu', 'Haijie Tian', 'Zhiwei Fang', 'Hanqing Lu'],\n",
              "   'abstract': 'In this paper, we address the scene segmentation task by capturing rich contextual dependencies based on the self-attention mechanism. Unlike previous works that capture contexts by multi-scale features fusion, we propose a Dual Attention Networks (DANet) to adaptively integrate local features with their global dependencies. Specifically, we append two types of attention modules on top of traditional dilated FCN, which model the semantic interdependencies in spatial and channel dimensions respectively. The position attention module selectively aggregates the features at each position by a weighted sum of the features at all positions. Similar features would be related to each other regardless of their distances. Meanwhile, the channel attention module selectively emphasizes interdependent channel maps by integrating associated features among all channel maps. We sum the outputs of the two attention modules to further improve feature representation which contributes to more precise segmentation results. We achieve new state-of-the-art segmentation performance on three challenging scene segmentation datasets, i.e., Cityscapes, PASCAL Context and COCO Stuff dataset. In particular, a Mean IoU score of 81.5% on Cityscapes test set is achieved without using coarse data.',\n",
              "   'year': 2018,\n",
              "   'url': 'https://www.semanticscholar.org/paper/ad655c25e052fa4eeed53421344aca6f239c4c9d',\n",
              "   'source': 'Semantic Scholar',\n",
              "   'citation_count': 5126,\n",
              "   'venue': 'Computer Vision and Pattern Recognition',\n",
              "   'publication_date': '2018-09-09',\n",
              "   'content': 'In this paper, we address the scene segmentation task by capturing rich contextual dependencies based on the self-attention mechanism. Unlike previous works that capture contexts by multi-scale features fusion, we propose a Dual Attention Networks (DANet) to adaptively integrate local features with their global dependencies. Specifically, we append two types of attention modules on top of traditional dilated FCN, which model the semantic interdependencies in spatial and channel dimensions respectively. The position attention module selectively aggregates the features at each position by a weighted sum of the features at all positions. Similar features would be related to each other regardless of their distances. Meanwhile, the channel attention module selectively emphasizes interdependent channel maps by integrating associated features among all channel maps. We sum the outputs of the two attention modules to further improve feature representation which contributes to more precise segmentation results. We achieve new state-of-the-art segmentation performance on three challenging scene segmentation datasets, i.e., Cityscapes, PASCAL Context and COCO Stuff dataset. In particular, a Mean IoU score of 81.5% on Cityscapes test set is achieved without using coarse data.',\n",
              "   'external_ids': {'MAG': '2892219791',\n",
              "    'DBLP': 'journals/corr/abs-1809-02983',\n",
              "    'ArXiv': '1809.02983',\n",
              "    'DOI': '10.1109/CVPR.2019.00326',\n",
              "    'CorpusId': 52180375}},\n",
              "  {'title': 'Figure 5: Comparison of attention modules; (A) SE attention; (B) CBAM attention; (C) Position information attention.',\n",
              "   'authors': [],\n",
              "   'abstract': '',\n",
              "   'year': None,\n",
              "   'url': 'https://doi.org/10.7717/peerj-cs.2826/fig-5',\n",
              "   'doi': '10.7717/peerj-cs.2826/fig-5',\n",
              "   'publisher': 'PeerJ',\n",
              "   'journal': '',\n",
              "   'source': 'CrossRef',\n",
              "   'content': ''},\n",
              "  {'title': 'Consciousness‚ÄìAttention Dissociation and the Evolution of Conscious Attention',\n",
              "   'authors': [],\n",
              "   'abstract': '',\n",
              "   'year': 2015,\n",
              "   'url': 'https://doi.org/10.7551/mitpress/10283.003.0007',\n",
              "   'doi': '10.7551/mitpress/10283.003.0007',\n",
              "   'publisher': 'The MIT Press',\n",
              "   'journal': 'Consciousness, Attention, and Conscious Attention',\n",
              "   'source': 'CrossRef',\n",
              "   'content': ''},\n",
              "  {'title': 'Conscious Attention',\n",
              "   'authors': [],\n",
              "   'abstract': '',\n",
              "   'year': 2015,\n",
              "   'url': 'https://doi.org/10.7551/mitpress/10283.003.0006',\n",
              "   'doi': '10.7551/mitpress/10283.003.0006',\n",
              "   'publisher': 'The MIT Press',\n",
              "   'journal': 'Consciousness, Attention, and Conscious Attention',\n",
              "   'source': 'CrossRef',\n",
              "   'content': ''},\n",
              "  {'title': 'Forms of Attention',\n",
              "   'authors': [],\n",
              "   'abstract': '',\n",
              "   'year': 2015,\n",
              "   'url': 'https://doi.org/10.7551/mitpress/10283.003.0004',\n",
              "   'doi': '10.7551/mitpress/10283.003.0004',\n",
              "   'publisher': 'The MIT Press',\n",
              "   'journal': 'Consciousness, Attention, and Conscious Attention',\n",
              "   'source': 'CrossRef',\n",
              "   'content': ''},\n",
              "  {'title': 'Figure 3: The GAM attention mechanism includes two submodules: channel attention and spatial attention.',\n",
              "   'authors': [],\n",
              "   'abstract': '',\n",
              "   'year': None,\n",
              "   'url': 'https://doi.org/10.7717/peerjcs.2224/fig-3',\n",
              "   'doi': '10.7717/peerjcs.2224/fig-3',\n",
              "   'publisher': 'PeerJ',\n",
              "   'journal': '',\n",
              "   'source': 'CrossRef',\n",
              "   'content': ''},\n",
              "  {'title': 'Pay Strict Attention',\n",
              "   'authors': ['David Martin-Jones'],\n",
              "   'abstract': '<p>This chapter firstly outlines two complementary dimensions through which <italic>Columbo</italic> negotiates the shaping of attention: textual, and, socio-historical. Initially, how television attracts attention is outlined, enabling an exploration of how the unique narrative structure of <italic>Columbo</italic> (which functions like a game) works to attract and shape viewer attention. Theories of attention, especially those which focus on it being historically determined, are introduced - including the idea of the attention economy, indicating the continued relevance of how attention is increasingly monetized in the late Twentieth/early Twenty-first Century.\\nSecondly, a brief analysis of the episodes ‚ÄòIdentity Crisis‚Äô and ‚ÄòColumbo Cries Wolf‚Äô illuminate <italic>Columbo</italic>‚Äòs engagement with how attention is channelled societally during these decades, whilst neoliberalism emerged to prominence from the ashes of the Cold War. What Columbo does ‚Äì paying attention in order to solve a crime ‚Äì does not change. How he performs, learns, polices and locates himself, are equally evident across both episodes. The ingredients for being a good attentive labourer were thus already evident in the early 1970s as the world turned towards neoliberalism. What changes is the backdrop, revealing the historical transformation in how (as in, <italic>to what ends</italic>) attention was channelled during the end of the Twentieth Century.</p>',\n",
              "   'year': 2021,\n",
              "   'url': 'https://doi.org/10.3366/edinburgh/9781474479790.003.0003',\n",
              "   'doi': '10.3366/edinburgh/9781474479790.003.0003',\n",
              "   'publisher': 'Edinburgh University Press',\n",
              "   'journal': 'Columbo',\n",
              "   'source': 'CrossRef',\n",
              "   'content': '<p>This chapter firstly outlines two complementary dimensions through which <italic>Columbo</italic> negotiates the shaping of attention: textual, and, socio-historical. Initially, how television attracts attention is outlined, enabling an exploration of how the unique narrative structure of <italic>Columbo</italic> (which functions like a game) works to attract and shape viewer attention. Theories of attention, especially those which focus on it being historically determined, are introduced - including the idea of the attention economy, indicating the continued relevance of how attention is increasingly monetized in the late Twentieth/early Twenty-first Century.\\nSecondly, a brief analysis of the episodes ‚ÄòIdentity Crisis‚Äô and ‚ÄòColumbo Cries Wolf‚Äô illuminate <italic>Columbo</italic>‚Äòs engagement with how attention is channelled societally during these decades, whilst neoliberalism emerged to prominence from the ashes of the Cold War. What Columbo does ‚Äì paying attention in order to solve a crime ‚Äì does not change. How he performs, learns, polices and locates himself, are equally evident across both episodes. The ingredients for being a good attentive labourer were thus already evident in the early 1970s as the world turned towards neoliberalism. What changes is the backdrop, revealing the historical transformation in how (as in, <italic>to what ends</italic>) attention was channelled during the end of the Twentieth Century.</p>'},\n",
              "  {'title': 'Neurotransmitters in Attention Deficit Disorder',\n",
              "   'authors': [],\n",
              "   'abstract': '',\n",
              "   'year': 2013,\n",
              "   'url': 'https://doi.org/10.4324/9781315827643-19',\n",
              "   'doi': '10.4324/9781315827643-19',\n",
              "   'publisher': 'Routledge',\n",
              "   'journal': 'Attention Deficit Disord Pod',\n",
              "   'source': 'CrossRef',\n",
              "   'content': ''},\n",
              "  {'title': 'ssEM Image Restoration via Diffusion Models with Multi-output Joint Strategy for Noise Estimation.',\n",
              "   'authors': ['Hongyu Yang',\n",
              "    'Ao Cheng',\n",
              "    'Jiahao Shi',\n",
              "    'Guoqiang Zhao',\n",
              "    'Jiachi Chen',\n",
              "    'Deshuang Huang',\n",
              "    'Ruobing Zhang'],\n",
              "   'abstract': 'Serial section electron microscopy (ssEM) is a pivotal technique for investigating neuronal connections and brain microstructures. However, imperfect sample preparation and image acquisition often lead to degradation, posing challenges for subsequent analysis. While previous deep learning methods, such as the interpolation model using spatially adaptive convolutions, have been proven to outperform conventional approaches, they struggle to recover high-frequency details, resulting in poor perceptual quality and segmentation performance. This study presents a novel approach leveraging diffusion models to restore missing slices of ssEM images. To accommodate the anisotropic characteristic of ssEM images, we enhance the backbone network with asymmetric and symmetric 3D convolutions. Additionally, we propose the Adaptive and Learnable Reconstruction (ALR) module with the First and Last slices Attention Block (FLAB) for effective feature extraction. A Multi-output Joint Strategy (MJS) is utilized for noise estimation, reducing training-testing discrepancies and achieving diffusion correction. Moreover, we also redesign the inference process to optimize the restoration of partially damaged slices, enabling restoration without additional artifact simulation or retraining. Experiment results demonstrate the effectiveness of our approach in generating more realistic slices and its superior performance in downstream tasks, surpassing previous methods.',\n",
              "   'year': 2025,\n",
              "   'url': 'https://pubmed.ncbi.nlm.nih.gov/40627490/',\n",
              "   'pmid': '40627490',\n",
              "   'source': 'PubMed',\n",
              "   'content': 'Serial section electron microscopy (ssEM) is a pivotal technique for investigating neuronal connections and brain microstructures. However, imperfect sample preparation and image acquisition often lead to degradation, posing challenges for subsequent analysis. While previous deep learning methods, such as the interpolation model using spatially adaptive convolutions, have been proven to outperform conventional approaches, they struggle to recover high-frequency details, resulting in poor perceptual quality and segmentation performance. This study presents a novel approach leveraging diffusion models to restore missing slices of ssEM images. To accommodate the anisotropic characteristic of ssEM images, we enhance the backbone network with asymmetric and symmetric 3D convolutions. Additionally, we propose the Adaptive and Learnable Reconstruction (ALR) module with the First and Last slices Attention Block (FLAB) for effective feature extraction. A Multi-output Joint Strategy (MJS) is utilized for noise estimation, reducing training-testing discrepancies and achieving diffusion correction. Moreover, we also redesign the inference process to optimize the restoration of partially damaged slices, enabling restoration without additional artifact simulation or retraining. Experiment results demonstrate the effectiveness of our approach in generating more realistic slices and its superior performance in downstream tasks, surpassing previous methods.'},\n",
              "  {'title': 'Fed-HeLLo: Efficient Federated Foundation Model Fine-Tuning With Heterogeneous LoRA Allocation.',\n",
              "   'authors': ['Zikai Zhang', 'Ping Liu', 'Jiahao Xu', 'Rui Hu'],\n",
              "   'abstract': \"Federated learning (FL) has recently been used to collaboratively fine-tune foundation models (FMs) across multiple clients. Notably, federated low-rank adaptation (LoRA)-based fine-tuning methods have recently gained attention, which allows clients to fine-tune FMs with a small portion of trainable parameters locally. However, most existing methods do not account for the heterogeneous resources of clients or lack an effective local training strategy to maximize global fine-tuning performance under limited resources. In this work, we propose federated LoRA-based fine-tuning framework with heterogeneous LoRA allocation (Fed-HeLLo), a novel federated LoRA-based fine-tuning framework that enables clients to collaboratively fine-tune an FM with different local trainable LoRA layers. To ensure its effectiveness, we develop several heterogeneous LoRA allocation (HLA) strategies that adaptively allocate local trainable LoRA layers based on clients' resource capabilities and the layer importance. Specifically, based on the dynamic layer importance, we design a Fisher information matrix score-based HLA (FIM-HLA) that leverages dynamic gradient norm information. To better stabilize the training process, we consider the intrinsic importance of LoRA layers and design a geometrically defined HLA (GD-HLA) strategy. It shapes the collective distribution of trainable LoRA layers into specific geometric patterns, such as triangle, inverted triangle, bottleneck, and uniform. Moreover, we extend GD-HLA into a randomized version, named randomized GD-HLA (RGD-HLA), for enhanced model accuracy with randomness. By codesigning the proposed HLA strategies, we incorporate both the dynamic and intrinsic layer importance into the design of our HLA strategy. To thoroughly evaluate our approach, we simulate various complex federated LoRA-based fine-tuning settings using five datasets and three levels of data distributions ranging from independent identically distributed (i.i.d.) to extreme non-i.i.d. The experimental results demonstrate the effectiveness and efficiency of Fed-HeLLo with the proposed HLA strategies. The code is available at https://github.com/ TNI-playground/Fed_HeLLo.\",\n",
              "   'year': 2025,\n",
              "   'url': 'https://pubmed.ncbi.nlm.nih.gov/40627488/',\n",
              "   'pmid': '40627488',\n",
              "   'source': 'PubMed',\n",
              "   'content': \"Federated learning (FL) has recently been used to collaboratively fine-tune foundation models (FMs) across multiple clients. Notably, federated low-rank adaptation (LoRA)-based fine-tuning methods have recently gained attention, which allows clients to fine-tune FMs with a small portion of trainable parameters locally. However, most existing methods do not account for the heterogeneous resources of clients or lack an effective local training strategy to maximize global fine-tuning performance under limited resources. In this work, we propose federated LoRA-based fine-tuning framework with heterogeneous LoRA allocation (Fed-HeLLo), a novel federated LoRA-based fine-tuning framework that enables clients to collaboratively fine-tune an FM with different local trainable LoRA layers. To ensure its effectiveness, we develop several heterogeneous LoRA allocation (HLA) strategies that adaptively allocate local trainable LoRA layers based on clients' resource capabilities and the layer importance. Specifically, based on the dynamic layer importance, we design a Fisher information matrix score-based HLA (FIM-HLA) that leverages dynamic gradient norm information. To better stabilize the training process, we consider the intrinsic importance of LoRA layers and design a geometrically defined HLA (GD-HLA) strategy. It shapes the collective distribution of trainable LoRA layers into specific geometric patterns, such as triangle, inverted triangle, bottleneck, and uniform. Moreover, we extend GD-HLA into a randomized version, named randomized GD-HLA (RGD-HLA), for enhanced model accuracy with randomness. By codesigning the proposed HLA strategies, we incorporate both the dynamic and intrinsic layer importance into the design of our HLA strategy. To thoroughly evaluate our approach, we simulate various complex federated LoRA-based fine-tuning settings using five datasets and three levels of data distributions ranging from independent identically distributed (i.i.d.) to extreme non-i.i.d. The experimental results demonstrate the effectiveness and efficiency of Fed-HeLLo with the proposed HLA strategies. The code is available at https://github.com/ TNI-playground/Fed_HeLLo.\"},\n",
              "  {'title': 'Assessment and management of portal hypertension in patients with MASLD: advances and caveats.',\n",
              "   'authors': ['Diego Rojo',\n",
              "    'Alba Jim√©nez-Masip',\n",
              "    'Laura Pag√®s',\n",
              "    'Juan Ba√±ares',\n",
              "    'Clara Sabiote',\n",
              "    'Mar√≠a Mart√≠nez-G√≥mez',\n",
              "    'Laia Aceituno',\n",
              "    'M Serra Cusid√≥',\n",
              "    'M Teresa Salcedo-Allende',\n",
              "    'Zyanya Calixto',\n",
              "    'M√≤nica Pons',\n",
              "    'Joan Genesc√†',\n",
              "    'Juan M Peric√†s'],\n",
              "   'abstract': 'Metabolic dysfunction-associated steatotic liver disease (MASLD) is increasingly gaining relevance both as a public health issue and a clinical challenge. The development of portal hypertension in MASLD challenges traditional paradigm that it arises only in the context of cirrhosis. This review explores the understanding of portal hypertension in MASLD, addressing recent advances in pathophysiology, diagnosis, and potential therapeutic approaches. Particular attention is given to studies that highlight non-cirrhotic contributors to increased portal pressure. Pathogenic mechanisms, as well as advances in noninvasive diagnostic tools are discussed, focusing on their utility in identifying early hemodynamic changes and stratifying the risk of clinical complications. A comprehensive literature search was conducted using Pubmed and major databases, including studies published up to April 2025. The future of MASLD management lies in early detection, improved noninvasive risk stratification, and personalized treatment strategies. Advances in technology and artificial intelligence will likely enhance diagnostic precision while ongoing research into molecular mechanisms and portal hypertension may enable earlier and effective therapeutic interventions.',\n",
              "   'year': 2025,\n",
              "   'url': 'https://pubmed.ncbi.nlm.nih.gov/40627432/',\n",
              "   'pmid': '40627432',\n",
              "   'source': 'PubMed',\n",
              "   'content': 'Metabolic dysfunction-associated steatotic liver disease (MASLD) is increasingly gaining relevance both as a public health issue and a clinical challenge. The development of portal hypertension in MASLD challenges traditional paradigm that it arises only in the context of cirrhosis. This review explores the understanding of portal hypertension in MASLD, addressing recent advances in pathophysiology, diagnosis, and potential therapeutic approaches. Particular attention is given to studies that highlight non-cirrhotic contributors to increased portal pressure. Pathogenic mechanisms, as well as advances in noninvasive diagnostic tools are discussed, focusing on their utility in identifying early hemodynamic changes and stratifying the risk of clinical complications. A comprehensive literature search was conducted using Pubmed and major databases, including studies published up to April 2025. The future of MASLD management lies in early detection, improved noninvasive risk stratification, and personalized treatment strategies. Advances in technology and artificial intelligence will likely enhance diagnostic precision while ongoing research into molecular mechanisms and portal hypertension may enable earlier and effective therapeutic interventions.'},\n",
              "  {'title': 'Reasons to avoid using weasel words in ethology: establishing a more technical terminology may facilitate synthesis in the behavioural sciences.',\n",
              "   'authors': ['Roszik S√°ra', 'Mikl√≥si √Åd√°m'],\n",
              "   'abstract': \"The term 'weasel word' is used for vague, ambiguous, or misleading language when the speaker creates an impression of meaning or certainty without committing to specific facts. In academic writings, clear presentation of the declarant's thoughts is inevitable for the transmission and/or sharing of knowledge with high fidelity; hence, weasel words and phrases should be avoided. So far, the topic has received little attention in the behavioural sciences, including ethology, although research in fields focusing on behaviour and the mind provide fertile ground where such rhetorical strategies may go unnoticed. By presenting examples of weasel words used in ethology, we aim to demonstrate how they can lead to misunderstandings among scientists, as well as to prolonged and unproductive debates. We argue that the use of weasel words may stem equally from ingrained bad habits, knowledge gaps, anthropomorphism and inconsistent jargon. Instead of referring to mental states, the description of the behaviours should be in focus from a functional point of view. This allows more precise behaviour measurement and greater agreement in the use of terminology. Adopting a more descriptive and precise terminology may facilitate interdisciplinary discussions and lay the groundwork for novel approaches, such as synthetic or computational ethology.\",\n",
              "   'year': 2025,\n",
              "   'url': 'https://pubmed.ncbi.nlm.nih.gov/40627308/',\n",
              "   'pmid': '40627308',\n",
              "   'source': 'PubMed',\n",
              "   'content': \"The term 'weasel word' is used for vague, ambiguous, or misleading language when the speaker creates an impression of meaning or certainty without committing to specific facts. In academic writings, clear presentation of the declarant's thoughts is inevitable for the transmission and/or sharing of knowledge with high fidelity; hence, weasel words and phrases should be avoided. So far, the topic has received little attention in the behavioural sciences, including ethology, although research in fields focusing on behaviour and the mind provide fertile ground where such rhetorical strategies may go unnoticed. By presenting examples of weasel words used in ethology, we aim to demonstrate how they can lead to misunderstandings among scientists, as well as to prolonged and unproductive debates. We argue that the use of weasel words may stem equally from ingrained bad habits, knowledge gaps, anthropomorphism and inconsistent jargon. Instead of referring to mental states, the description of the behaviours should be in focus from a functional point of view. This allows more precise behaviour measurement and greater agreement in the use of terminology. Adopting a more descriptive and precise terminology may facilitate interdisciplinary discussions and lay the groundwork for novel approaches, such as synthetic or computational ethology.\"},\n",
              "  {'title': 'What are the Main Diagnoses in Hospitalized Patients in Madagascar ? A Sentinel Surveillance in 18 Hospitals from 2014 to 2018.',\n",
              "   'authors': ['Laurence Randrianasolo',\n",
              "    'Tojonirina Rabehasimbola',\n",
              "    'L√©a Randriamampionona',\n",
              "    'Mireille Randria',\n",
              "    'Toky Ramarokoto',\n",
              "    'Barivola Bernardson',\n",
              "    'Feno Manitra Rakotoarimanana',\n",
              "    'Maherisoa Ratsitorahina',\n",
              "    'Roland Razanatsimba Andriamasoandro',\n",
              "    'Hasina Alain Randimbitsialonina',\n",
              "    'Hajalalaina Razafindrazaka',\n",
              "    'Jery Soa Bakolitiana Rafaliarisoa',\n",
              "    'Nirinarilala Ramanantoanina',\n",
              "    'Haja Randrianary',\n",
              "    'Solofinirina Rakotonimanana',\n",
              "    'Gis√®le Ranarijaona',\n",
              "    'Giovanie Djaosany',\n",
              "    'Ramananarivo Rasoamirantsoa',\n",
              "    'Jacqueline Rasolofoharizanany',\n",
              "    'Iharisoa Ravaonandrasana',\n",
              "    'Delbert Radama Andriamanjava',\n",
              "    'Liva Fanambinantsoa',\n",
              "    'Victor Ralijaona',\n",
              "    'Prosper Randrianasolo',\n",
              "    'Jaona Ralaivao',\n",
              "    'Noelson Rakotovao',\n",
              "    'Nyelsen Ama√Øde Tsikomia',\n",
              "    'Antoine Olivier Randrianantenaina',\n",
              "    'Jean Erick Botoihely',\n",
              "    'St√©phan Pierre',\n",
              "    'Aim√© Bruno Zafilahy Totohako',\n",
              "    'Claude Marcel Andrianantenaina',\n",
              "    'Patrice Piola',\n",
              "    'Laurence Baril',\n",
              "    'Anou Dreyfus',\n",
              "    'Rindra Randremanana'],\n",
              "   'abstract': 'In Madagascar, a sentinel surveillance system was set up in 18 hospitals since 2014 and was managed by the Ministry of Public Health and the Institut Pasteur de Madagascar. In order to improve the access to appropriate health care in Madagascar, the main clinical diagnoses in hospitalized patients were analyzed. At hospitalization of a patient, each unit involved in the sentinel surveillance recorded the clinical diagnosis through an e-health platform. Data from September 2014 to July 2018 were analyzed. Morbidity and annual incidence of diseases according to ICD-10 were reported. A total of 140,789 inpatients information was recorded. The median age was 28.2\\xa0years (IQR: 18.3; 45.3). 21.6% of the children\\u2009<\\u200915\\xa0years suffered from communicable diseases. The hospital morbidity was 4.01% for malaria, 0.84% for tuberculosis, 0.09% for HIV/AIDS and 0.05% for plague. The hospital morbidity of non-communicable diseases was higher compared to communicable diseases with 7.8%, 7.1% and 3.1% for\\xa0\"Diseases of the circulatory system\", \"Injury, poisoning and certain other consequences of external causes\" and \"Mental and behavioural disorders\", respectively. \"Pregnancy, childbirth and puerperium\" represented 20.5% of the hospitalized patients. The e-health platform enabled continuous and standardized data collection. Communicable diseases affect mainly children\\u2009<\\u200915\\xa0years. Non-communicable diseases are on the rise and need more attention by national health authorities. A number of hospitalizations could be prevented by a better health care management at the community-based health care level and by implementation of a Universal Health Coverage (UHC) in Madagascar.',\n",
              "   'year': 2025,\n",
              "   'url': 'https://pubmed.ncbi.nlm.nih.gov/40627272/',\n",
              "   'pmid': '40627272',\n",
              "   'source': 'PubMed',\n",
              "   'content': 'In Madagascar, a sentinel surveillance system was set up in 18 hospitals since 2014 and was managed by the Ministry of Public Health and the Institut Pasteur de Madagascar. In order to improve the access to appropriate health care in Madagascar, the main clinical diagnoses in hospitalized patients were analyzed. At hospitalization of a patient, each unit involved in the sentinel surveillance recorded the clinical diagnosis through an e-health platform. Data from September 2014 to July 2018 were analyzed. Morbidity and annual incidence of diseases according to ICD-10 were reported. A total of 140,789 inpatients information was recorded. The median age was 28.2\\xa0years (IQR: 18.3; 45.3). 21.6% of the children\\u2009<\\u200915\\xa0years suffered from communicable diseases. The hospital morbidity was 4.01% for malaria, 0.84% for tuberculosis, 0.09% for HIV/AIDS and 0.05% for plague. The hospital morbidity of non-communicable diseases was higher compared to communicable diseases with 7.8%, 7.1% and 3.1% for\\xa0\"Diseases of the circulatory system\", \"Injury, poisoning and certain other consequences of external causes\" and \"Mental and behavioural disorders\", respectively. \"Pregnancy, childbirth and puerperium\" represented 20.5% of the hospitalized patients. The e-health platform enabled continuous and standardized data collection. Communicable diseases affect mainly children\\u2009<\\u200915\\xa0years. Non-communicable diseases are on the rise and need more attention by national health authorities. A number of hospitalizations could be prevented by a better health care management at the community-based health care level and by implementation of a Universal Health Coverage (UHC) in Madagascar.'},\n",
              "  {'title': 'Excipients in pharmaceuticals: mechanisms of hypersensitivity and the role of global pharmacovigilance.',\n",
              "   'authors': ['Ruba Malkawi', 'Lora Altahrawi'],\n",
              "   'abstract': 'Excipients are important inactive components in drug formulations that ensure stability, bioavailability, and patient compliance. However, emerging evidence suggests that certain excipients, once considered inert, can cause hypersensitivity reactions in certain individuals. Such reactions include mild erythema due to systemic anaphylaxis and create clinical challenges that are difficult to handle. This review presents a systematic review of the existing literature on excipient hypersensitivity, with specific attention paid to commonly implicated excipients such as polyethylene glycol (PEG), parabens, and tartrazine. Hypersensitivity mechanisms (immune-mediated [IgE, T-cell] and non-immune) are discussed, along with their clinical features and diagnostic challenges. In addition, geographic variations in reporting are discussed, which in turn focus on the role of pharmacovigilance in the reduction of risk. Geographic variations in excipient hypersensitivity reporting are also discussed, highlighting disparities in pharmacovigilance efforts across different regions. This review also discusses recent work, regulatory issues, and desensitization protocols for the control of hypersensitivity reactions. Persistent surveillance and individual strategies are needed to enhance patient safety in the context of excipient-induced hypersensitivity.',\n",
              "   'year': 2025,\n",
              "   'url': 'https://pubmed.ncbi.nlm.nih.gov/40627271/',\n",
              "   'pmid': '40627271',\n",
              "   'source': 'PubMed',\n",
              "   'content': 'Excipients are important inactive components in drug formulations that ensure stability, bioavailability, and patient compliance. However, emerging evidence suggests that certain excipients, once considered inert, can cause hypersensitivity reactions in certain individuals. Such reactions include mild erythema due to systemic anaphylaxis and create clinical challenges that are difficult to handle. This review presents a systematic review of the existing literature on excipient hypersensitivity, with specific attention paid to commonly implicated excipients such as polyethylene glycol (PEG), parabens, and tartrazine. Hypersensitivity mechanisms (immune-mediated [IgE, T-cell] and non-immune) are discussed, along with their clinical features and diagnostic challenges. In addition, geographic variations in reporting are discussed, which in turn focus on the role of pharmacovigilance in the reduction of risk. Geographic variations in excipient hypersensitivity reporting are also discussed, highlighting disparities in pharmacovigilance efforts across different regions. This review also discusses recent work, regulatory issues, and desensitization protocols for the control of hypersensitivity reactions. Persistent surveillance and individual strategies are needed to enhance patient safety in the context of excipient-induced hypersensitivity.'},\n",
              "  {'title': 'Perceptual features win again: the role of knowledge of acting with objects in visual search.',\n",
              "   'authors': ['Anufrieva Anastasia', 'Gorbunova Elena'],\n",
              "   'abstract': \"Can the activation of a motor programme help find object? On the assumption that knowledge about the way of acting with an object is included in its general representation, in the naming and categorization tasks a compatibility effect was obtained. Since during visual search an attention template is formed on the basis of theobject representation, the question of the occurrence of compatibility effect within a visual search task arises. This study focuses on the issue of the emergence of compatibility effects in visual search using subsequent search misses (SSM) paradigm. In a visual search task, subjects had to find a target stimulus (images of real objects) among distractors. There could be two, one, or none targets. During the search, subjects performed a differentiated grasping or pinching movements that were congruent, incongruent, and partially congruent to the target object. Additionally, an experiment was conducted where subjects didn't perform any movement while searching for the same objects. The results demonstrated that visual search efficiency for the first or single target depends on perceptual features of objects rather than motor program congruency. Moreover, reaction time for the second target or reporting its absence linked with the search efficiency of the first or single ones: the more time spent searching for the first or single target, the less time was required for the second or for reporting absence. Generally speaking, activation of motor program has no impact on visual search for images of real objects in SSM paradigm.\",\n",
              "   'year': 2025,\n",
              "   'url': 'https://pubmed.ncbi.nlm.nih.gov/40627243/',\n",
              "   'pmid': '40627243',\n",
              "   'source': 'PubMed',\n",
              "   'content': \"Can the activation of a motor programme help find object? On the assumption that knowledge about the way of acting with an object is included in its general representation, in the naming and categorization tasks a compatibility effect was obtained. Since during visual search an attention template is formed on the basis of theobject representation, the question of the occurrence of compatibility effect within a visual search task arises. This study focuses on the issue of the emergence of compatibility effects in visual search using subsequent search misses (SSM) paradigm. In a visual search task, subjects had to find a target stimulus (images of real objects) among distractors. There could be two, one, or none targets. During the search, subjects performed a differentiated grasping or pinching movements that were congruent, incongruent, and partially congruent to the target object. Additionally, an experiment was conducted where subjects didn't perform any movement while searching for the same objects. The results demonstrated that visual search efficiency for the first or single target depends on perceptual features of objects rather than motor program congruency. Moreover, reaction time for the second target or reporting its absence linked with the search efficiency of the first or single ones: the more time spent searching for the first or single target, the less time was required for the second or for reporting absence. Generally speaking, activation of motor program has no impact on visual search for images of real objects in SSM paradigm.\"},\n",
              "  {'title': 'Ononin induces ferroptosis in colorectal cancer cells via the PI3K/AKT/Nrf2 pathway to enhance anti-cancer immunotherapy.',\n",
              "   'authors': ['Yang Gui',\n",
              "    'Shuangjiao Deng',\n",
              "    'Jingjing Li',\n",
              "    'Dongmei Zuo',\n",
              "    'Heng Fan'],\n",
              "   'abstract': 'Colorectal cancer (CRC) represents one of the most prevalent forms of malignant neoplasms affecting the digestive tract. Recent studies have demonstrated that the induction of ferroptosis in tumor cells represents a novel therapeutic strategy. Ononin, an isoflavone glycoside compound derived from traditional Chinese medicinal plants, has garnered attention for its purported therapeutic efficacy. This study integrated network pharmacology and experimental studies to elucidate the underlying mechanism involved in the therapeutic action of ononin against CRC. The results demonstrated that ononin induced lipid peroxidation and a reduction in mitochondrial membrane potential (MMP), indicative of mitochondrial damage in CRC cells, accompanied by a pronounced decline in GPX4 expression. The combination of ononin and anti-PD-L1 therapy demonstrated a notable enhancement in tumor growth inhibition in the MC38 mouse CRC model, accompanied by a marked increase in the proportion of intratumoral IFNŒ≥+CD8+T cells. In summary, ononin triggers ferroptosis in colorectal cancer cells by increasing lipid ROS through the PI3K/AKT/Nrf2 pathway and reducing GPX4 expression. The combination of ononin and anti-PD-L1 therapy shows a synergistic anti-tumor effect, effectively inhibiting tumor growth and enhancing immune response. This study demonstrates that ononin is a promising therapeutic agent for CRC and may potentially serve as an immuno-adjuvant to enhance immunotherapy efficacy.',\n",
              "   'year': 2025,\n",
              "   'url': 'https://pubmed.ncbi.nlm.nih.gov/40627200/',\n",
              "   'pmid': '40627200',\n",
              "   'source': 'PubMed',\n",
              "   'content': 'Colorectal cancer (CRC) represents one of the most prevalent forms of malignant neoplasms affecting the digestive tract. Recent studies have demonstrated that the induction of ferroptosis in tumor cells represents a novel therapeutic strategy. Ononin, an isoflavone glycoside compound derived from traditional Chinese medicinal plants, has garnered attention for its purported therapeutic efficacy. This study integrated network pharmacology and experimental studies to elucidate the underlying mechanism involved in the therapeutic action of ononin against CRC. The results demonstrated that ononin induced lipid peroxidation and a reduction in mitochondrial membrane potential (MMP), indicative of mitochondrial damage in CRC cells, accompanied by a pronounced decline in GPX4 expression. The combination of ononin and anti-PD-L1 therapy demonstrated a notable enhancement in tumor growth inhibition in the MC38 mouse CRC model, accompanied by a marked increase in the proportion of intratumoral IFNŒ≥+CD8+T cells. In summary, ononin triggers ferroptosis in colorectal cancer cells by increasing lipid ROS through the PI3K/AKT/Nrf2 pathway and reducing GPX4 expression. The combination of ononin and anti-PD-L1 therapy shows a synergistic anti-tumor effect, effectively inhibiting tumor growth and enhancing immune response. This study demonstrates that ononin is a promising therapeutic agent for CRC and may potentially serve as an immuno-adjuvant to enhance immunotherapy efficacy.'},\n",
              "  {'title': 'Predicting SARS-CoV-2-specific CD4+ and CD8+ T-cell responses elicited by inactivated vaccines in healthy adults using machine learning models.',\n",
              "   'authors': ['Jie Ning',\n",
              "    'Yayi Ren',\n",
              "    'Zelin Zhang',\n",
              "    'Xianhuang Zeng',\n",
              "    'Qinjin Wang',\n",
              "    'Jia Xie',\n",
              "    'Yue Xu',\n",
              "    'Yali Fan',\n",
              "    'Huilan Li',\n",
              "    'Aixia Zhai',\n",
              "    'Bin Li',\n",
              "    'Chao Wu',\n",
              "    'Ying Chen'],\n",
              "   'abstract': \"The ongoing evolution of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) variants highlights the importance of monitoring immune responses to guide vaccination strategies. Although neutralizing antibodies (NAbs) have garnered increasing attention, T-cells are crucial for conferring long-lasting immunity, especially their resilience against viral mutations. However, assessing T-cell responses clinically has been hindered by cost and complexity. In this study, we recruited a cohort of 134 healthy adults, who had been immunized with three doses of the SARS-CoV-2 inactivated vaccine. Cellular immunity elicited by a comprehensive array of overlapping peptides covering the entire sequence of the virus's structural proteins was assessed by intracellular cytokine staining (ICS). Additionally, a dataset including demographic information, routine blood indices, and immune cell indicators comprising 32 variables was collected. Multivariate analysis revealed age and days post-vaccination as key factors influencing the strength of the T-cell response. Importantly, random forest (RF) and classification and regression tree (CART) algorithms were employed, along with 8 easily accessible indicators to formulate predictive models for the SARS-CoV-2-specific CD4+ and CD8+ T-cell responses. Besides, these models demonstrated substantial accuracy (r\\u2009>\\u20090.9) in both the training and testing sets. Our findings offer an efficient and economical methodology for evaluating the T-cell reactions in healthy adults following inactivated SARS-CoV-2 vaccination, which is visualizable and easy to use, providing a novel strategy for assessing cellular immunity after vaccination.\",\n",
              "   'year': 2025,\n",
              "   'url': 'https://pubmed.ncbi.nlm.nih.gov/40627199/',\n",
              "   'pmid': '40627199',\n",
              "   'source': 'PubMed',\n",
              "   'content': \"The ongoing evolution of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) variants highlights the importance of monitoring immune responses to guide vaccination strategies. Although neutralizing antibodies (NAbs) have garnered increasing attention, T-cells are crucial for conferring long-lasting immunity, especially their resilience against viral mutations. However, assessing T-cell responses clinically has been hindered by cost and complexity. In this study, we recruited a cohort of 134 healthy adults, who had been immunized with three doses of the SARS-CoV-2 inactivated vaccine. Cellular immunity elicited by a comprehensive array of overlapping peptides covering the entire sequence of the virus's structural proteins was assessed by intracellular cytokine staining (ICS). Additionally, a dataset including demographic information, routine blood indices, and immune cell indicators comprising 32 variables was collected. Multivariate analysis revealed age and days post-vaccination as key factors influencing the strength of the T-cell response. Importantly, random forest (RF) and classification and regression tree (CART) algorithms were employed, along with 8 easily accessible indicators to formulate predictive models for the SARS-CoV-2-specific CD4+ and CD8+ T-cell responses. Besides, these models demonstrated substantial accuracy (r\\u2009>\\u20090.9) in both the training and testing sets. Our findings offer an efficient and economical methodology for evaluating the T-cell reactions in healthy adults following inactivated SARS-CoV-2 vaccination, which is visualizable and easy to use, providing a novel strategy for assessing cellular immunity after vaccination.\"},\n",
              "  {'title': 'Probing instructed but unnecessary switches of attentional strategy.',\n",
              "   'authors': ['Svantje T K√§hler',\n",
              "    'Mike Wendt',\n",
              "    'Aquiles Luna-Rodriguez',\n",
              "    'Thomas Jacobsen'],\n",
              "   'abstract': 'It is widely assumed that attentional strategies can be intentionally shifted. Experimental evidence of such adjustment stems almost exclusively from situations associated with changes concerning perceptual stimulus features, stimulus-related contingencies, or response demands, however. In a series of experiments, we investigated intention- (i.e., instruction-) based shifts of attentional strategy in the absence of additional changes in the task/stimulus environment compared with conditions associated with maintenance of the attentional strategy (i.e., keeping task stimuli, responses, stimulus-response assignments, and presentation contingencies constant for conditions of shift and maintenance). Our method involved a probe task procedure diagnostic of the attentional strategy applied (i.e., strong or weak focusing of visual attention on the centrally presented stimulus element). In Experiment 1, participants were instructed to change the strategy after the first half of the trials. Probe task results provided evidence for adherence to instruction. In Experiments 2 and 3, which involved presenting instructional cues on a trial-by-trial basis, adjustment of attentional strategy appeared confined to a high degree of motivation. Experiment 4 suggests the carryover of instructed attentional strategies to a following (probe task) trial when no novel instruction was presented. Our study demonstrates instruction-based shifts in attentional strategy that are discernably unnecessary for solving the current task and occur without support from a change in the task/stimulus environment.',\n",
              "   'year': 2025,\n",
              "   'url': 'https://pubmed.ncbi.nlm.nih.gov/40627167/',\n",
              "   'pmid': '40627167',\n",
              "   'source': 'PubMed',\n",
              "   'content': 'It is widely assumed that attentional strategies can be intentionally shifted. Experimental evidence of such adjustment stems almost exclusively from situations associated with changes concerning perceptual stimulus features, stimulus-related contingencies, or response demands, however. In a series of experiments, we investigated intention- (i.e., instruction-) based shifts of attentional strategy in the absence of additional changes in the task/stimulus environment compared with conditions associated with maintenance of the attentional strategy (i.e., keeping task stimuli, responses, stimulus-response assignments, and presentation contingencies constant for conditions of shift and maintenance). Our method involved a probe task procedure diagnostic of the attentional strategy applied (i.e., strong or weak focusing of visual attention on the centrally presented stimulus element). In Experiment 1, participants were instructed to change the strategy after the first half of the trials. Probe task results provided evidence for adherence to instruction. In Experiments 2 and 3, which involved presenting instructional cues on a trial-by-trial basis, adjustment of attentional strategy appeared confined to a high degree of motivation. Experiment 4 suggests the carryover of instructed attentional strategies to a following (probe task) trial when no novel instruction was presented. Our study demonstrates instruction-based shifts in attentional strategy that are discernably unnecessary for solving the current task and occur without support from a change in the task/stimulus environment.'}],\n",
              " 'count': 37,\n",
              " 'message': 'Found 37 papers'}"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        " research_mate.quick_search('attention')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "01c8c68536464ed4b2c0a7abbf2bd814": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "02395738b2874fc4a2f08922188826a8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0284d6b08c7f42bd8c446825416f6258": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "02eb541780c34a6882c6c229fd396e82": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "048ded1ed86b499e963a79633aed190c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0622a55a482740139a1200eaf6532617": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0704525e4399411a999c40a9712ceb08": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09f3066952054f78965a74584cd0e1ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0b151a27391e4f6186d2b8a0b1f57382": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0c23ac18d5cf479086127d2eb4cc5eb6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0cd803469ed74da8a17fd66b4d29d9d0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0fb495a137bc4affa94ee09498cf9c12": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "10f55dbba6a649239b1a5a103af460a7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15824affe4d744458f9db3a3e1491f65": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8b35063955e54c09a5dc4ba99e7bf3f3",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_b89c551ca92c4298b9b54feb12c40711",
            "value": "vocab.txt:‚Äá"
          }
        },
        "1a6d66dc1dde466fb3f455c4ce31362b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1dc9780edc89478cb0a113fced247622": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bbd8d5a7a88f43e2b8afdfff46a1ddca",
            "max": 612,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3051416b47474430834067a266be7062",
            "value": 612
          }
        },
        "1e713b8997b3420cb74aa90152055c00": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_671f83945874437a962d1acaa1f173c5",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_61ce5e244ea4444d8d660bd32db8eaf8",
            "value": "‚Äá349/349‚Äá[00:00&lt;00:00,‚Äá30.3kB/s]"
          }
        },
        "1f69800c098a43bbb9646e4dbf4a6001": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a000eb0f593b42e99197e27137f5729f",
            "max": 116,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0fb495a137bc4affa94ee09498cf9c12",
            "value": 116
          }
        },
        "24e1819cf504499ead647ce2b3c99279": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "260925605f934c35a2826635d8f7c0b2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26e2b7e6f8094564a964d22c86336b24": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3b7c3c3c6ff145cfbc44f5d8912aa579",
              "IPY_MODEL_ea6d7b06d7664bcfb2a53ab1fe33fd27",
              "IPY_MODEL_41e658e12d7943e88d08a419894fd72b"
            ],
            "layout": "IPY_MODEL_9c4b98bb430c47a5b070ca906bf1a2c1"
          }
        },
        "27b65473a60f4fb59bfa50e5d6a23c5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b708d5332e214b35bf7be28772db57d0",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_09f3066952054f78965a74584cd0e1ce",
            "value": "‚Äá10.5k/?‚Äá[00:00&lt;00:00,‚Äá614kB/s]"
          }
        },
        "27e50cca49be46cfa408ca6179c8aab0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2a0062c2dae84d049a343ccde5121dfb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de08fa989d3242a998afe9b1f3b8bea4",
            "max": 53,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_450d5ff4f0f847df8168550541682c17",
            "value": 53
          }
        },
        "2b18ec66f5eb4535b064927174bc9729": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10f55dbba6a649239b1a5a103af460a7",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_eab91f3980ec4bef95e744fbb72b3cb8",
            "value": "‚Äá116/116‚Äá[00:00&lt;00:00,‚Äá10.2kB/s]"
          }
        },
        "2ef870942bfc4f08941db0064f26ad73": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ef54d00e6f8d4fe39feabd929f0756cc",
              "IPY_MODEL_977c2b37901941ac9cf2a7fe555c6b45",
              "IPY_MODEL_b82e6746847445e2b09949f045a9770c"
            ],
            "layout": "IPY_MODEL_02395738b2874fc4a2f08922188826a8"
          }
        },
        "3051416b47474430834067a266be7062": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3332939f682448528033b95364677ae1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33bb0df178b24b0eb8abffd9fb7fb859": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b7c3c3c6ff145cfbc44f5d8912aa579": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb350b4787e64481adee64061ea49100",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_4f634dca7a5640299b031c1872d7f416",
            "value": "special_tokens_map.json:‚Äá100%"
          }
        },
        "3c6265126d664477bea2376dccea0914": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "41e658e12d7943e88d08a419894fd72b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_935e91b7d6dc475dae1dc40899088e86",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_3c6265126d664477bea2376dccea0914",
            "value": "‚Äá112/112‚Äá[00:00&lt;00:00,‚Äá1.71kB/s]"
          }
        },
        "42b82f4a91a344968bcbfffd66af087c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "447ebd4c59a24062bdbd98d7301d543e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "450d5ff4f0f847df8168550541682c17": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4680b945991c440ea83ba2933ab18239": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4759d7e761474f81944395a05d26d9bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "49c601085670420ab7fd919673fbd400": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4ba16e2312224aa3b3c56c6f532f99a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bcde909348124dabb6bf33e322004068",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_7e3b6be4c79e4715b82502498ad97c6e",
            "value": "‚Äá53.0/53.0‚Äá[00:00&lt;00:00,‚Äá3.98kB/s]"
          }
        },
        "4c2cbf3e4e564493aeb0b37941f6b051": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4dc6e9f044034cbbbff05478519c1dde": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_15824affe4d744458f9db3a3e1491f65",
              "IPY_MODEL_c99d3b98c3574a13a86d42d93e6b99f9",
              "IPY_MODEL_c3db88cccb9a4e82b0cd6d00fc05579c"
            ],
            "layout": "IPY_MODEL_e3465ce42b634f95bec698a7d6fb8e7d"
          }
        },
        "4f634dca7a5640299b031c1872d7f416": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "51586428e3a84ad084961d0410cb37c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_780e6b411a8f4873a2ad658e95984744",
              "IPY_MODEL_77735b885035477d908adbd7a789539a",
              "IPY_MODEL_942fdc37fde44231ab273f9299708878"
            ],
            "layout": "IPY_MODEL_f16933aa4e3f465b882507541708d2a1"
          }
        },
        "518a65634f6145f6b89fce0e2249a4fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5477616877cb481a985d43aee7796797": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ccbafb9904940d3bafcff3f1644f783": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e6b970307824f1db05c6a5eeaf67e18": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "61ce5e244ea4444d8d660bd32db8eaf8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "653c1cce08334983b335f8bc57c98bcb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "670bce775ac14e8f919ed7bbdebff95d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "671f83945874437a962d1acaa1f173c5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6fafcc1e7fa84f2da0c952187091f53d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c23ac18d5cf479086127d2eb4cc5eb6",
            "max": 90868376,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4759d7e761474f81944395a05d26d9bb",
            "value": 90868376
          }
        },
        "70448b95ebbc435b8f572ce518f0ec7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a307d782d86747bd94ddeba74e1999c9",
              "IPY_MODEL_6fafcc1e7fa84f2da0c952187091f53d",
              "IPY_MODEL_723e4aeb89884e7e90ed0a02cfb209bb"
            ],
            "layout": "IPY_MODEL_8d6820c9076e45619667c0df29b4e1d4"
          }
        },
        "70f5f9d53a4d46cbb978e62628997c3f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "723e4aeb89884e7e90ed0a02cfb209bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_70f5f9d53a4d46cbb978e62628997c3f",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_9acf329bfcfb4b549645e2230ccaff5f",
            "value": "‚Äá90.9M/90.9M‚Äá[00:01&lt;00:00,‚Äá72.6MB/s]"
          }
        },
        "72d89fa17bb74863810f802d7ea3d674": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "738420ad403b467e9dd0a751cdc41b21": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef3f04b3a70440c4ac88665aa1dedee2",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_c124b9c134eb4614872027641ecf95c6",
            "value": "README.md:‚Äá"
          }
        },
        "77735b885035477d908adbd7a789539a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c63184b41f534298a13305481ffe65e0",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_518a65634f6145f6b89fce0e2249a4fd",
            "value": 190
          }
        },
        "780e6b411a8f4873a2ad658e95984744": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4680b945991c440ea83ba2933ab18239",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_b01f362bece5437285fabb41344cc6c1",
            "value": "config.json:‚Äá100%"
          }
        },
        "7e3b6be4c79e4715b82502498ad97c6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8b35063955e54c09a5dc4ba99e7bf3f3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d6820c9076e45619667c0df29b4e1d4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d9b102b6b2a4e17b812eccbe1de6592": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a6d66dc1dde466fb3f455c4ce31362b",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_dc8d37e8c9fc4a34bd18d4bb3b66ffac",
            "value": "config.json:‚Äá100%"
          }
        },
        "8e0d1db6f08b4331a65b27327e6e4dbd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4eabb690fb44fac80bb1ed140c4fd98",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_f33a6bf6f75040ca8e38c8497b2ecb40",
            "value": "modules.json:‚Äá100%"
          }
        },
        "8e70e588a6af4bddbb4bedefe6c6ebfa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "915c98ebc6a94f9fa51a8abbe78479c7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9324aed3cba94c4191cc8601d71ac42c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_02eb541780c34a6882c6c229fd396e82",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_0b151a27391e4f6186d2b8a0b1f57382",
            "value": "config_sentence_transformers.json:‚Äá100%"
          }
        },
        "935e91b7d6dc475dae1dc40899088e86": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94163f516e284471bb2f57e9372c28d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c4c72c7b35e54f63a0a3b821a3ae1690",
              "IPY_MODEL_a47d1c5d0fc9430292253eccbf46cf07",
              "IPY_MODEL_f48073e335fb4dc88fdd924270148dc8"
            ],
            "layout": "IPY_MODEL_447ebd4c59a24062bdbd98d7301d543e"
          }
        },
        "942fdc37fde44231ab273f9299708878": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_260925605f934c35a2826635d8f7c0b2",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_9e439eba769143569409dd69a4534999",
            "value": "‚Äá190/190‚Äá[00:00&lt;00:00,‚Äá4.77kB/s]"
          }
        },
        "9615932f580c4e54a7338a926d34b634": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "977c2b37901941ac9cf2a7fe555c6b45": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_42b82f4a91a344968bcbfffd66af087c",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0622a55a482740139a1200eaf6532617",
            "value": 1
          }
        },
        "9acf329bfcfb4b549645e2230ccaff5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9c4b98bb430c47a5b070ca906bf1a2c1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e439eba769143569409dd69a4534999": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a000eb0f593b42e99197e27137f5729f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a307d782d86747bd94ddeba74e1999c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ccbafb9904940d3bafcff3f1644f783",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_c64d833fa8534220909d79c245a5dfba",
            "value": "model.safetensors:‚Äá100%"
          }
        },
        "a30bb18bba054c2fb0cdb3ffab4112a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9324aed3cba94c4191cc8601d71ac42c",
              "IPY_MODEL_1f69800c098a43bbb9646e4dbf4a6001",
              "IPY_MODEL_2b18ec66f5eb4535b064927174bc9729"
            ],
            "layout": "IPY_MODEL_3332939f682448528033b95364677ae1"
          }
        },
        "a47d1c5d0fc9430292253eccbf46cf07": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5477616877cb481a985d43aee7796797",
            "max": 350,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dcb3bac21fea465ebe566d37939a8103",
            "value": 350
          }
        },
        "a49ad0afc244473d8987da737970dad0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_738420ad403b467e9dd0a751cdc41b21",
              "IPY_MODEL_cd377d8af3ac456d8b343f881f720b6c",
              "IPY_MODEL_27b65473a60f4fb59bfa50e5d6a23c5c"
            ],
            "layout": "IPY_MODEL_e07469a013774453a0b31a12551d398f"
          }
        },
        "a57487e20776447a9e57d77df4288528": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ee87ef7578e14101b11a1d448f9a58a9",
              "IPY_MODEL_2a0062c2dae84d049a343ccde5121dfb",
              "IPY_MODEL_4ba16e2312224aa3b3c56c6f532f99a2"
            ],
            "layout": "IPY_MODEL_dbd8cd875c0a487799037439f6c51588"
          }
        },
        "a6fb36df165c4c6b844d6555b19bca9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a751281285f44cd09cf31949de5a916f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8d9b102b6b2a4e17b812eccbe1de6592",
              "IPY_MODEL_1dc9780edc89478cb0a113fced247622",
              "IPY_MODEL_e9d8c925c88d4d68bc8d09be0755f8cd"
            ],
            "layout": "IPY_MODEL_653c1cce08334983b335f8bc57c98bcb"
          }
        },
        "b01f362bece5437285fabb41344cc6c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b4803ed445094570bc9fd1fbd434c547": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b708d5332e214b35bf7be28772db57d0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b82e6746847445e2b09949f045a9770c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24e1819cf504499ead647ce2b3c99279",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_c5db68cd83c54225bea7dd70d3eeb7fc",
            "value": "‚Äá466k/?‚Äá[00:00&lt;00:00,‚Äá7.95MB/s]"
          }
        },
        "b89c551ca92c4298b9b54feb12c40711": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bbd8d5a7a88f43e2b8afdfff46a1ddca": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bcde909348124dabb6bf33e322004068": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c124b9c134eb4614872027641ecf95c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c1e93cc3f19d444ead23a5dcb598e570": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_915c98ebc6a94f9fa51a8abbe78479c7",
            "max": 349,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8e70e588a6af4bddbb4bedefe6c6ebfa",
            "value": 349
          }
        },
        "c3db88cccb9a4e82b0cd6d00fc05579c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9615932f580c4e54a7338a926d34b634",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_b4803ed445094570bc9fd1fbd434c547",
            "value": "‚Äá232k/?‚Äá[00:00&lt;00:00,‚Äá4.45MB/s]"
          }
        },
        "c4c72c7b35e54f63a0a3b821a3ae1690": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0704525e4399411a999c40a9712ceb08",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_27e50cca49be46cfa408ca6179c8aab0",
            "value": "tokenizer_config.json:‚Äá100%"
          }
        },
        "c5db68cd83c54225bea7dd70d3eeb7fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c63184b41f534298a13305481ffe65e0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c64d833fa8534220909d79c245a5dfba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c99d3b98c3574a13a86d42d93e6b99f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_01c8c68536464ed4b2c0a7abbf2bd814",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4c2cbf3e4e564493aeb0b37941f6b051",
            "value": 1
          }
        },
        "cd377d8af3ac456d8b343f881f720b6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de133caa40d447d480d733cba0afe27b",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_49c601085670420ab7fd919673fbd400",
            "value": 1
          }
        },
        "d0bcff3978c84c9fadedbbfae00d61a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d4eabb690fb44fac80bb1ed140c4fd98": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dbd8cd875c0a487799037439f6c51588": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc8d37e8c9fc4a34bd18d4bb3b66ffac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dcb3bac21fea465ebe566d37939a8103": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "de08fa989d3242a998afe9b1f3b8bea4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de133caa40d447d480d733cba0afe27b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "e057a92a108048c79fd098a5dfe3b54c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e07469a013774453a0b31a12551d398f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3465ce42b634f95bec698a7d6fb8e7d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e348ab8f71204eb0b084fda45ade0fa9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9d8c925c88d4d68bc8d09be0755f8cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e348ab8f71204eb0b084fda45ade0fa9",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_670bce775ac14e8f919ed7bbdebff95d",
            "value": "‚Äá612/612‚Äá[00:00&lt;00:00,‚Äá46.0kB/s]"
          }
        },
        "ea6d7b06d7664bcfb2a53ab1fe33fd27": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e057a92a108048c79fd098a5dfe3b54c",
            "max": 112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_048ded1ed86b499e963a79633aed190c",
            "value": 112
          }
        },
        "eab91f3980ec4bef95e744fbb72b3cb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eb350b4787e64481adee64061ea49100": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee87ef7578e14101b11a1d448f9a58a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0284d6b08c7f42bd8c446825416f6258",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_5e6b970307824f1db05c6a5eeaf67e18",
            "value": "sentence_bert_config.json:‚Äá100%"
          }
        },
        "ef3f04b3a70440c4ac88665aa1dedee2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef54d00e6f8d4fe39feabd929f0756cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33bb0df178b24b0eb8abffd9fb7fb859",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_a6fb36df165c4c6b844d6555b19bca9e",
            "value": "tokenizer.json:‚Äá"
          }
        },
        "f16933aa4e3f465b882507541708d2a1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f17c7c4b81654bd182c84c04047b5e94": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8e0d1db6f08b4331a65b27327e6e4dbd",
              "IPY_MODEL_c1e93cc3f19d444ead23a5dcb598e570",
              "IPY_MODEL_1e713b8997b3420cb74aa90152055c00"
            ],
            "layout": "IPY_MODEL_0cd803469ed74da8a17fd66b4d29d9d0"
          }
        },
        "f33a6bf6f75040ca8e38c8497b2ecb40": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f48073e335fb4dc88fdd924270148dc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_72d89fa17bb74863810f802d7ea3d674",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_d0bcff3978c84c9fadedbbfae00d61a2",
            "value": "‚Äá350/350‚Äá[00:00&lt;00:00,‚Äá7.70kB/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
